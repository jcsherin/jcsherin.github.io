{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Jacob&#39;s blog",
  "language": "en",
  "home_page_url": "https://jacobsherin.com/",
  "feed_url": "https://example.com/feed/feed.json",
  "description": "On database building blocks.",
  "author": {
    "name": "Your Name Here",
    "url": "https://example.com/about-me/"
  },
  "items": [{
      "id": "https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/",
      "url": "https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/",
      "title": "A 6X Speedup for a Parquet Shredding Pipeline",
      "content_html": "<p>Lately, I've been poking around record shredding and needed a dataset of nested data structures for tracing query execution of shredded data. For this, I implemented a data generator which follows a <a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">Zipfian-like</a> distribution. The generated data is staged in-memory as <a href=\"https://arrow.apache.org/rust/parquet/arrow/index.html\">Arrow RecordBatches</a>, and then written to disk as Parquet files.</p>\n<p>The baseline version I wrote is a simple pipeline using Rust MPSC which connects multiple data generation (producer) threads to a single Parquet writer (consumer) thread. For a nested dataset of 10 million rows, it ~3.7s to complete. In this post, we'll see how a sequence of performance optimizations, reduced the total runtime to ~533ms (6x speedup).</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/hyperfine_trend_plot.png\" alt=\"Performance Trend Across Runs\" /></p>\n<figcaption>Fig 1. The performance trend across a sequence of code optimizations, measured using <code>hyperfine</code>. The black line indicates the median runtime in seconds, while the shaded area indicates the range between min and max runtime.</figcaption>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/ipc_trend_plot.png\" alt=\"IPC Trend Across Runs\" /></p>\n<p>This chart displays only improvements in total runtime, which does not tell the whole story. While some optimizations here show no difference in the total runtime, the improvements came from higher IPC (instructions per cycle), fewer cache misses and fewer branch mispredictions.</p>\n<p>A string interning optimization (no. 9) looked like a guaranteed win. It was introduced to eliminate a lot of small string allocations in the data generation (producer) threads. The performance got worse (more on this later in this post) and the change had to be reverted. This strongly reinforces, the importance of measurements and profiling data for knowing unambiguously if a code optimization made an improvement or did the opposite.</p>\n<p>All benchmarks were run on a Linux machine with the following configuration:</p>\n<ul>\n<li>Ubuntu 24.04.2 LTS (Kernel 6.8)</li>\n<li>AMD Ryzen 7 PRO 8700GE (8 Cores, 16 Threads)</li>\n<li>64 GB of DDR5-5600 ECC RAM</li>\n<li>512 GB NVMe SSDs.</li>\n</ul>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#background\">Background</a></li>\n    <li>\n        <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-1:-getting-started\">Phase 1: Getting Started</a>\n        <ul>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#01:-use-a-dictionary-data-type\">01: Use a Dictionary Data Type</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#02:-eliminate-intermediate-vector-allocation\">02: Eliminate Intermediate Vector Allocation</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#03:-preallocate-a-string-buffer\">03: Preallocate a String Buffer</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#04:-preallocate-a-string-buffer-2\">04: Preallocate a String Buffer 2</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#why-is-the-runtime-unchanged\">Why is the Runtime Unchanged?</a></li>\n        </ul>\n    </li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-2:-architectural-changes\">Phase 2: Architectural Changes</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#05:-increase-parquet-encoding-bandwidth\">05: Increase Parquet Encoding Bandwidth</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#06:-make-data-generation-lightweight\">06: Make Data Generation Lightweight</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#07:-increase-parquet-writer-threads\">07: Increase Parquet Writer Threads</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#08:-introduce-thread-local-state\">08: Introduce Thread-Local State</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#measuring-impact\">Measuring Impact</a></li>\n      </ul>\n    </li>\n    <li>\n        <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-3:-a-performance-regression\">Phase 3: A Performance Regression</a>\n        <ul>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#09:-global-string-interning\">09: Global String Interning</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#10:-revert\">10: Revert</a></li>\n        </ul>\n    </li>\n    <li>\n        <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#a-back-of-the-envelope-estimation\">A Back of the Envelope Estimation</a>\n    </li>\n    <li>\n        <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#fine-tuning-configuration-for-optimal-performance\">Fine-tuning Configuration for Optimal Performance</a>\n    </li>\n  </ol>\n</nav>\n<h2 id=\"background\" tabindex=\"-1\">Background <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#background\" aria-hidden=\"true\">#</a></h2>\n<p>The program is a CLI tool for generating a target number of rows of nested data structures and then written to disk in Parquet format.</p>\n<p>Nested data structures do not naturally fit into a flat columnar format. Record shredding is a process which converts the nested data into a flat, columnar format while preserving the original structural hierarchy of the raw data.</p>\n<p>The generated data follows a <a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">Zipfian-like</a> distribution. It is staged in memory as <a href=\"https://arrow.apache.org/rust/parquet/arrow/index.html\">Arrow RecordBatches</a>, before being written to disk as Parquet files.</p>\n<p>The data is generated in parallel using a <a href=\"https://github.com/rayon-rs/rayon\">Rayon</a> thread pool. Then data generator threads (producers) sends the data to a Parquet writer thread (consumer). The number of writers are configurable from the CLI.</p>\n<h2 id=\"phase-1:-getting-started\" tabindex=\"-1\">Phase 1: Getting Started <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-1:-getting-started\" aria-hidden=\"true\">#</a></h2>\n<p>First, we build the CLI program in release mode and use that for end to end benchmarking using <a href=\"https://github.com/sharkdp/hyperfine\">hyperfine</a>.</p>\n<p>In <code>Cargo.toml</code> the following section is added for release builds:</p>\n<pre class=\"language-toml\"><code class=\"language-toml\"><span class=\"token punctuation\">[</span><span class=\"token table class-name\">profile.release</span><span class=\"token punctuation\">]</span><br /><span class=\"token key property\">debug</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"line-tables-only\"</span><br /><span class=\"token key property\">strip</span> <span class=\"token punctuation\">=</span> <span class=\"token boolean\">false</span></code></pre>\n<p>This will include just enough debug information in the release binary which will help us trace hotspots back to the exact line of code in Rust. This is necessary when recording the call-graphs of the program's execution using <code>perf</code>.</p>\n<p>When generating flamegraphs, we will use <a href=\"https://github.com/luser/rustfilt\">rustfilt</a> to demangle the symbols for improved readability.</p>\n<p>We will also collect hardware performance counters like - cycles, instructions retired, cache references, cache misses, branch instructions and branch mispredictions.</p>\n<p>The following optimizations from 01 through 04, uses the flamegraph to identify hotspots indicated by tall towers and then attempt to squash it.</p>\n<h3 id=\"01:-use-a-dictionary-data-type\" tabindex=\"-1\">01: Use a Dictionary Data Type <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#01:-use-a-dictionary-data-type\" aria-hidden=\"true\">#</a></h3>\n<p>In the baseline version, the <code>PhoneType</code> Rust enum is mapped to a string data type (<code>DataType::Utf8</code>) in the Arrow schema.</p>\n<pre class=\"language-rust\"><code class=\"language-rust\"><span class=\"token keyword\">pub</span> <span class=\"token keyword\">enum</span> <span class=\"token type-definition class-name\">PhoneType</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token class-name\">Mobile</span><span class=\"token punctuation\">,</span><br />    <span class=\"token class-name\">Home</span><span class=\"token punctuation\">,</span><br />    <span class=\"token class-name\">Work</span><span class=\"token punctuation\">,</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Instead, by changing the Arrow field data type to <code>DataType::Dictionary</code>, the expectation is that the total memory footprint of the program, and storage size of the Parquet file will improve.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">pub fn get_contact_phone_fields() -> Vec&lt;Arc&lt;Field>> {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    vec![<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        Arc::from(Field::new(\"number\", DataType::Utf8, true)),<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">        Arc::from(Field::new(\"phone_type\", DataType::Utf8, true)),<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">        Arc::from(Field::new(<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            \"phone_type\",<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            DataType::Dictionary(Box::new(DataType::UInt8), Box::new(DataType::Utf8)),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            true,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        )),<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    ]<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">}</span></span></code></pre>\n<p>After the change, the maximum RSS (resident set size) is reduced by ~1MB in a run for generating 10 million rows. The Parquet storage size improvement is negligible. There is a minor regression in runtime.</p>\n<p>Even though, there are no dramatic gains here like we expected, we will maintain this change because it removes the mismatch between the underlying Rust and Arrow data types. That is definitely a readability improvement.</p>\n<h3 id=\"02:-eliminate-intermediate-vector-allocation\" tabindex=\"-1\">02: Eliminate Intermediate Vector Allocation <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#02:-eliminate-intermediate-vector-allocation\" aria-hidden=\"true\">#</a></h3>\n<p>The generate data with a predefined data skew (Zipfian-like), a data template value is first generated. The holes in the templates are filled in to generate the final <code>Contact</code> struct value, which is then converted to an Arrow <code>RecordBatch</code>. The series of value transformations looks like this:</p>\n<p><code>Vec&lt;PartialContact&gt;</code> → <code>Vec&lt;Contact&gt;</code> → <code>RecordBatch</code>.</p>\n<p>Instead of creating the intermediate <code>Vec&lt;Contact&gt;</code>, we can do a late materialization of the final <code>Contact</code> value when building a <code>RecordBatch</code> by directly passing it the instructions within <code>Vec&lt;PartialContact&gt;</code>. After eliminating the intermediate step, the value transformation will look like this:</p>\n<p><code>Vec&lt;PartialContact&gt;</code> → <code>RecordBatch</code>.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">  // Assemble the Vec&lt;Contact> for this small chunk<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">  let contacts_chunk: Vec&lt;Contact> = partial_contacts<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">      .into_iter()<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">      .map(|partial_contact| { ... })<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">      .collect();<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\"><br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">  // Convert the chunk to a RecordBatch and send it to the writer<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">  let record_batch = create_record_batch(parquet_schema.clone(), &amp;contacts_chunk)<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">      .expect(\"Failed to create RecordBatch\");<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">  let record_batch =<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      to_record_batch(<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">           parquet_schema.clone(), &amp;phone_id_counter, partial_contacts)<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">       .expect(\"Failed to create RecordBatch\");<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"></span></span></code></pre>\n<p>After the change, there is no noticeable change in total runtime. On the other hand, there is a noticeable improvement across the board in CPU utilization metrics. Even though the pipeline did not execute any faster, it ran more efficiently.</p>\n<h3 id=\"03:-preallocate-a-string-buffer\" tabindex=\"-1\">03: Preallocate a String Buffer <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#03:-preallocate-a-string-buffer\" aria-hidden=\"true\">#</a></h3>\n<p>In the hot loop, where a <code>RecordBatch</code> is being created, a string is allocated in the heap for each generated value. For a run of 10 million rows this is the equivalent of 10 million heap allocations.</p>\n<p>We can eliminate 99% of these allocations by reusing a mutable string buffer within the loop where <code>PartialContact</code> template values are being materialized and appended into the <code>RecordBatch</code>.</p>\n<p>Suppose a <code>RecordBatch</code> is created from a chunk of 1K row values, it now requires only 10K heap allocations.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let mut phone_number_buf = String::with_capacity(16);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    for PartialContact(name, phones) in chunk {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        name_builder.append_option(name);<br /></span></span><br />@@ -155,11 +158,13 @@ fn to_record_batch(<br /><br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    if has_phone_number {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">       let id = phone_id_counter.fetch_add(1, Ordering::Relaxed);<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">       let phone_number = Some(format!(\"+91-99-{id:08}\"));<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">       write!(phone_number_buf, \"+91-99-{id:08}\")?;<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">       struct_builder<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">           .field_builder::&lt;StringBuilder>(PHONE_NUMBER_FIELD_INDEX)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">           .unwrap()<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">           .append_option(phone_number);<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">           .append_value(&amp;phone_number_buf);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">       phone_number_buf.clear();</span></span></code></pre>\n<p>After this change, there is again no noticeable change in the total runtime. But similar to earlier change, all measures point to an overall improvement in the CPU efficiency of the program.</p>\n<h3 id=\"04:-preallocate-a-string-buffer-2\" tabindex=\"-1\">04: Preallocate a String Buffer 2 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#04:-preallocate-a-string-buffer-2\" aria-hidden=\"true\">#</a></h3>\n<p>This is a follow up optimization from the previous one. The idea is the same, to eliminate 99% of heap allocations when generating data, by preallocating a mutable string buffer, and reusing it.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">fn name_strategy() -> BoxedStrategy&lt;Option&lt;String>> {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    prop_oneof![<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">        80 => Just(()).prop_map(|_| Some(format!(\"{} {}\", FirstName().fake::&lt;String>(), LastName().fake::&lt;String>()))),<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">        80 => Just(()).prop_map(|_| {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            let mut name_buf = String::with_capacity(32);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            write!(&amp;mut name_buf, \"{} {}\", FirstName().fake::&lt;&amp;str>(), LastName().fake::&lt;&amp;str>()).unwrap();<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            Some(name_buf)<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">         }),<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        20 => Just(None)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    ].boxed()<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">}<br /></span></span></code></pre>\n<p>The results are identical to the previous optimization. No change in the total runtime. But there is considerable improvement in the CPU efficiency of the program.</p>\n<h3 id=\"why-is-the-runtime-unchanged\" tabindex=\"-1\">Why is the Runtime Unchanged? <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#why-is-the-runtime-unchanged\" aria-hidden=\"true\">#</a></h3>\n<p>The optimizations so far had little to no effect on the total runtime of the program, which has remained stable.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/hyperfine_boxplot_grid_phase1.png\" alt=\"Hyperfine box plots for baseline version up to run 04\" /></p>\n<p>The flamegraph profiles taken after each optimization also display a similar consistency.</p>\n<p>We have not seen a speedup in the underlying program despite the optimizations is related to <a href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\">Amdhal's law</a>. The pipeline execution spent only a small fraction of its total execution time in the hot loops which were optimized. This is characterized by tall but narrow towers in the flamegraph profile. To achieve a runtime speedup, we have to focus on the widest towers, as they indicate where the most amount of time is spend.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/flamegraph_montage_phase1.png\" alt=\"Flamegraph montage for baseline version up to run 04\" /></p>\n<p>The CPU efficiency has improved across most metrics from the baseline version because of eliminating allocations.</p>\n<p>The same program now executes in less CPU cycles, requires less instructions. Reducing heap allocations is particularly noticeable as reduced cache-references, cache-misses, branch-instructions and branch-misses.</p>\n<p>Even though the runtime has not changed, the user time metric shows that we have shaved off ~2s (from 28s to under 26s) with these optimizations.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/perf_stats_phase1.png\" alt=\"Perf stats for baseline version up to run 04 \" /></p>\n<p>The individual performance counter metrics have improved, but the IPC (instructions per cycle) has gone down from 1.20 to 1.18. Even so, we are now executing the workload using less CPU instructions and cycles. That counts as an efficiency improvement.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/ipc_trend_phase1.png\" alt=\"IPC trend for baseline version up to run 04\" /></p>\n<h2 id=\"phase-2:-architectural-changes\" tabindex=\"-1\">Phase 2: Architectural Changes <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-2:-architectural-changes\" aria-hidden=\"true\">#</a></h2>\n<p>The lesson learned from the previous optimizations, is that to speed up the pipeline we need to focus our efforts on the most time consuming parts of the execution. The most obvious optimization then, is to increase the write throughput, by adding more writers.</p>\n<p>We can do a lot better here to improve the speed of the pipeline, by exploiting data parallelism. The data generation is parallelized, but the data encoding to Parquet is not. It is great candidate for making parallel because it is also a compute-bound workload which is now single-threaded.</p>\n<p>The size of 10 million rows on disk in Parquet format is ~292MB, and the program takes ~4s to execute. So we know for certain that the writer thread is not I/O bound. We need to be writing an order of magnitude more bytes to disk to saturate the NVME I/O write speeds.</p>\n<p>We can also optimize the data generation to speed up the program. It currently depends on <a href=\"https://github.com/proptest-rs/proptest\">proptest</a> (a property testing library). I reused it instead of rolling my own, because the <a href=\"https://docs.rs/proptest/latest/proptest/strategy/trait.Strategy.html\">Strategy trait</a> provides a nice API for defining data skew for the fields of the nested data structure. From the flamegraph profile it is evident that it does a lot more work than which is strictly needed in our case.</p>\n<h3 id=\"05:-increase-parquet-encoding-bandwidth\" tabindex=\"-1\">05: Increase Parquet Encoding Bandwidth <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#05:-increase-parquet-encoding-bandwidth\" aria-hidden=\"true\">#</a></h3>\n<p>The simplest possible thing to do here is to increase the number of writers from one to two. For that we can partition the data generator threads into two retaining the MPSC pattern. Each partition is connected to a Parquet writer. This effectively doubles the encoding bandwidth, and it requires only a minimal lines of code to be changed.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">    let (tx, rx) = mpsc::sync_channel::&lt;RecordBatch>(num_threads * 2);<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let (tx1, rx1) = mpsc::sync_channel::&lt;RecordBatch>(num_threads);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let (tx2, rx2) = mpsc::sync_channel::&lt;RecordBatch>(num_threads);<br /></span></span><br /><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let writer_handle_1 = create_writer_thread(\"contacts_1.parquet\", rx1);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let writer_handle_2 = create_writer_thread(\"contacts_2.parquet\", rx2);<br /></span></span><br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    let chunk_count = target_contacts.div_ceil(BASE_CHUNK_SIZE);<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    let parquet_schema = get_contact_schema();<br /></span></span><br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    (0..chunk_count)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        .into_par_iter()<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">        .for_each_with(tx, |tx, chunk_index| {<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">        .for_each_with((tx1, tx2), |(tx1, tx2), chunk_index| {<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">            let start_index = chunk_index * BASE_CHUNK_SIZE;<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">            let current_chunk_size = std::cmp::min(BASE_CHUNK_SIZE, target_contacts - start_index);<br /></span></span><br />@@ -263,11 +275,17 @@ fn main() -> Result&lt;(), Box&lt;dyn Error + Send + Sync>> {<br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">                to_record_batch(parquet_schema.clone(), &amp;phone_id_counter, partial_contacts)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">                    .expect(\"Failed to create RecordBatch\");<br /></span></span><br /><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">            tx.send(record_batch).unwrap();<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">            if chunk_index % 2 == 0 {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">                tx1.send(record_batch).unwrap();<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            } else {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">                tx2.send(record_batch).unwrap();<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            }<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        });<br /></span></span><br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    // Teardown<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">    writer_handle.join().unwrap()?;<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    writer_handle_1.join().unwrap()?;<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    writer_handle_2.join().unwrap()?;<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"></span></span></code></pre>\n<p>This shaved off ~1s (3.6s to 2.7s) from execution time. This is significant, and I need to now find if adding more writers will further reduce the runtime. The major gains are not going to come from this it.</p>\n<h3 id=\"06:-make-data-generation-lightweight\" tabindex=\"-1\">06: Make Data Generation Lightweight <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#06:-make-data-generation-lightweight\" aria-hidden=\"true\">#</a></h3>\n<p>The flamegraph profile shows that over 80% of the pipeline execution time is spend in the data generation methods. It is the most dominant factor we need to focus on. Around 20% of that time is spend in <a href=\"https://docs.rs/proptest/latest/proptest/strategy/trait.Strategy.html#tymethod.new_tree\">Strategy::new_tree</a> alone, which is the entry point for data generation.</p>\n<p>There is a lot of performed here which does not contribute to data generation, but is necessary for a test runner. We can eliminate this extra work by implementing light-weight functions, but keeping the ergonomic API design.</p>\n<p>Maybe, I could have done better at the beginning by rolling my own implementation. But the goal at the beginning was to have a correct, simple working implementation. Performance is important, but it will have been pure guesswork if I had predicted that this will so dominant in the runtime. The other reason is I like the ergonomic API design, which I can copy in the light-weight implementation.</p>\n<p>The diff below shows the implementation for the <code>phone_type</code> field in the nested data structure. You can see the structural similarities between the old and new versions of the code.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">// A Zipfian-like categorical distribution for `phone_type`<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">//<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// | Phone Type | Probability |<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// |------------|-------------|<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// | Mobile     | 0.55        |<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// | Work       | 0.35        |<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// | Home       | 0.10        |<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">//<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">fn phone_type_strategy() -> BoxedStrategy&lt;PhoneType> {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    prop_oneof![<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        55 => Just(PhoneType::Mobile),<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        35 => Just(PhoneType::Work),<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        10 => Just(PhoneType::Home),<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    ]<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    .boxed()<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">fn generate_phone_type(rng: &amp;mut impl Rng) -> PhoneType {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    match rng.random_range(0..100) {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        0..=54 => PhoneType::Mobile, // 0.55<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        55..=89 => PhoneType::Work,  // 0.35<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        _ => PhoneType::Home,        // 0.10<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    }<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">}<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"></span></span></code></pre>\n<p>This refactoring has to be applied uniformly for every field and methods which compose nested fields. The diff below shows how the property-testing runner is replaced with a simple for loop. In this refactoring we completely eliminate the <code>proptest</code> dependency.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\">diff --git a/Cargo.lock b/Cargo.lock<br />index 626375c..a6b06a8 100644<br /><span class=\"token coord\">--- a/Cargo.lock</span><br /><span class=\"token coord\">+++ b/Cargo.lock</span><br />@@ -2322,7 +2322,7 @@ dependencies = [<br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\"> \"log\",<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\"> \"parquet\",<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\"> \"parquet-common\",<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\"> \"proptest\",<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\"> \"rand 0.9.1\",<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\"> \"rayon\",<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">]<br /></span></span><br />diff --git a/crates/parquet-parallel-nested/src/main.rs b/crates/parquet-parallel-nested/src/main.rs<br />index f6d8610..0fa2083 100644<br /><span class=\"token coord\">--- a/crates/parquet-parallel-nested/src/main.rs</span><br /><span class=\"token coord\">+++ b/crates/parquet-parallel-nested/src/main.rs</span><br /><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">fn generate_contacts_chunk(size: usize, seed: u64) -> Vec&lt;PartialContact> {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let mut rng = StdRng::seed_from_u64(seed);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let mut contacts = Vec::with_capacity(size);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let mut name_buf = String::with_capacity(32);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    for _ in 0..size {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        contacts.push(generate_partial_contact(&amp;mut rng, &amp;mut name_buf));<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    }<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    contacts<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">}<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\"><br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">fn generate_contacts_chunk(size: usize, seed: u64) -> Vec&lt;PartialContact> {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    let mut runner = TestRunner::new(Config {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        rng_seed: RngSeed::Fixed(seed),<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        ..Config::default()<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    });<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\"><br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    let strategy = proptest::collection::vec(contact_strategy(), size);<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    strategy<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        .new_tree(&amp;mut runner)<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        .expect(\"Failed to generate chunk of partial contacts\")<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        .current()<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">}</span></span></code></pre>\n<p>The pipeline is now 2.3x faster. The total runtime decreased from 2.70s to 1.18s (~1.5s). The IPC (instructions per cycle) nearly doubled, from 1.05 to 2.02. Every other stat shows similar improvements.</p>\n<p>This is a strong result. The program speed increased, and it is also now more efficient in core utilization.</p>\n<h3 id=\"07:-increase-parquet-writer-threads\" tabindex=\"-1\">07: Increase Parquet Writer Threads <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#07:-increase-parquet-writer-threads\" aria-hidden=\"true\">#</a></h3>\n<p>The Parquet encoding step remains a bottleneck as the data generators outpace the two writer threads. A simple test is to see the effect of doubling the writers again.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">    if chunk_index % 2 == 0 {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">            tx1.send(record_batch).unwrap();<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        } else {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">            tx2.send(record_batch).unwrap();<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        }<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    });<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let record_batch =<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">       to_record_batch(parquet_schema.clone(), &amp;phone_id_counter, partial_contac<br /></span></span>ts)<br /><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">       .expect(\"Failed to create RecordBatch\");<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    match chunk_index % 4 {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        0 => s1.send(record_batch).expect(\"Failed to send to rx1\"),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        1 => s2.send(record_batch).expect(\"Failed to send to rx2\"),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        2 => s3.send(record_batch).expect(\"Failed to send to rx3\"),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        _ => s4.send(record_batch).expect(\"Failed to send to rx4\"),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    }</span></span></code></pre>\n<p>Another ~1.5x speedup in runtime. The total runtime dropped below a second (800ms) for the first time. On the other hand, the IPC dropped to 1.76 from the previous high.</p>\n<h3 id=\"08:-introduce-thread-local-state\" tabindex=\"-1\">08: Introduce Thread-Local State <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#08:-introduce-thread-local-state\" aria-hidden=\"true\">#</a></h3>\n<p>The flamegraph profile now shows that around 20% of the time is spend in resizing vectors, and cloning strings in data generation.</p>\n<p>The current data generation is stateless. As soon as a chunk of nested records are created, a <code>RecordBatch</code> is created. And this is immediately send to the writer thread. The chunk size setting is hard-coded as 256. It creates ~39K <code>RecordBatches</code> for 10 million records. We could increase chunk size, but a better thing to do here is decouple the chunk size from the row count at which we finalize a <code>RecordBatch</code>, so that they can be tuned separately.</p>\n<p>For example, if the chunk size is 256, we can configure a <code>RecordBatch</code> to be finalized when we have 5K nested records. Now only ~2K <code>RecordBatch</code>es are created for a run of 10 million records. For this we introduce <code>GeneratorState</code> struct which contains the <code>RecordBatch</code> fields to which we are appending the chunk values.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">struct GeneratorState {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    schema: SchemaRef,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    name: StringBuilder,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    phone_number_buf: String,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    counter: Arc&lt;AtomicUsize>,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    phones: ListBuilder&lt;StructBuilder>,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    current_chunks: usize,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">}<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">enum GeneratorStateError {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    NotEnoughChunks { current: usize, required: usize },<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    TryFlushZeroChunks,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">}<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span></span>@@ -411,38 +351,95 @@ fn main() -> Result&lt;(), Box&lt;dyn Error + Send + Sync>> {<br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        .num_threads(num_producers)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        .build()<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        .unwrap();<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\"><br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    pool.install(|| {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        let chunk_count = target_contacts.div_ceil(BASE_CHUNK_SIZE);<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        let parquet_schema = get_contact_schema();<br /></span></span><br /><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">        (0..num_producers).into_par_iter().for_each(|producer_id| {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            // Each thread gets its own state and a clone of the senders.<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            let mut generator_state =<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">                GeneratorState::new(parquet_schema.clone(), phone_id_counter.clone());</span></span></code></pre>\n<p>This reduces the runtime by another 25% (~800ms to ~600ms). We have also regained IPC and it is at an all time high of 2.21. Every efficiency parameter has improved.</p>\n<p>Yet another significant improvement both in speedup and efficiency of the program's execution.</p>\n<h3 id=\"measuring-impact\" tabindex=\"-1\">Measuring Impact <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#measuring-impact\" aria-hidden=\"true\">#</a></h3>\n<p>We improved the total pipeline throughput by increasing the writers. The data generator got lighter, and faster. Finally, we increased the <code>RecordBatch</code> size.</p>\n<p>All of the above optimizations to the pipeline has resulted in a 6X speedup, with the runtime dropping from 3.61s to 0.58s.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/hyperfine_boxplot_grid_phase2.png\" alt=\"Hyperfine box plots from run 04 to run 08\" /></p>\n<p>The cores are now being utilized more efficiently, with every stats we tracked improving significantly.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/perf_stats_phase2.png\" alt=\"Perf stats from run 04 to run 08\" /></p>\n<p>The IPC improved from 1.18 to 2.21 (an 87% increase).</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/ipc_trend_phase2.png\" alt=\"IPC trend from run 04 to run 08\" /></p>\n<p>The final flamegraph shows a concentrated workload which is evenly divided. The data generator (producer) threads profile occupies the left side, while the parquet writer (consumer) threads profile occupies to the right side. The transition to the final flamegraph, shows a clear improvement from a fragmented hotspots to more efficient pipeline execution.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/flamegraph_montage_phase2.png\" alt=\"Flamegraph montage from run 04 to run 08\" /></p>\n<h2 id=\"phase-3:-a-performance-regression\" tabindex=\"-1\">Phase 3: A Performance Regression <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-3:-a-performance-regression\" aria-hidden=\"true\">#</a></h2>\n<p>The generated <code>name</code> field is <code>NULL</code> 20% of the time. When generating 10 million rows, we therefore expect the <code>name</code> column to contain roughly 8 million name strings. To my surprise, the total no. of unique names were only ~1.4 million.</p>\n<p>When generating millions of <code>fake</code> names, the chance of a collision becomes very high. The <code>fake</code> implementation is most likely sampling first name, last name pairing with replacement. Since we are generating a large number of names, the collisions become more frequent. This is also known as the <a href=\"https://en.wikipedia.org/wiki/Birthday_problem\">Birthday problem</a>.</p>\n<p>It looks like we can minimize string allocations by 82.5% because only ~1.4 million unique names are generated for a run of total size 10 million.</p>\n<h3 id=\"09:-global-string-interning\" tabindex=\"-1\">09: Global String Interning <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#09:-global-string-interning\" aria-hidden=\"true\">#</a></h3>\n<p>When a new name is generated, we check to see if it is unique in a global hashmap. If it is new and unique it is added to the hashmap. If it already exists in the hashmap, we reuse the allocated string stored within the hashmap.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">  let name = Some(name_buf.clone());<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">  let name = if let Some(interned) = interner.get(name_buf) {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      interned.clone()<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">  } else {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      let new_arc = Arc::new(name_buf.clone());<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      interner.insert(name_buf.clone(), new_arc.clone());<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      new_arc<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">  };<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">  // clear the name buffer for next use<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">  name_buf.clear();<br /></span></span><br /><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">  name<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">  Some(name)</span></span></code></pre>\n<p>The savings are realized when we finally add the generated name when constructing the <code>RecordBatch</code>.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">            name.append_option(generate_name(rng, &amp;mut name_buf));<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">            name.append_option(generate_name(rng, &amp;mut name_buf, &amp;self.interner).as_deref());</span></span></code></pre>\n<p>An extra allocation is avoided by directly passing a reference to the interned string.</p>\n<h3 id=\"10:-revert\" tabindex=\"-1\">10: Revert <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#10:-revert\" aria-hidden=\"true\">#</a></h3>\n<p>The benchmarks shows a runtime regression of 65%, from 583ms to 965ms. The IPC halved from 2.21 to 1.10. The cache misses increased to 22% from 10%. The measurements leaves no doubt, this is a clear performance regression.</p>\n<p>But why did string interning not work?</p>\n<p>The data generator threads are generating at a rapid pace, and the names are being inserted into the same internal buckets. These may fit into a cache line, and shared across cores. But the high volume writes are constantly invalidating the cache lines, and this points to a cache coherency issue. A data generator thread can invalidate the cache line when another data generator thread is attempting to access the same cache line to get a reference to the interned string. This causes the CPU to stall, and wait for the new cache line to be fetched. Both problems are visible in the lower IPC and higher cache miss rate.</p>\n<p>While reverting the code I noticed that the earlier version, was cloning the name string buffer before passing it the <code>name</code> field builder. This is not necessary, as we can pass the reference directly without cloning, as Arrow will make a copy internally. So an extra clone was removed in the end.</p>\n<p>The median runtime is now 533ms from 584ms, shaving off another 51ms from the final runtime.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/hyperfine_boxplot_grid_phase3.png\" alt=\"Hyperfine box plots from run 08 to run 10\" /></p>\n<p>The changes in hardware performance counters are negligible in most cases, but the total instructions, and branch instructions have reduced significantly. This could be attributed to removing the unnecessary cloning of the mutable buffer.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/perf_stats_phase3.png\" alt=\"Perf stats from run 08 to run 10\" /></p>\n<h2 id=\"a-back-of-the-envelope-estimation\" tabindex=\"-1\">A Back of the Envelope Estimation <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#a-back-of-the-envelope-estimation\" aria-hidden=\"true\">#</a></h2>\n<p>The pipeline throughput is dependent on its slowest stage. In micro-benchmarks, where the data generator and writer throughput is measured on a single core, the performance is comparable.</p>\n<table>\n<thead>\n<tr>\n<th>Record Batch Size</th>\n<th>Data Generation Time per Record (ns)</th>\n<th>Write Time per Record (ns)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1024</td>\n<td>140.89</td>\n<td>135.30</td>\n</tr>\n<tr>\n<td>2048</td>\n<td>145.76</td>\n<td>133.05</td>\n</tr>\n<tr>\n<td>4096</td>\n<td>149.08</td>\n<td>131.51</td>\n</tr>\n<tr>\n<td>8192</td>\n<td>149.66</td>\n<td>129.82</td>\n</tr>\n</tbody>\n</table>\n<p>Therefore for this workload, the optimal ratio of producers to consumers is close to 1:1. On the 16-core machine used for end-end benchmarking, we should see the best performance when using 8 generator threads paired together with 8 writer threads.</p>\n<p>From the table we can calculate, the theoretical ceiling for throughput which comes to ~7 million records/second/core. Our highest observed throughput value is 21 millions/records/second on 16 cores with 8-writers, which is ~2.6 million records/second/core.</p>\n<p>On 16-cores, the user time is 4.82s and system time is 0.71s for a combined 5.53s. The total wall clock time is 0.51s. This means we are effectively using 11 cores (5.53s / 0.51s) of the total available 16 cores. This is a high-level of parallelism, where for the duration of the pipelines execution 11 cores are fully busy.</p>\n<p>For future improvements, the single-core efficiency has to be improved, but where?</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/flamegraph-8w-8192rb.png\" alt=\"Flamegraph of latest version after all optimizations are applied\" /></p>\n<p>The flamegraph shows a equal split of work distribution between the data generation threads and writer threads. The next bottleneck appears as the overhead of Rayon in dividing the work between the data generation threads. The data generator is extremely fast that the overhead of distributing work is greater.</p>\n<p>For a run of 10 million rows, with a record batch size of 8192, we now generate 1221 small batches for Rayon to distribute in parallel to cores running the data generator threads. A single batch completes in 1.2ms, so Rayon has to do constantly schedule tasks to cores at the rate of 833 tasks/second. The next optimization should target reducing this overhead.</p>\n<h2 id=\"fine-tuning-configuration-for-optimal-performance\" tabindex=\"-1\">Fine-tuning Configuration for Optimal Performance <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#fine-tuning-configuration-for-optimal-performance\" aria-hidden=\"true\">#</a></h2>\n<p>Keeping the target rows constant at 10 million, we can execute the pipeline a range of record batch sizes, writer threads to compare record throughput (million records/second) and total runtime.</p>\n<pre class=\"language-text\"><code class=\"language-text\">$ ./target/release/parquet-nested-parallel --help<br />A tool for generating and writing nested Parquet data in parallel<br /><br />Usage: parquet-nested-parallel [OPTIONS]<br /><br />Options:<br />      --target-records <TARGET_RECORDS><br />          The target number of records to generate [default: 10000000]<br />      --record-batch-size <RECORD_BATCH_SIZE><br />          The size of each record batch [default: 4096]<br />      --num-writers <NUM_WRITERS><br />          The number of parallel writers [default: 4]<br />      --output-dir <OUTPUT_DIR><br />          The output directory for the Parquet files<br />      --output-filename <OUTPUT_FILENAME><br />          The base filename for the output Parquet files<br />      --dry-run<br />          Do not execute the pipeline<br />  -h, --help<br />          Print help<br />  -V, --version<br />          Print version</OUTPUT_FILENAME></OUTPUT_DIR></NUM_WRITERS></RECORD_BATCH_SIZE></TARGET_RECORDS></code></pre>\n<p>In the micro-benchmark comparison from earlier, the single-core performance of the data generator and writer threads are comparable. So a best performance was predicted to come from a 1:1 allocation of CPU cores between the data generation and writer threads.</p>\n<p>Below we can see that 8 writers, 8 producers produces the highest observed throughput of 23 million records/second. A higher record batch size has little to no effect beyond 10K. With 6 writers, 10 producers, the throughput is above 20 million record/second, but is not optimal because of the imbalanced allocation.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/performance_heatmap_10M_record_throughput_m_sec.png\" alt=\"Record Throughput Analysis\" /></p>\n<p>No surprises here as well. The lowest recorded runtime is when we have a 1:1 allocation, with 8 writers and 8 producer threads.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/performance_heatmap_10M_total_time_ms.png\" alt=\"Total Runtime Analysis\" /></p>\n<p>To recap, future optimizations should target reducing the Rayon threadpool overhead to improve single-core efficiency. The current pipeline achieves a high-level of parallelism by being able to fully utilize 11 out of 16 available cores. There is close to ~3X headroom remaining for improving the current ~2.6 million records/per/core throughput, to a maximum possible ~7 million records/per/core throughput. Since the bottleneck changes after each optimization, it is therefore important to continue with a data-driven approach.</p>\n",
      "date_published": "2025-09-02T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/",
      "url": "https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/",
      "title": "Practical Hurdles In Crab Latching Concurrency",
      "content_html": "<p>An implementation of the crab latching protocol enforces a strict top-down order for acquiring latches on a B+Tree node. This avoid deadlocks from ever occurring during concurrent operations. This is distinct from deadlock detection and resolution which is a runtime mechanism.</p>\n<p>Deadlock avoidance is guaranteed by design through careful programming of critical sections in the code. Any mistakes here will result in deadlocks. Even worse, a data race which silently corrupts the index.</p>\n<p>The main strength of a B+Tree index (compared to a hash index) is its unique capability to perform range scans. This is possible because all the entries are stored in key lexicographic order in the leaf nodes, and the leaf nodes themselves are connected to each other like a doubly-linked list. So scanning is efficient once you locate the starting leaf node. Scanning in ascending or descending key order is as simple as following the left or right sibling pointers.</p>\n<p>This forwards or backwards movement during index scans violates the strict top-down ordering required for safety and correctness by the crab latching protocol.</p>\n<p>A delete algorithm which implements a symmetrical tree rebalancing procedure requires acquiring an exclusive latch on either a left or right sibling for merging nodes. There is an equal chance of nodes merging left-right and right-left. This too violates the strict ordering requirement.</p>\n<p>Therefore, an implementation has to come up with practical methods to avoid serial execution order and preserve concurrency. There is no formal verification of correctness via proof in these scenarios. We can improve our confidence in the implementation through engineering effort: code reviews, test suites, analyzers (ThreadSanitizer). Though the existence of data races cannot be ruled out, in practice this is sufficient for a robust and reliable implementation as evidenced by major OLTP systems.</p>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#an-overview-of-crab-latching\">An Overview Of Crab Latching</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#how-do-deadlocks-happen\">How Do Deadlocks Happen?</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#how-are-deadlocks-prevented\">How Are Deadlocks Prevented?</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#efficient-fine-grained-crab-latching\">Efficient Fine-Grained Crab Latching</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#concurrent-index-scans\">Concurrent Index Scans</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#extension-for-concurrent-bi-directional-scans\">Extension For Concurrent Bi-directional Scans</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#deadlock:-lock-order-inversion\">Deadlock: Lock Order Inversion</a></li>\n      </ul>\n    </li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#extension-for-symmetric-deletion\">Extension For Symmetric Deletion</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#concurrent-scans-can-miss-entries\">Concurrent Scans Can Miss Entries</a></li>\n  </ol>\n</nav>\n<h2 id=\"an-overview-of-crab-latching\" tabindex=\"-1\">An Overview Of Crab Latching <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#an-overview-of-crab-latching\" aria-hidden=\"true\">#</a></h2>\n<blockquote>\n<p>Latches are held only during a critical section, that is, while a data structure is read or updated. Deadlocks are avoided by appropriate coding disciplines, for example, requesting multiple latches in carefully designed sequences. Deadlock resolution requires a facility to rollback prior actions, whereas deadlock avoidance does not. Thus, deadlock avoidance is more appropriate for latches, which are designed for minimal overhead\nand maximal performance and scalability. Latch acquisition and release may\nrequire tens of instructions only, usually with no additional cache faults since a latch can be embedded in the data structure it protects.</p>\n<p>Goetz Graefe, &quot;A Survey of B-Tree Locking Techniques&quot; (2010)</p>\n</blockquote>\n<h3 id=\"how-do-deadlocks-happen\" tabindex=\"-1\">How Do Deadlocks Happen? <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#how-do-deadlocks-happen\" aria-hidden=\"true\">#</a></h3>\n<p>(<code>Thread 1</code>) Holds an exclusive (write) latch on <code>Node P</code>. It now wants to acquire an exclusive latch on it's child <code>Node C</code> for inserting an element.</p>\n<p>(<code>Thread 2</code>) Already holds an exclusive latch on <code>Node C</code>. It is waiting to acquire an exclusive latch on it's parent <code>Node P</code>, so that the pivot key can be updated.</p>\n<p>This creates a deadlock, where neither threads can make any progress. This could have been prevented if a strict ordering of the direction in which latches are acquired existed. A top-down ordering is better because all traversals begin at the root node to reach a leaf node.</p>\n<h3 id=\"how-are-deadlocks-prevented\" tabindex=\"-1\">How Are Deadlocks Prevented? <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#how-are-deadlocks-prevented\" aria-hidden=\"true\">#</a></h3>\n<p>The ordering requirement implies that only a parent node can acquire an exclusive latch on a child node. The implementation of <code>Thread 2</code> becomes an invalid state and should not be possible. So instead the exclusive latch on <code>Node P</code> is never released when traversing down to the child <code>Node C</code>. Since only one writer can hold the exclusive (write) latch at a time, this will not create a deadlock with <code>Thread 1</code>. The first thread to acquire the exclusive latch on <code>Node P</code>, will block the second thread.</p>\n<p>Even though latches are light-weight and held only for a short duration of time, it is not a good idea to hold latches which are strictly not necessary. It will create contention in hot paths which is bad news for throughput. This is where the crab latching protocol shines with its efficiency.</p>\n<h3 id=\"efficient-fine-grained-crab-latching\" tabindex=\"-1\">Efficient Fine-Grained Crab Latching <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#efficient-fine-grained-crab-latching\" aria-hidden=\"true\">#</a></h3>\n<p>In crab latching, a child node is considered &quot;unsafe&quot; if the current operation will cause it to either split (overflow) or merge (underflow). An &quot;unsafe&quot; node may also end up modifying it's parent like <code>Thread 2</code>. So exclusive latches are held for the entire path segment, which contains &quot;unsafe&quot; nodes.</p>\n<p>But we know that a node split or merge is going to be rare. More often an insert or delete will be local to a leaf node and does not have cascading changes which recurse back to the root node. Such nodes are considered to be &quot;safe&quot;</p>\n<p>In the common scenario, when crab latching sees that a child node is &quot;safe&quot;, it first acquires a shared (read-only) latch on the child node and then release it's shared (read-only) latch on the parent. Hence the &quot;crab latching&quot; terminology. A shared latch does not block other threads. An exclusive latch is only acquired on the leaf node at the time of an insertion or deletion.</p>\n<p>This fine-grained latching is efficient and allows other concurrent readers and only makes writers to wait, improving overall throughput. This is the optimistic approach which assumes will happen in the general case. We fallback to the pessimistic approach only if leaf node will underflow or overflow after an operation.</p>\n<h2 id=\"concurrent-index-scans\" tabindex=\"-1\">Concurrent Index Scans <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#concurrent-index-scans\" aria-hidden=\"true\">#</a></h2>\n<p>The fundamental problem is that the concurrency models do not take into consideration B+Tree iterators. At the leaf node, traversing to a sibling uses the bi-directional links between leaf nodes. An ascending scan moves from left-right, while a descending scan moves from right-left. This conflicts with the safety property for avoiding deadlocks that traversals have a strictly enforced direction. Following the protocol exactly means the implementation can provide only one type of scan, either forward (ascending) or reverse (descending), but never both together.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// A forward index scan</span><br /><span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">auto</span> iter <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">Begin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> iter <span class=\"token operator\">!=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">End</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>iter<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">int</span> key <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">int</span> value <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>second<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// A reverse index scan</span><br /><span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">auto</span> iter <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">RBegin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> iter <span class=\"token operator\">!=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">REnd</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token operator\">--</span>iter<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">int</span> key <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">int</span> value <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>second<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">/**<br /> * index.Begin()  : first element<br /> * index.End()    : one-past-the-last element<br /> * index.RBegin() : last element<br /> * index.REnd()   : one-past-the-first element<br /> */</span></code></pre>\n<h3 id=\"extension-for-concurrent-bi-directional-scans\" tabindex=\"-1\">Extension For Concurrent Bi-directional Scans <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#extension-for-concurrent-bi-directional-scans\" aria-hidden=\"true\">#</a></h3>\n<p>A shared (read-only) or exclusive (write) latch blocks execution until the latch is acquired. For scans which are sideways traversals, we do not want to limit the traversal to any one direction. A blocking latch will create deadlocks if two concurrent scans proceed in opposite directions.</p>\n<p>Therefore we need to use a non-blocking latch to prevent blocking and avoid deadlocks. A non-blocking latch will try to acquire a latch, and will return immediately.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;shared_mutex></span></span><br /><br /><span class=\"token keyword\">class</span> <span class=\"token class-name\">SharedLatch</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// Blocking</span><br />  <span class=\"token comment\">//</span><br />  <span class=\"token comment\">// If another thread holds the latch, execution will block</span><br />  <span class=\"token comment\">// until the latch is acquired.</span><br />  <span class=\"token comment\">//</span><br />  <span class=\"token comment\">// Used in insert, delete &amp; search index operations</span><br />  <span class=\"token keyword\">void</span> <span class=\"token function\">LockShared</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />      latch_<span class=\"token punctuation\">.</span><span class=\"token function\">lock_shared</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token comment\">// Non-blocking</span><br />  <span class=\"token comment\">//</span><br />  <span class=\"token comment\">// Tries to acquire a latch. If successful returns `true`,</span><br />  <span class=\"token comment\">// otherwise returns `false`.</span><br />  <span class=\"token comment\">//</span><br />  <span class=\"token comment\">// Used in ascending &amp; descending index scans</span><br />  <span class=\"token keyword\">bool</span> <span class=\"token function\">TryLockShared</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token keyword\">return</span> latch_<span class=\"token punctuation\">.</span><span class=\"token function\">try_lock_shared</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token keyword\">private</span><span class=\"token operator\">:</span><br />    <span class=\"token comment\">// A wrapper around std::shared_mutex</span><br />    std<span class=\"token double-colon punctuation\">::</span>shared_mutex latch_<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Using <code>TryLockShared()</code> forces us rethink how a scan implementation should work if latch acquisition fails. In contrast, the concurrent insert, delete and search implementations are always expected to return a result matching its output type in the signature.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Returns `std::nullopt` if the key is not found in the index</span><br />std<span class=\"token double-colon punctuation\">::</span>optional<span class=\"token operator\">&lt;</span>KeyType<span class=\"token operator\">></span> <span class=\"token function\">MaybeGet</span><span class=\"token punctuation\">(</span>KeyType key<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token comment\">// Returns `false` if key is a duplicate.</span><br /><span class=\"token comment\">//</span><br /><span class=\"token comment\">// Note: This prevents overwriting an existing key. The handling of</span><br /><span class=\"token comment\">// duplicate keys is an implementation specific detail.</span><br /><span class=\"token keyword\">bool</span> <span class=\"token function\">Insert</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> KeyValuePair element<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token comment\">// Returns `false` if the key does not exist.</span><br /><span class=\"token keyword\">bool</span> <span class=\"token function\">Delete</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> KeyType key_to_remove<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>We can introduce a failure mode, where if latch acquisition fails during a scan we set its internal state to <code>RETRY</code>.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">enum</span> <span class=\"token class-name\">IteratorState</span> <span class=\"token punctuation\">{</span><br />    VALID<span class=\"token punctuation\">,</span> INVALID<span class=\"token punctuation\">,</span> END<span class=\"token punctuation\">,</span> REND<span class=\"token punctuation\">,</span> RETRY<br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre>\n<p>The implementation for forward scan which uses a non-blocking latch and retriable iterator looks like this:</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Move forward one element at a time. If latch acquisition</span><br /><span class=\"token comment\">// failed, then set internal state to `RETRY`.</span><br /><span class=\"token keyword\">void</span> <span class=\"token keyword\">operator</span><span class=\"token operator\">++</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// ...</span><br /><br />  <span class=\"token comment\">// The `TrySharedLock()` is a non-blocking read-only latch</span><br />  <span class=\"token comment\">// which returns `true` or `false` immediately.</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">!</span><span class=\"token punctuation\">(</span>current_node_<span class=\"token operator\">-></span><span class=\"token function\">TrySharedLock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    previous_node<span class=\"token operator\">-></span><span class=\"token function\">ReleaseNodeSharedLatch</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />    <span class=\"token comment\">// Invalidates the iterator.</span><br />    <span class=\"token comment\">// Sets internal state to `RETRY`.</span><br />    <span class=\"token function\">SetRetryIterator</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    <span class=\"token keyword\">return</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token comment\">// ...</span><br /><br /><span class=\"token punctuation\">}</span><br /><br /><br /><span class=\"token keyword\">void</span> <span class=\"token function\">SetRetryIterator</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// Resets internal state</span><br />  current_node_ <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span><br />  current_element_ <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span><br /><br />  state_ <span class=\"token operator\">=</span> RETRY<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>We have a working bi-directional iterator implementation which will avoid deadlocks, but it is not yet free from data races.</p>\n<h3 id=\"deadlock:-lock-order-inversion\" tabindex=\"-1\">Deadlock: Lock Order Inversion <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#deadlock:-lock-order-inversion\" aria-hidden=\"true\">#</a></h3>\n<p>The current API introduces a deadlock if the user initializes two iterators within the same scope, within the same thread.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">auto</span> iter_forward <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">Begin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token comment\">// The second iterator creates a lock order inversion.</span><br /><span class=\"token keyword\">auto</span> iter_reverse <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">RBegin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>A deadlock is prevented by enforcing a strict direction for latching. Any concurrent operation must therefore acquire a latch on an ancestor node before acquiring a latch on a descendant node (top-down traversal).</p>\n<p>The iterator here holds a shared (read-only) latch on the leaf it points to. If that iterator remains alive while another operation begins a new top-down traversal from the root, we can get a deadlock.</p>\n<p>(<code>Thread 1</code>): Creates a forward iterator, which holds a shared latch on a leaf node.</p>\n<p>(<code>Thread 2</code>): Begins an insert in the pessimistic path acquiring an exclusive latch beginning at the root node all the way down to the parent of the same leaf node.</p>\n<p>(<code>Thread 2</code>): Now attempts to acquire an exclusive latch on the leaf node but it blocks, waiting for the forward iterator to complete and release its shared latch on the leaf node.</p>\n<p>(<code>Thread 1</code>): Creates a second reverse iterator, which is now blocking to acquire a shared lock on the root. It is waiting for the insert operation to release the exclusive latch.</p>\n<p>This creates a deadlock, even though in implementation we enforced a strict ordering. We can ensure that this does not happen by ensuring that iterator lifetimes do not intersect each other, by introducing a local scope.</p>\n<p>Most importantly, the shared latch is released at the end of the scope and therefore it also enforces a global ordering for latch acquisition and will not result in a deadlock like described above.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">auto</span> iter_forward <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">Begin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// The lifetime of the first iterator ends before this</span><br /><span class=\"token comment\">// scope starts. This guarantees that the shared latch</span><br /><span class=\"token comment\">// on the leaf node is released before we start traversal</span><br /><span class=\"token comment\">// from the root node.</span><br /><span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">auto</span> iter_reverse <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">RBegin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>To ensure safety, the latching protocol has to be enforced for concurrent operations within the same thread. Unfortunately, our non-blocking, retriable concurrent scan iterators has introduced an API which is easy for the user to incorrectly implement, and must come with warnings.</p>\n<p>The pattern of creating two iterators in the same scope creates a lock-order-inversion within a single thread. While this does not create a deadlock by itself, because of shared latches, it creates the precondition for a deadlock with any concurrent operation which falls down the pessimistic concurrency path.</p>\n<h2 id=\"extension-for-symmetric-deletion\" tabindex=\"-1\">Extension For Symmetric Deletion <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#extension-for-symmetric-deletion\" aria-hidden=\"true\">#</a></h2>\n<p>A symmetric delete algorithm will proceed with a tree rebalancing operation after an underflow by attempting to merge with either the left sibling, and if that doesn't work with the right sibling at any level in the B+Tree. This violates our strict ordering principle for acquiring latches, because a merge can proceed in either direction.</p>\n<p>(<code>Thread 1</code>): operating on Node B, needs to merge with its previous sibling, Node A. It holds an exclusive latch on B and tries to acquire an exclusive latch on A. (left to right)</p>\n<p>(<code>Thread 2</code>): operating on Node A, needs to merge with its next sibling, Node B. It holds an exclusive latch on A and tries to acquire an exclusive latch on B. (right to left)</p>\n<p>The creates a deadlock.</p>\n<p>The fix is to enforce a strict one-way (say left-to-right) latch acquisition order. This is a delicate operation. Before locking the previous sibling A, the exclusive lock on the current node is released. The locks are reacquired in the correct order. First on the previous sibling, then on the current node.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><br />  <span class=\"token keyword\">auto</span> maybe_previous <span class=\"token operator\">=</span> <span class=\"token generic-function\"><span class=\"token function\">static_cast</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span>InnerNode <span class=\"token operator\">*</span><span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span>parent<span class=\"token punctuation\">)</span><span class=\"token operator\">-></span><span class=\"token function\">MaybePreviousWithSeparator</span><span class=\"token punctuation\">(</span>key_to_remove<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>maybe_previous<span class=\"token punctuation\">.</span><span class=\"token function\">has_value</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br /><br />    <span class=\"token comment\">/* ... */</span><br /><br />    <span class=\"token comment\">// Enforcing a strict left-to-right (one-way) ordering</span><br />    right<span class=\"token operator\">-></span><span class=\"token function\">ReleaseNodeExclusiveLatch</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    left<span class=\"token operator\">-></span><span class=\"token function\">GetNodeExclusiveLatch</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    right<span class=\"token operator\">-></span><span class=\"token function\">GetNodeExclusiveLatch</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />    <span class=\"token comment\">/* ... */</span><br />  <span class=\"token punctuation\">}</span></code></pre>\n<h2 id=\"concurrent-scans-can-miss-entries\" tabindex=\"-1\">Concurrent Scans Can Miss Entries <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#concurrent-scans-can-miss-entries\" aria-hidden=\"true\">#</a></h2>\n<p>Earlier in the case of concurrent scans we saw how local reasoning of strict ordering is not sufficient, because of the extension. A global strict ordering has to be enforced to prevent unintended deadlocks from interaction of scans with other concurrent operations.</p>\n<p>Now let us look at a scenario where both the extensions can interact in a manner which violates strict ordering and introduces lock order inversion bugs in subtle ways.</p>\n<p>(<code>Thread 1</code>): A forward scan iterator has completed scanning entries in Node A, and is next going to process it's right sibling Node B.</p>\n<p>(<code>Thread 2</code>): A delete operation has taken the pessimistic path, because Node B is going to underflow after removing this entry. So it decides to merge with left sibling Node A. It already holds an exclusive latch on Node B and has to acquire an exclusive latch on Node B.</p>\n<p>If the forward scan operator releases its shared latch on Node A, before it acquires a shared latch on Node B, the following sequence of events can happen with the right timing of events.</p>\n<p>(<code>Thread 1</code>): Releases latch on Node A before acquiring a latch on Node B.</p>\n<p>(<code>Thread 2</code>): Release exclusive latch on Node B. Acquires exclusive latch on Node A. Acquires the exclusive latch back on Node B.</p>\n<p>(<code>Thread 1</code>): Is blocked by the exclusive latch held on Node B, by the delete operation in <code>Thread 2</code>.</p>\n<p>(<code>Thread 2</code>): Merges Node A and B together, and modifies the right sibling pointer to Node C.</p>\n<p>(<code>Thread 1</code>): Finally acquires a latch on Node C which is the new right sibling on Node A, not realizing it missed node B completely.</p>\n<p>If latches acquisition is not crafted carefully in the iterator code, the following situation creates a race condition which is very hard to even detect, or reproduce.</p>\n<p>The fix here is to correctly implement crab latching in the iterator code. A shared lock should not be released before <code>TrySharedLock()</code> is attempted on the sibling node. Doing so results in race conditions which are impossible to track down. This requires careful programming discipline.</p>\n",
      "date_published": "2025-08-21T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/",
      "url": "https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/",
      "title": "Cache-Friendly B+Tree Nodes With Dynamic Fanout",
      "content_html": "<p>For a high-performance B+Tree, the memory layout of each node must be a single contiguous block. This improves locality of reference, increasing the likelihood that the node's contents reside in the CPU cache.</p>\n<p>In C++, achieving this means forgoing the use of <code>std::vector</code>, as it introduces a layer of indirection through a separate memory allocation. The solution to this problem though inevitably increases the implementation complexity and is mired with hidden drawbacks. Nevertheless, this is still a necessary trade-off for unlocking high performance.</p>\n<pre class=\"language-text\"><code class=\"language-text\">  +----------------------+<br />  | Node Metadata Header |<br />  +----------------------+<br />  | node_type_           |<-- Inner Node or Leaf Node<br />  | max_size_            |<-- Maximum Capacity (aka Node Fanout)<br />  | node_latch_          |<-- RW-Lock Mutex<br />  | iter_end_            |--------------------+<br />  +----------------------+                    |<br />  | Node Data            |                    |<br />  +----------------------+                    |<br />  | entries_[0]          | <--+               |<br />  | entries_[1]          |    |               |<br />  | entries_[2]          |    + used space    |<br />  | ...                  |    |               |<br />  | entries_[k]          | <--+               |<br />  +----------------------+<-------------------+ iter_end_ points to<br />  |                      |    entries_[k+1], which is one-past-the-last<br />  | (unused space)       |    entry in the node.<br />  | ...                  |<br />  +----------------------+</code></pre>\n<figcaption>Fig 1. Memory Layout of a B+Tree Node as a single contiguous block in heap</figcaption>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#challenges\">Challenges</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#the-struct-hack\">The Struct Hack</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#b+tree-node-declaration\">B+Tree Node Declaration</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#raw-memory-buffer\">Raw Memory Buffer</a></li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#the-price-of-fine-grained-control\">The Price Of Fine-Grained Control</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#manual-handling-of-deallocation\">Manual Handling Of Deallocation</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#adding-new-members-in-a-derived-class\">Adding New Members In A Derived Class</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#reinventing-the-wheel\">Reinventing The Wheel</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#hidden-data-type-assumptions\">Hidden Data Type Assumptions</a></li>\n      </ul>\n    </li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#conclusion\">Conclusion</a></li>\n  </ol>\n</nav>\n<h2 id=\"challenges\" tabindex=\"-1\">Challenges <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#challenges\" aria-hidden=\"true\">#</a></h2>\n<p>Using <code>std::vector</code> for a B+Tree node's entries is a non-starter. A <code>std::vector</code> object holds a pointer to its entries which are stored in a separate block of memory on the heap. This indirection fragments the memory layout, forcing us to fall back on C-style arrays for a contiguous layout when storing variable-length node entries.</p>\n<p>This leads to a dilemma. The size of the array must be known at compilation time, yet we need to allow users to configure the fanout (the array's size) at runtime. Furthermore, the implementation should allow inner nodes and leaf nodes to have different fanouts.</p>\n<p>This isn't just a B+Tree problem. It is a common challenge in systems programming whenever an object needs to contain a variable-length payload whose size is only known at runtime. How can you define a class that occupies a single block of memory when a part of the block has a dynamic size?</p>\n<p>The solution isn't obvious, but it's a well-known trick that systems programmers have used for decades, a technique so common it has eventually been standardized in C99.</p>\n<h2 id=\"the-struct-hack\" tabindex=\"-1\">The Struct Hack <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#the-struct-hack\" aria-hidden=\"true\">#</a></h2>\n<p>The solution to this problem is a technique originating in C programming known as the struct hack. The variable-length member (array) is placed at the last position in the struct. To satisfy the compiler an array size of one is hard-coded, ensuring the array size is known at compilation time.</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">struct</span> <span class=\"token class-name\">Payload</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">/* Header Section */</span><br />  <span class=\"token comment\">// ...</span><br /><br />  <span class=\"token comment\">/* Data Section */</span><br /><br />  <span class=\"token comment\">// The variable-length member is in last position.</span><br />  <span class=\"token comment\">// The size `1` satisfies the compiler.</span><br />  <span class=\"token keyword\">char</span> elements<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>At runtime, when the required size <code>N</code> is known, you allocate a single block of memory for the struct and the <code>N</code> elements combined. The compiler treats this as an opaque block, and provides no safety guarantees. However, accessing the extra allocated space is safe because the variable-length member is the final field in the struct.</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token comment\">// The (N - 1) adjusts for the 1-element array in Payload struct</span><br />Payload <span class=\"token operator\">*</span>item <span class=\"token operator\">=</span> <span class=\"token function\">malloc</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span>Payload<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>N <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">char</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre>\n<p>This pattern was officially standardized in C99, where it is known as a <a href=\"https://en.wikipedia.org/wiki/Flexible_array_member\">flexible array member</a>.</p>\n<p>The C++11 standard formally incorporates the flexible array member, referring to it as an <strong>array of unknown bound</strong> when it is the last member of a struct.</p>\n<blockquote>\n<p><strong>Arrays of unknown bound</strong></p>\n<p>If <code>expr</code> is omitted in the declaration of an array, the type declared is &quot;array of unknown bound of T&quot;, which is a kind of incomplete type, ...</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">extern</span> <span class=\"token keyword\">int</span> x<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>      <span class=\"token comment\">// the type of x is \"array of unknown bound of int\"</span><br /><span class=\"token keyword\">int</span> a<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span> <span class=\"token comment\">// the type of a is \"array of 3 int\"</span></code></pre>\n</blockquote>\n<p>This means that in C++, the size can be omitted from the final array declaration (e.g. <code>entries_[]</code>), and the code will compile, enabling the same pattern.</p>\n<h2 id=\"b+tree-node-declaration\" tabindex=\"-1\">B+Tree Node Declaration <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#b+tree-node-declaration\" aria-hidden=\"true\">#</a></h2>\n<p>Using the flexible array member syntax, we can now declare a B+Tree node with a memory layout which is a contiguous single block in the heap.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">template</span> <span class=\"token operator\">&lt;</span><span class=\"token keyword\">typename</span> <span class=\"token class-name\">KeyType</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">typename</span> <span class=\"token class-name\">ValueType</span><span class=\"token operator\">></span><br /><span class=\"token keyword\">class</span> <span class=\"token class-name\">BPlusTreeNode</span> <span class=\"token punctuation\">{</span><br /><span class=\"token keyword\">public</span><span class=\"token operator\">:</span><br />  <span class=\"token keyword\">using</span> KeyValuePair <span class=\"token operator\">=</span> std<span class=\"token double-colon punctuation\">::</span>pair<span class=\"token operator\">&lt;</span>KeyType<span class=\"token punctuation\">,</span> ValueType<span class=\"token operator\">></span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token keyword\">private</span><span class=\"token operator\">:</span><br />  <span class=\"token comment\">// Node Header Members ... (elided)</span><br /><br />  <span class=\"token comment\">// Points to the memory location beyond the last key-value</span><br />  <span class=\"token comment\">// entry in the `entries_` array.</span><br />  KeyValuePair<span class=\"token operator\">*</span> iter_end_<span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// Array containing key-value entries of unknown bound.</span><br />  KeyValuePair entries_<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre>\n<p>Using a <code>std::vector&lt;KeyValuePair&gt;</code> for the node's entries would result in an indirection. This immediately fragments the memory layout. Accessing an entry within a node is slower, and has higher latency because of the pointer indirection. Chasing the pointer increases the probability of a cache miss, which will force the CPU to stall while it waits for the cache line to be fetched from a different region in main memory.</p>\n<p>A cache miss will cost hundreds of CPU cycles compared to just a few cycles for a cache hit. This cumulative latency is unacceptable for any high-performance data structure.</p>\n<p>This technique avoids the pointer indirection and provides fine-grained control over memory layout. The node header and data are co-located in one continuous memory block. This layout is cache-friendly and will result in fewer cache misses.</p>\n<h2 id=\"raw-memory-buffer\" tabindex=\"-1\">Raw Memory Buffer <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#raw-memory-buffer\" aria-hidden=\"true\">#</a></h2>\n<p>This is the key step. The construction of the object has to be separate from its memory allocation. We cannot therefore use the standard <code>new</code> syntax which will attempt to allocate storage, and then initialize the object in the same storage.</p>\n<p>Instead, we use the <a href=\"https://en.cppreference.com/w/cpp/language/new.html#Placement_new\">placement new</a> syntax which only constructs an object in a preallocated memory buffer provided by us. We know exactly how much space to allocate, which is information the standard <code>new</code> operator does not have in this scenario because of the flexible array member.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// A static helper to allocate storage for a B+Tree node.</span><br /><span class=\"token keyword\">static</span> BPlusTreeNode <span class=\"token operator\">*</span><span class=\"token function\">Get</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> p_fanout<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// calculate total buffer size</span><br />  size_t buf_size <span class=\"token operator\">=</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span>BPlusTreeNode<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> p_fanout <span class=\"token operator\">*</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span>KeyValuePair<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// allocate raw memory buffer</span><br />  <span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>buf <span class=\"token operator\">=</span> <span class=\"token double-colon punctuation\">::</span><span class=\"token keyword\">operator</span> <span class=\"token keyword\">new</span><span class=\"token punctuation\">(</span>buf_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// construct B+Tree node object in the preallocated buffer</span><br />  <span class=\"token keyword\">auto</span> node <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span><span class=\"token punctuation\">(</span>buf<span class=\"token punctuation\">)</span> <span class=\"token function\">BPlusTreeNode</span><span class=\"token punctuation\">(</span>p_fanout<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token keyword\">return</span> node<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>The result is a cache-friendly B+Tree node with a fanout that can be configured at runtime.</p>\n<h2 id=\"the-price-of-fine-grained-control\" tabindex=\"-1\">The Price Of Fine-Grained Control <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#the-price-of-fine-grained-control\" aria-hidden=\"true\">#</a></h2>\n<p>To create an instance of a B+Tree node with a fanout of <code>256</code>, it is not possible to write simple idiomatic code like this: <code>new BPlusTreeNode(256)</code>.</p>\n<p>Instead we use the custom <code>BPlusTreeNode::Get</code> helper which knows how much raw memory to allocate for the object including the data section.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\">BPlusTreeNode <span class=\"token operator\">*</span>root <span class=\"token operator\">=</span> <span class=\"token class-name\">BPlusTreeNode</span><span class=\"token operator\">&lt;</span>KeyValuePair<span class=\"token operator\">></span><span class=\"token double-colon punctuation\">::</span><span class=\"token function\">Get</span><span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<h3 id=\"manual-handling-of-deallocation\" tabindex=\"-1\">Manual Handling Of Deallocation <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#manual-handling-of-deallocation\" aria-hidden=\"true\">#</a></h3>\n<p>The destructor code is also not idiomatic anymore. When the lifetime of the B+Tree node ends, the deallocation code has to be carefully crafted to avoid resource or memory leaks.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">BPlusTreeNode</span> <span class=\"token punctuation\">{</span><br /><br />  <span class=\"token keyword\">void</span> <span class=\"token function\">FreeNode</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token comment\">// Call the destructor for each key-value entry.</span><br />    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>KeyValuePair<span class=\"token operator\">*</span> element <span class=\"token operator\">=</span> entries_<span class=\"token punctuation\">;</span> element <span class=\"token operator\">&lt;</span> iter_end_<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>element<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />      element<span class=\"token operator\">-></span><span class=\"token operator\">~</span><span class=\"token function\">KeyValuePair</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    <span class=\"token punctuation\">}</span><br /><br />    <span class=\"token comment\">// Call the node destructor</span><br />    <span class=\"token keyword\">this</span><span class=\"token operator\">-></span><span class=\"token operator\">~</span><span class=\"token function\">BPlusTreeNode</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />    <span class=\"token comment\">// Deallocate the raw memory</span><br />    <span class=\"token double-colon punctuation\">::</span><span class=\"token keyword\">operator</span> <span class=\"token keyword\">delete</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>This carefully ordered cleanup is necessary because we took manual control of memory. The process is the mirror opposite of our <code>Get</code> function. We constructed the object outside in: <em>raw memory buffer -&gt; node object -&gt; individual elements</em>. So we teardown in the opposite direction, from the inside out: <em>individual elements -&gt; node object -&gt; raw memory buffer</em>.</p>\n<h3 id=\"adding-new-members-in-a-derived-class\" tabindex=\"-1\">Adding New Members In A Derived Class <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#adding-new-members-in-a-derived-class\" aria-hidden=\"true\">#</a></h3>\n<p>Adding a new member to a derived class will result in data corruption. It is not possible to add new fields to a specialized <code>InnerNode</code> or <code>LeafNode</code> class.</p>\n<pre class=\"language-text\"><code class=\"language-text\">+----------------------+<br />| Node Metadata Header |<br />+----------------------+<br />| ...                  |<br />+----------------------+<br />| Node Data            |<br />+----------------------+<-- offset where the data buffer starts<br />| entries_[0]          |<-- offset where the derived class members<br />| entries_[1]          |    will be written to, overwriting the<br />| ...                  |    entries<br />| entries_[N]          |<br />+----------------------+</code></pre>\n<figcaption>Fig 2. Adding new members in a derived class will overwrite the <code>entries_</code> array in memory.</figcaption>\n<p>The raw memory we manually allocated is opaque to the compiler and it cannot safely reason about where the newly added members to the derived class are physically located. The end result is it will overwrite the data buffer and cause data corruption.</p>\n<p>The workaround is to break encapsulation and add derived members to the base class so that the flexible array member is always in the last position. This is a significant drawback when we begin using flexible array members.</p>\n<pre class=\"language-text\"><code class=\"language-text\">+----------------------+<br />| Node Metadata Header |<br />+----------------------+<br />| ...                  |<br />| low_key_             |<-- `InnerNode`: left-most node pointer<br />| left_sibling_        |<-- `LeafNode` : link to left sibling<br />| right_sibling_       |<-- `LeafNode` : link to right sibling<br />+----------------------+<br />| Node Data            |<br />+----------------------+<-- Flexible array member guaranteed to<br />| entries_[0]          |    be in the last position<br />| entries_[1]          |<br />| ...                  |<br />| entries_[N]          |<br />+----------------------+</code></pre>\n<figcaption>Fig 3. Memory layout of base class with members necessary for the derived <code>InnerNode</code> and <code>LeafNode</code> implementations.</figcaption>\n<h3 id=\"reinventing-the-wheel\" tabindex=\"-1\">Reinventing The Wheel <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#reinventing-the-wheel\" aria-hidden=\"true\">#</a></h3>\n<p>By using a raw C-style array, we effectively reinvent parts of <code>std::vector</code>, implementing our own utilities for insertion, deletion and iteration. This not only raises the complexity and maintenance burden but also means we are responsible for ensuring our custom implementation is as performant as the highly-optimized standard library version.</p>\n<p>The engineering cost to make this implementation production-grade is significant.</p>\n<h3 id=\"hidden-data-type-assumptions\" tabindex=\"-1\">Hidden Data Type Assumptions <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#hidden-data-type-assumptions\" aria-hidden=\"true\">#</a></h3>\n<p>The <code>BPlusTreeNode</code>'s generic signature implies it will work for any <code>KeyType</code> or <code>ValueType</code>, but this is dangerously misleading. Using a non-trivial type like <code>std::string</code> will cause undefined behavior.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">template</span> <span class=\"token operator\">&lt;</span><span class=\"token keyword\">typename</span> <span class=\"token class-name\">KeyType</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">typename</span> <span class=\"token class-name\">ValueType</span><span class=\"token operator\">></span><br /><span class=\"token keyword\">class</span> <span class=\"token class-name\">BPlusTreeNode</span> <span class=\"token punctuation\">{</span><br /><span class=\"token keyword\">public</span><span class=\"token operator\">:</span><br />  <span class=\"token keyword\">using</span> KeyValuePair <span class=\"token operator\">=</span> std<span class=\"token double-colon punctuation\">::</span>pair<span class=\"token operator\">&lt;</span>KeyType<span class=\"token punctuation\">,</span> ValueType<span class=\"token operator\">></span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// ...</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>To understand why, let's look at how entries are inserted. To make space for a new element, existing entries must be shifted to the right. With our low-level memory layout, this is done using bitwise copy, as the following implementation shows.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">bool</span> <span class=\"token function\">Insert</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> KeyValuePair <span class=\"token operator\">&amp;</span>element<span class=\"token punctuation\">,</span> KeyValuePair <span class=\"token operator\">*</span>pos<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// The node is currently full so we cannot insert this element.</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token function\">GetCurrentSize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> <span class=\"token function\">GetMaxSize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span> <span class=\"token keyword\">return</span> <span class=\"token boolean\">false</span><span class=\"token punctuation\">;</span> <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token comment\">// Shift elements from `pos` to the right by one to make</span><br />  <span class=\"token comment\">// place for inserting new `element`.</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">distance</span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">,</span> <span class=\"token function\">End</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />      <span class=\"token comment\">// Bitwise copying</span><br />      std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">memmove</span><span class=\"token punctuation\">(</span><br />        <span class=\"token comment\">// Destination Address</span><br />        <span class=\"token generic-function\"><span class=\"token function\">reinterpret_cast</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span><span class=\"token keyword\">void</span> <span class=\"token operator\">*</span><span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span>std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">next</span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><br />        <span class=\"token comment\">// Source Address</span><br />        <span class=\"token generic-function\"><span class=\"token function\">reinterpret_cast</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span><span class=\"token keyword\">void</span> <span class=\"token operator\">*</span><span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><br />        <span class=\"token comment\">// Byte Count</span><br />        std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">distance</span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">,</span> <span class=\"token function\">End</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span>KeyValuePair<span class=\"token punctuation\">)</span><br />      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token comment\">// Insert element at `pos`.</span><br />  <span class=\"token keyword\">new</span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">)</span> KeyValuePair<span class=\"token punctuation\">{</span>element<span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// Bookkeeping</span><br />  std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">advance</span><span class=\"token punctuation\">(</span>iter_end_<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token keyword\">return</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>The use of <code>std::memmove</code> introduces a hidden constraint: <code>KeyValuePair</code> must be trivially copyable. This means the implementation only works correctly for simple, C-style data structures despite its generic-looking interface.</p>\n<p>Using <code>std::memmove</code> on a <code>std::string</code> object creates a shallow copy. We now have two <code>std::string</code> objects whose internal pointers both point to the same character buffer on the heap. When the destructor of the original string is eventually called, it deallocates that buffer. The copied string is now left with a dangling pointer to freed memory, leading to use-after-free errors or a double-free crash when its own destructor runs.</p>\n<h2 id=\"conclusion\" tabindex=\"-1\">Conclusion <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#conclusion\" aria-hidden=\"true\">#</a></h2>\n<p>We started with a clear goal: a cache-friendly, contiguous B+Tree node with a dynamic, runtime-configurable fanout. The flexible array member proved to be the perfect tool, giving us direct control over memory layout while supporting variable-length entries.</p>\n<p>However, this fine-grained control comes at a steep cost. We must abandon idiomatic C++, manually manage memory, give up inheritance, and enforce hidden data type constraints. This is the fundamental trade-off: we sacrifice simplicity and safety for raw performance.</p>\n",
      "date_published": "2025-08-18T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/",
      "url": "https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/",
      "title": "A B+Tree Node Underflows: Merge or Borrow?",
      "content_html": "<p>A B+Tree's stable algorithmic performance relies on a single invariant: the path from its root to any leaf is always the same length. However, a delete operation can cause a node to underflow (falling below its minimum occupancy), triggering a rebalancing procedure to maintain this critical invariant.</p>\n<p>Modern B+Trees use fast, optimistic latching protocols which operate under the assumption that rebalancing happens rarely. The mere possibility of an underflow can force the rebalancing into the slow, pessimistic path, acquiring exclusive locks that stall other operations.</p>\n<p>How the implementation decides to fix the underflow is therefore a critical design choice: merge with a sibling node to reclaim free space, or borrow keys from a sibling node to minimize the impact on write latency. Simply put, it's a classic trade-off between space and time. In this post, we will also explore how major OLTP systems expertly implement sophisticated strategies which go beyond this classic trade-off.</p>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#fixing-node-underflow\">Fixing Node Underflow</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#the-merge-first-approach\">The Merge-First Approach</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#the-borrow-first-approach\">The Borrow-First Approach</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#in-oltp-systems\">In OLTP systems</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#background-merge-in-mysql-innodb\">Background Merge In MySQL InnoDB</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#do-nothing-strategy-in-postgresql\">Do Nothing Strategy In PostgreSQL</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#key-takeaways\">Key Takeaways</a>\n    </li>\n  </ol>\n</nav>\n<h2 id=\"fixing-node-underflow\" tabindex=\"-1\">Fixing Node Underflow <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#fixing-node-underflow\" aria-hidden=\"true\">#</a></h2>\n<p>A node underflow happens when the used space (or occupancy) within a node falls below a minimum threshold. This is a possibility after removing an entry from the node. A viable solution is to do nothing. By doing nothing, a tree balancing procedure is never activated. The major downside though is index bloat. A failure to garbage collect the unused memory results in the nodes becoming progressively sparse as entries continue to be added and removed.</p>\n<blockquote>\n<p>In contrast, a node overflow will always trigger a tree rebalancing because it creates a new node whose reference needs to be inserted in the parent node. Here, doing nothing is not even an available option.</p>\n</blockquote>\n<p>The two basic strategies for fixing node underflow both involve merging and borrowing. They differ by which operation is executed first: a merge or a borrow. The merge-first approach prioritizes immediate garbage collection of unused space. It trades-off write speed for more efficient utilization of space. In contrast, the borrow-first approach prioritizes write throughput through redistribution of keys within existing nodes avoiding a merge whenever possible, trading-off long term space-efficiency for short-term write speed.</p>\n<h3 id=\"the-merge-first-approach\" tabindex=\"-1\">The Merge-First Approach <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#the-merge-first-approach\" aria-hidden=\"true\">#</a></h3>\n<p>For a merge to work, the combined entries in the underflowing node and the target sibling node must fit within a single node. After merging two nodes into one, the memory belonging to the underflowing node can be garbage collected.</p>\n<blockquote>\n<p>An efficient implementation will avoid allocation for a new node, and reuse the memory of the sibling node.</p>\n</blockquote>\n<p>The problem with the merge-first approach is that in the worst-case it can recursively cascade all the way back to the root of the B+Tree. In practice though this should happen rarely. The reason for this behavior is that merge eliminates an existing node, and its reference has to be removed from the parent inner node. Removing an entry from a node, has the potential however small to again cause an underflow.</p>\n<p>What if the combined entries will not fit into a single node?</p>\n<p>Then we fallback to borrowing entries from a sibling to fix the underflow. This will not cause a cascading rebalance, as there is no change in nodes, only a redistribution of keys.</p>\n<blockquote>\n<p><em>Disclaimer</em>: The following is a simplified view of how the B+Tree algorithm and concurrency protocols interleave with each other. An implementation in code is more complex.</p>\n</blockquote>\n<p>In optimistic (crab latching) concurrency, if removing an entry will cause a node to underflow it is categorized as an &quot;unsafe&quot; node. The optimistic approach is abandoned and we restart traversal back from the root. It involves acquiring a chain of exclusive latches along the search path to safely complete the operation. This significantly slower path blocks other readers and writers from accessing the latched nodes until the rebalancing operation completes.</p>\n<p>The merge-first approach compresses nodes and therefore more keys are stored within the minimum amount of nodes. This occupies less space on disk and therefore requires less read I/O to be performed. For range scans then a minimum number of nodes needs to be read from disk for a given key range. The higher node density also results in better cache locality and this further improves compute performance.</p>\n<h3 id=\"the-borrow-first-approach\" tabindex=\"-1\">The Borrow-First Approach <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#the-borrow-first-approach\" aria-hidden=\"true\">#</a></h3>\n<p>For borrowing to work, removing the entries from the sibling node must not result in an underflow. Since there is no change in nodes the tree rebalancing is guaranteed not to cascade and therefore completes faster. If borrowing will cause a node underflow, we fallback to merging nodes.</p>\n<p>The concurrency aspects are similar to the merge-first approach. In comparison, it is reasonable to expect that the exclusive latches held on the nodes in the search path segment, will be for a relatively shorter duration. It is still orders of magnitude slower than the optimistic approach.</p>\n<p>This approach prioritizes faster writes and predictable latency by avoiding merging nodes unless strictly necessary. The downside is that node density is lower. The range scans now require more node I/O because the same key range is now spread over a wide span of leaf nodes.</p>\n<h2 id=\"in-oltp-systems\" tabindex=\"-1\">In OLTP Systems <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#in-oltp-systems\" aria-hidden=\"true\">#</a></h2>\n<p>The discussion so far is confined to a stand-alone thread-safe B+Tree implementation. We are looking at behavior and performance at the data structure level. In major OLTP database management systems, the B+Tree index is also tightly integrated with the transaction manager, write ahead log and recovery manager. So the scope of the design decisions are not limited to the data structure level, rather how it impacts the overall systems performance.</p>\n<h3 id=\"background-merge-in-mysql-innodb\" tabindex=\"-1\">Background Merge In MySQL InnoDB <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#background-merge-in-mysql-innodb\" aria-hidden=\"true\">#</a></h3>\n<p>In MYSQL's InnoDB, node underflow is handling through merging. Rather than performing an online tree balancing, it is offloaded as a separate asynchronous process in the background. The minimum occupancy of a node is configurable through the <a href=\"https://dev.mysql.com/doc/refman/8.4/en/index-page-merge-threshold.html\">MERGE_THRESHOLD</a> parameter.</p>\n<blockquote>\n<p>If the “page-full” percentage for an index page falls below the MERGE_THRESHOLD value when a row is deleted or when a row is shortened by an UPDATE operation, InnoDB attempts to merge the index page with a neighboring index page. The default MERGE_THRESHOLD value is 50, which is the previously hard-coded value. The minimum MERGE_THRESHOLD value is 1 and the maximum value is 50.</p>\n</blockquote>\n<p>You can monitor the background merge by querying the following InnoDB metrics:</p>\n<pre class=\"language-text\"><code class=\"language-text\">mysql> SELECT NAME, COMMENT FROM INFORMATION_SCHEMA.INNODB_METRICS<br />       WHERE NAME like '%index_page_merge%';<br />+-----------------------------+----------------------------------------+<br />| NAME                        | COMMENT                                |<br />+-----------------------------+----------------------------------------+<br />| index_page_merge_attempts   | Number of index page merge attempts    |<br />| index_page_merge_successful | Number of successful index page merges |<br />+-----------------------------+----------------------------------------+</code></pre>\n<p>Merging entries from a sibling reduces the unused space remaining within a node. If new insertions land on this node, it can immediately overflow. The overflow forces a node split which requires a new node allocation and redistribution of entries. Now both nodes are half-full, and a delete from either node can tip another underflow creating a cycle of merging and splitting. This thrashing merge-split behavior is terrible news for index performance.</p>\n<blockquote>\n<p>If both pages are close to 50% full, a page split can occur soon after the pages are merged. If this merge-split behavior occurs frequently, it can have an adverse affect on performance. To avoid frequent merge-splits, you can lower the MERGE_THRESHOLD value so that InnoDB attempts page merges at a lower “page-full” percentage. Merging pages at a lower page-full percentage leaves more room in index pages and helps reduce merge-split behavior.</p>\n</blockquote>\n<p>What happens if you over-tune the <code>MERGE_THRESHOLD</code> knob for your workload?</p>\n<blockquote>\n<p>A MERGE_THRESHOLD setting that is too small could result in large data files due to an excessive amount of empty page space.</p>\n</blockquote>\n<p>This results in index bloat, which directly harms read efficiency. The same key space instead of being densely packed, is now spread over a larger number of sparse nodes, requiring more I/O for range scans, and takes up more buffer pool capacity to keep the index in memory.</p>\n<h3 id=\"do-nothing-strategy-in-postgresql\" tabindex=\"-1\">Do Nothing Strategy In PostgreSQL <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#do-nothing-strategy-in-postgresql\" aria-hidden=\"true\">#</a></h3>\n<p>In PostgreSQL deleting an index entry happens in two separate phases: a logical deletion followed by a physical deletion. The logical deletion creates a tombstone marker, and is lightweight. The physical deletion is more expensive, and happens in the background.</p>\n<p>When a node underflows, PostgreSQL does nothing to rectify the situation. The only time it attempts to reclaim space is when all the entries are removed from a leaf node and it becomes empty.</p>\n<blockquote>\n<p>We consider deleting an entire page from the btree only when it's become\ncompletely empty of items. (Merging partly-full pages would allow better\nspace reuse, but it seems impractical to move existing data items left or\nright to make this happen --- a scan moving in the opposite direction\nmight miss the items if so.)</p>\n</blockquote>\n<p>There is an exception to this rule. PostgreSQL will never delete the right-most child of a parent on any given level, even if it becomes empty. Removing the right-most node will have to be followed by transferring the key space used for navigation to the next or previous parent. Since we do not hold latches on those nodes yet, they will have to be freshly acquired. All of this is avoided by sticking to this rule, and it simplifies the implementation.</p>\n<blockquote>\n<p>To preserve consistency on the parent level, we cannot merge the key space\nof a page into its right sibling unless the right sibling is a child of\nthe same parent --- otherwise, the parent's key space assignment changes\ntoo, meaning we'd have to make bounding-key updates in its parent, and\nperhaps all the way up the tree. Since we can't possibly do that\natomically, we forbid this case.</p>\n</blockquote>\n<p>The deletion of a node is also separated into logical and physical phases. A logical deletion happens first which marks a tombstone. A physical deletion, which reclaims the space for reuse is only performed when this node is not visible to any active transactions.</p>\n<blockquote>\n<p>Recycling a page is decoupled from page deletion. A deleted page can only\nbe put in the FSM to be recycled once there is no possible scan or search\nthat has a reference to it; until then, it must stay in place with its\nsibling links undisturbed, as a tombstone that allows concurrent searches\nto detect and then recover from concurrent deletions (which are rather\nlike concurrent page splits to searchers).</p>\n</blockquote>\n<p><a href=\"https://github.com/postgres/postgres/blob/master/src/backend/access/nbtree/README\">Source</a>: <code>src/backend/access/nbtree/README</code></p>\n<h2 id=\"key-takeaways\" tabindex=\"-1\">Key Takeaways <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#key-takeaways\" aria-hidden=\"true\">#</a></h2>\n<p>Fixing a node underflow is presented as a binary choice between a merge-first or borrow-first approach for tree rebalancing at the data structure level. For a concurrent implementation, when a node underflow happens the pessimistic (slower) path is preferred for correctness and protecting the integrity of the B+Tree. For non-OLTP use cases, neither merge-first nor borrow-first is inherently better than the other.</p>\n<p>In MySQL the rebalancing is offline, and happens in the background. While in PostgreSQL, rebalancing is not undertaken for node underflows. The priority is higher concurrency by avoiding rebalancing. The trade-off in both systems is accumulating index bloat. The burden of managing index bloat now falls upon the operator.</p>\n<p>Two brilliant lessons we can learn from these OLTP systems to improve concurrency are: offline (asynchronous) rebalancing, and separating the deletion of entries, and nodes into logical and physical phases.</p>\n",
      "date_published": "2025-08-16T00:00:00Z"
    }
  ]
}
