{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Jacob&#39;s blog",
  "language": "en",
  "home_page_url": "https://jacobsherin.com/",
  "feed_url": "https://example.com/feed/feed.json",
  "description": "On database building blocks.",
  "author": {
    "name": "Jacob Sherin",
    "url": "https://jacobsherin.com/about/"
  },
  "items": [{
      "id": "https://jacobsherin.com/posts/test-page/",
      "url": "https://jacobsherin.com/posts/test-page/",
      "title": "Test Page: All Markdown Patterns",
      "content_html": "<p>This page is a test bed for all the markdown and HTML elements used in this blog. Use it to test CSS changes.</p>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/test-page/#headings\">Headings</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/test-page/#heading-3\">Heading 3</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/test-page/#heading-4\">Heading 4</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/test-page/#heading-5\">Heading 5</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/test-page/#heading-6\">Heading 6</a></li>\n      </ul>\n    </li>\n    <li><a href=\"https://jacobsherin.com/posts/test-page/#paragraphs-and-text-formatting\">Paragraphs and Text Formatting</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/test-page/#links\">Links</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/test-page/#blockquotes\">Blockquotes</a></li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/test-page/#lists\">Lists</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/test-page/#unordered-list\">Unordered List</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/test-page/#ordered-list\">Ordered List</a></li>\n      </ul>\n    </li>\n    <li><a href=\"https://jacobsherin.com/posts/test-page/#images\">Images</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/test-page/#code-blocks\">Code Blocks</a></li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/test-page/#tables\">Tables</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/test-page/#markdown-table\">Markdown Table</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/test-page/#html-table\">HTML Table</a></li>\n      </ul>\n    </li>\n  </ol>\n</nav>\n<h2 id=\"headings\" tabindex=\"-1\">Headings <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#headings\" aria-hidden=\"true\"></a></h2>\n<p>This is a Heading 2 (the post title is H1).</p>\n<h3 id=\"heading-3\" tabindex=\"-1\">Heading 3 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#heading-3\" aria-hidden=\"true\"></a></h3>\n<p>This is a Heading 3. It's used for sub-sections.</p>\n<h4 id=\"heading-4\" tabindex=\"-1\">Heading 4 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#heading-4\" aria-hidden=\"true\"></a></h4>\n<p>This is a Heading 4.</p>\n<h5 id=\"heading-5\" tabindex=\"-1\">Heading 5 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#heading-5\" aria-hidden=\"true\"></a></h5>\n<p>This is a Heading 5.</p>\n<h6 id=\"heading-6\" tabindex=\"-1\">Heading 6 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#heading-6\" aria-hidden=\"true\"></a></h6>\n<p>This is a Heading 6.</p>\n<h2 id=\"paragraphs-and-text-formatting\" tabindex=\"-1\">Paragraphs and Text Formatting <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#paragraphs-and-text-formatting\" aria-hidden=\"true\"></a></h2>\n<p>This is a standard paragraph of text. It contains <strong>bold text</strong>, <em>italic text using underscores</em>, and <em>italic text using asterisks</em>. You can also have <code>inline code</code> which is useful for mentioning variables or filenames.</p>\n<p>Another paragraph follows, demonstrating the flow of text. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>\n<h2 id=\"links\" tabindex=\"-1\">Links <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#links\" aria-hidden=\"true\"></a></h2>\n<p>Here is an <a href=\"https://www.google.com/\">inline link to Google</a>.\nHere is a <a href=\"https://www.11ty.dev/\">reference-style link to the Eleventy docs</a>.</p>\n<h2 id=\"blockquotes\" tabindex=\"-1\">Blockquotes <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#blockquotes\" aria-hidden=\"true\"></a></h2>\n<blockquote>\n<p>This is a blockquote. It's useful for highlighting a quote or an important note from another source. It can span multiple lines.</p>\n</blockquote>\n<blockquote>\n<p>An efficient implementation will avoid allocation for a new node, and reuse the memory of the sibling node.</p>\n</blockquote>\n<h2 id=\"lists\" tabindex=\"-1\">Lists <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#lists\" aria-hidden=\"true\"></a></h2>\n<h3 id=\"unordered-list\" tabindex=\"-1\">Unordered List <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#unordered-list\" aria-hidden=\"true\"></a></h3>\n<ul>\n<li>Item 1</li>\n<li>Item 2\n<ul>\n<li>Nested Item 2a</li>\n<li>Nested Item 2b</li>\n</ul>\n</li>\n<li>Item 3</li>\n</ul>\n<h3 id=\"ordered-list\" tabindex=\"-1\">Ordered List <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#ordered-list\" aria-hidden=\"true\"></a></h3>\n<ol>\n<li>First item</li>\n<li>Second item\n<ol>\n<li>Nested item 2.1</li>\n<li>Nested item 2.2</li>\n</ol>\n</li>\n<li>Third item</li>\n</ol>\n<h2 id=\"images\" tabindex=\"-1\">Images <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#images\" aria-hidden=\"true\"></a></h2>\n<p>This is an SVG image using the new <code>figure</code> shortcode:</p>\n<figure>\n              <img src=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/img/figure-1.svg\" alt=\"An SVG image\" />\n              <figcaption>Fig 1. An SVG image that should scale correctly.</figcaption>\n            </figure>\n<p>This is a PNG image using our new <code>figure</code> shortcode:</p>\n<figure>\n              <img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/flamegraph_grid_numbered.png\" alt=\"A flamegraph grid\" />\n              <figcaption>Fig 2. A PNG image that may need styling to prevent overflow on mobile.</figcaption>\n            </figure>\n<h2 id=\"code-blocks\" tabindex=\"-1\">Code Blocks <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#code-blocks\" aria-hidden=\"true\"></a></h2>\n<p>Here is a <code>text</code> code block:</p>\n<pre class=\"language-text\"><code class=\"language-text\">Some plain text inside a code block.<br />No syntax highlighting is applied here.<br />+-----------------------------+----------------------------------------+<br />| NAME                        | COMMENT                                |<br />+-----------------------------+----------------------------------------+</code></pre>\n<figcaption>A caption for a text code block.</figcaption>\n<p>A <code>rust</code> code block with syntax highlighting:</p>\n<pre class=\"language-rust\"><code class=\"language-rust\"><span class=\"token keyword\">fn</span> <span class=\"token function-definition function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">-></span> <span class=\"token class-name\">Result</span><span class=\"token operator\">&lt;</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">Box</span><span class=\"token operator\">&lt;</span><span class=\"token keyword\">dyn</span> <span class=\"token class-name\">Error</span> <span class=\"token operator\">+</span> <span class=\"token class-name\">Send</span> <span class=\"token operator\">+</span> <span class=\"token class-name\">Sync</span><span class=\"token operator\">>></span> <span class=\"token punctuation\">{</span><br />    <span class=\"token macro property\">println!</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Hello, World!\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<h2 id=\"tables\" tabindex=\"-1\">Tables <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#tables\" aria-hidden=\"true\"></a></h2>\n<h3 id=\"markdown-table\" tabindex=\"-1\">Markdown Table <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#markdown-table\" aria-hidden=\"true\"></a></h3>\n<table>\n<thead>\n<tr>\n<th>Header 1</th>\n<th>Header 2</th>\n<th>Header 3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cell 1-1</td>\n<td>Cell 1-2</td>\n<td>Cell 1-3</td>\n</tr>\n<tr>\n<td>Cell 2-1</td>\n<td>Cell 2-2</td>\n<td>Cell 2-3</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"html-table\" tabindex=\"-1\">HTML Table <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/test-page/#html-table\" aria-hidden=\"true\"></a></h3>\n<table>\n  <thead>\n    <tr>\n      <th>HTML Header 1</th>\n      <th>HTML Header 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>HTML Cell 1-1</td>\n      <td>HTML Cell 1-2</td>\n    </tr>\n    <tr>\n      <td>HTML Cell 2-1</td>\n      <td>HTML Cell 2-2</td>\n    </tr>\n  </tbody>\n</table>\n",
      "date_published": "2025-09-24T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-09-09-parquet-embed-tantivy/",
      "url": "https://jacobsherin.com/posts/2025-09-09-parquet-embed-tantivy/",
      "title": "Faster LIKE Queries In Parquet Without an External Index",
      "content_html": "<p>The query:</p>\n<pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span><br />  <span class=\"token keyword\">FROM</span> <span class=\"token string\">'foo.parquet'</span><br /> <span class=\"token keyword\">WHERE</span> title <span class=\"token operator\">LIKE</span> <span class=\"token string\">'%Diary%'</span><span class=\"token punctuation\">;</span></code></pre>\n<p>The default query plan for <code>LIKE</code> queries in <a href=\"https://datafusion.apache.org/\">Apache DataFusion</a>.</p>\n<pre class=\"language-text\"><code class=\"language-text\">+---------------+-------------------------------+<br />| plan_type     | plan                          |<br />+---------------+-------------------------------+<br />| physical_plan | ┌───────────────────────────┐ |<br />|               | │    CoalesceBatchesExec    │ |<br />|               | │    --------------------   │ |<br />|               | │     target_batch_size:    │ |<br />|               | │            8192           │ |<br />|               | └─────────────┬─────────────┘ |<br />|               | ┌─────────────┴─────────────┐ |<br />|               | │         FilterExec        │ |<br />|               | │    --------------------   │ |<br />|               | │         predicate:        │ |<br />|               | │     title LIKE %Diary%    │ |<br />|               | └─────────────┬─────────────┘ |<br />|               | ┌─────────────┴─────────────┐ |<br />|               | │      RepartitionExec      │ |<br />|               | │    --------------------   │ |<br />|               | │ partition_count(in->out): │ |<br />|               | │          1 -> 12          │ |<br />|               | │                           │ |<br />|               | │    partitioning_scheme:   │ |<br />|               | │    RoundRobinBatch(12)    │ |<br />|               | └─────────────┬─────────────┘ |<br />|               | ┌─────────────┴─────────────┐ |<br />|               | │       DataSourceExec      │ |<br />|               | │    --------------------   │ |<br />|               | │          files: 1         │ |<br />|               | │      format: parquet      │ |<br />|               | │                           │ |<br />|               | │         predicate:        │ |<br />|               | │     title LIKE %Diary%    │ |<br />|               | └───────────────────────────┘ |<br />|               |                               |<br />+---------------+-------------------------------+<br />1 row(s) fetched.<br />Elapsed 0.016 seconds.<br /></code></pre>\n<p>The optimized query plan where an embedded Tantivy full-text index embedded within the Parquet file is used.</p>\n<pre class=\"language-text\"><code class=\"language-text\">+---------------+-------------------------------+<br />| plan_type     | plan                          |<br />+---------------+-------------------------------+<br />| physical_plan | ┌───────────────────────────┐ |<br />|               | │       DataSourceExec      │ |<br />|               | │    --------------------   │ |<br />|               | │          files: 1         │ |<br />|               | │      format: parquet      │ |<br />|               | │                           │ |<br />|               | │         predicate:        │ |<br />|               | │        id IN (3, 2)       │ |<br />|               | └───────────────────────────┘ |<br />|               |                               |<br />+---------------+-------------------------------+</code></pre>\n<p>The Tantivy full-text index is embedded within Parquet as a sequence of bytes.</p>\n<p>File format specification:</p>\n<pre class=\"language-text\"><code class=\"language-text\">+=================================================================+<br />|                              Header                             |<br />|-----------------------------------------------------------------|<br />| MAGIC_BYTES ('FTEP')                            |   (4 bytes)   |<br />|-------------------------------------------------+---------------|<br />| version                                         |   (1 byte)    |<br />|-------------------------------------------------+---------------|<br />| file_count                                      |   (4 bytes)   |<br />|-------------------------------------------------+---------------|<br />| total_data_block_size                           |   (8 bytes)   |<br />|-------------------------------------------------+---------------|<br />| file_metadata_size                              |   (4 bytes)   |<br />|-------------------------------------------------+---------------|<br />| file_metadata_crc32                             |   (4 bytes)   |<br />|-----------------------------------------------------------------|<br />|                  file_metadata_list (variable size)             |<br />|                  (A series of FileMetadata entries)             |<br />|                                                                 |<br />| +-------------------------------------------------------------+ |<br />| | FileMetadata Entry 1 (e.g., for \"meta.json\")                | |<br />| |-------------------------------------------------------------| |<br />| | data_offset (points to start of file in Data Block) | (8 B) | | ------+<br />| | data_content_len                                    | (8 B) | |       |<br />| | data_footer_len (0 for meta.json)                   | (1 B) | |       |<br />| | path_len                                            | (1 B) | |       |<br />| | path (\"meta.json\")                                  |(~9 B) | |       |<br />| +-------------------------------------------------------------+ |       |<br />| | FileMetadata Entry 2 (e.g., for \".fast\")                    | |       |<br />| |-------------------------------------------------------------| |       |<br />| | data_offset (points to start of file in Data Block) | (8 B) | | ----+ |<br />| | ... (other fields) ...                                      | |     | |<br />| +-------------------------------------------------------------+ |     | |<br />| |                             ...                             | |     | |<br />+=================================================================+     | |<br />|                            DataBlock                            |     | |<br />|                 (Size = total_data_block_size)                  |     | |<br />|-----------------------------------------------------------------| <---+ |<br />|                                                                 |       |<br />|             Contents of \"meta.json\" (raw bytes)                 |       |<br />|                                                                 |       |<br />|-----------------------------------------------------------------| <-----+<br />|                                                                 |<br />|               Contents of \".fast\" file (raw bytes)              |<br />|                                                                 |<br />|-----------------------------------------------------------------|<br />|                                                                 |<br />|        Reconstructed `TantivyFooterHack` for \".fast\" file       |<br />|                                                                 |<br />|-----------------------------------------------------------------|<br />|                                                                 |<br />|              Contents of \".fieldnorm\" file (raw bytes)          |<br />|                                                                 |<br />|-----------------------------------------------------------------|<br />|                                                                 |<br />|      Reconstructed `TantivyFooterHack` for \".fieldnorm\" file    |<br />|                                                                 |<br />|-----------------------------------------------------------------|<br />|                                                                 |<br />|      ... and so on for all other Tantivy index files            |<br />|              (.idx, .pos, .term, .store, etc.)                  |<br />|                                                                 |<br />+-----------------------------------------------------------------+</code></pre>\n<p>Histogram of control phrases in generated Parquet file.</p>\n<pre><code>DataFusion CLI v49.0.0\n&gt; SELECT\n    COUNT(CASE WHEN title LIKE '%concurrency%' THEN 1 END) AS concurrency_count,\n    COUNT(CASE WHEN title LIKE '%runtime%' THEN 1 END) AS runtime_count,\n    COUNT(CASE WHEN title LIKE '%indexing%' THEN 1 END) AS indexing_count,\n    COUNT(CASE WHEN title LIKE '%normalization%' THEN 1 END) AS normalization_count,\n    COUNT(CASE WHEN title LIKE '%atomics%' THEN 1 END) AS atomics_count,\n    COUNT(CASE WHEN title LIKE '%idempotency%' THEN 1 END) AS idempotency_count\nFROM '/Users/jcsherin/Projects/rusty-doodles/output/titles.parquet';\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| concurrency_count | runtime_count | indexing_count | normalization_count | atomics_count | idempotency_count |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| 12905516          | 4999816       | 2500033        | 498659              | 250832        | 49910             |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n1 row(s) fetched.\nElapsed 11.176 seconds.\n\n</code></pre>\n<p>Target size = 50 million rows. Command used:</p>\n<pre class=\"language-text\"><code class=\"language-text\">RUST_LOG=trace,tantivy=off cargo run --bin parquet_gen -- -t 50000000</code></pre>\n<p>2.5 GB on disk.</p>\n<pre class=\"language-text\"><code class=\"language-text\">❯ ls -l output/titles.parquet<br />-rw-r--r--  1 jcsherin  staff  2701649192 14 Sep 12:40 output/titles.parquet</code></pre>\n<h2 id=\"1-million-rows\" tabindex=\"-1\">1 million rows <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-09-parquet-embed-tantivy/#1-million-rows\" aria-hidden=\"true\"></a></h2>\n<pre class=\"language-text\"><code class=\"language-text\">-rw-r--r--  1 jcsherin  staff    96M 14 Sep 14:22 output/titles_with_fts_index.parquet<br />-rw-r--r--  1 jcsherin  staff    52M 14 Sep 14:22 output/titles.parquet</code></pre>\n<p>Histogram for</p>\n<pre><code>&gt; SELECT\n    COUNT(CASE WHEN title LIKE '%concurrency%' THEN 1 END) AS concurrency_count,\n    COUNT(CASE WHEN title LIKE '%runtime%' THEN 1 END) AS runtime_count,\n    COUNT(CASE WHEN title LIKE '%indexing%' THEN 1 END) AS indexing_count,\n    COUNT(CASE WHEN title LIKE '%normalization%' THEN 1 END) AS normalization_count,\n    COUNT(CASE WHEN title LIKE '%atomics%' THEN 1 END) AS atomics_count,\n    COUNT(CASE WHEN title LIKE '%idempotency%' THEN 1 END) AS idempotency_count\nFROM '/Users/jcsherin/Projects/rusty-doodles/output/titles_with_fts_index.parquet';\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| concurrency_count | runtime_count | indexing_count | normalization_count | atomics_count | idempotency_count |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| 258749            | 99804         | 49940          | 9949                | 4903          | 987               |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n1 row(s) fetched.\nElapsed 1.611 seconds.\n\n&gt; SELECT\n    COUNT(CASE WHEN title LIKE '%concurrency%' THEN 1 END) AS concurrency_count,\n    COUNT(CASE WHEN title LIKE '%runtime%' THEN 1 END) AS runtime_count,\n    COUNT(CASE WHEN title LIKE '%indexing%' THEN 1 END) AS indexing_count,\n    COUNT(CASE WHEN title LIKE '%normalization%' THEN 1 END) AS normalization_count,\n    COUNT(CASE WHEN title LIKE '%atomics%' THEN 1 END) AS atomics_count,\n    COUNT(CASE WHEN title LIKE '%idempotency%' THEN 1 END) AS idempotency_count\nFROM '/Users/jcsherin/Projects/rusty-doodles/output/titles.parquet';\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| concurrency_count | runtime_count | indexing_count | normalization_count | atomics_count | idempotency_count |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| 258749            | 99804         | 49940          | 9949                | 4903          | 987               |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n1 row(s) fetched.\nElapsed 1.652 seconds.\n</code></pre>\n<h2 id=\"10-million-rows\" tabindex=\"-1\">10 million rows <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-09-parquet-embed-tantivy/#10-million-rows\" aria-hidden=\"true\"></a></h2>\n<pre><code>-rw-r--r--@ 1 jcsherin  staff   926M 14 Sep 14:32 output/titles_with_fts_index.parquet\n-rw-r--r--@ 1 jcsherin  staff   515M 14 Sep 14:31 output/titles.parquet\n</code></pre>\n<pre><code>&gt; SELECT\n    COUNT(CASE WHEN title LIKE '%concurrency%' THEN 1 END) AS concurrency_count,\n    COUNT(CASE WHEN title LIKE '%runtime%' THEN 1 END) AS runtime_count,\n    COUNT(CASE WHEN title LIKE '%indexing%' THEN 1 END) AS indexing_count,\n    COUNT(CASE WHEN title LIKE '%normalization%' THEN 1 END) AS normalization_count,\n    COUNT(CASE WHEN title LIKE '%atomics%' THEN 1 END) AS atomics_count,\n    COUNT(CASE WHEN title LIKE '%idempotency%' THEN 1 END) AS idempotency_count\nFROM '/Users/jcsherin/Projects/rusty-doodles/output/titles.parquet';\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| concurrency_count | runtime_count | indexing_count | normalization_count | atomics_count | idempotency_count |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| 2580730           | 1000423       | 499792         | 99072               | 49798         | 9865              |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n1 row(s) fetched.\nElapsed 2.497 seconds.\n\n&gt; SELECT\n    COUNT(CASE WHEN title LIKE '%concurrency%' THEN 1 END) AS concurrency_count,\n    COUNT(CASE WHEN title LIKE '%runtime%' THEN 1 END) AS runtime_count,\n    COUNT(CASE WHEN title LIKE '%indexing%' THEN 1 END) AS indexing_count,\n    COUNT(CASE WHEN title LIKE '%normalization%' THEN 1 END) AS normalization_count,\n    COUNT(CASE WHEN title LIKE '%atomics%' THEN 1 END) AS atomics_count,\n    COUNT(CASE WHEN title LIKE '%idempotency%' THEN 1 END) AS idempotency_count\nFROM '/Users/jcsherin/Projects/rusty-doodles/output/titles_with_fts_index.parquet';\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| concurrency_count | runtime_count | indexing_count | normalization_count | atomics_count | idempotency_count |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n| 2580730           | 1000423       | 499792         | 99072               | 49798         | 9865              |\n+-------------------+---------------+----------------+---------------------+---------------+-------------------+\n1 row(s) fetched.\nElapsed 3.867 seconds.\n</code></pre>\n",
      "date_published": "2025-09-02T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/",
      "url": "https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/",
      "title": "Performance Tuning a Nested Data Generator for Parquet",
      "content_html": "<p>I built <a href=\"https://github.com/jcsherin/datablok/blob/main/crates/parquet-nested-parallel/README.md\">a CLI for procedurally generating nested Parquet data</a> where the values follow a <a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">Zipfian-like</a> distribution.</p>\n<pre class=\"language-text\"><code class=\"language-text\"><br />  ┌─────────────────┬───────┬──────────────────┬───────────────────────────┐<br />  │ Version         │ Time  │ Total Throughput │ Avg. Per-Core Throughput* │<br />  ├─────────────────┼───────┼──────────────────┼───────────────────────────┤<br />  │ Initial Version │ 3.7s  │ ~2.70 M rows/s   │ ~0.17 M rows/s            │<br />  │ Final Version   │ 0.44s │ ~22.73 M rows/s  │ ~1.42 M rows/s            │<br />  └─────────────────┴───────┴──────────────────┴───────────────────────────┘<br /></code></pre>\n<p>Lately, I've been poking around record shredding and needed a dataset of nested data structures for tracing query execution of shredded data. For this, I implemented a data generator which follows a <a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">Zipfian-like</a> distribution. The generated data is staged in-memory as <a href=\"https://arrow.apache.org/rust/parquet/arrow/index.html\">Arrow RecordBatches</a>, and then written to disk as Parquet files.</p>\n<p>The baseline version I wrote is a simple pipeline using Rust MPSC which connects multiple data generation (producer) threads to a single Parquet writer (consumer) thread. For a nested dataset of 10 million rows, it ~3.7s to complete. In this post, we'll see how a sequence of performance optimizations, reduced the total runtime to ~533ms (6x speedup).</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/hyperfine_trend_plot.png\" alt=\"Performance Trend Across Runs\" /></p>\n<figcaption>Fig 1. The performance trend across a sequence of code optimizations, measured using <code>hyperfine</code>. The black line indicates the median runtime in seconds, while the shaded area indicates the range between min and max runtime.</figcaption>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/ipc_trend_plot.png\" alt=\"IPC Trend Across Runs\" /></p>\n<p>This chart displays only improvements in total runtime, which does not tell the whole story. While some optimizations here show no difference in the total runtime, the improvements came from higher IPC (instructions per cycle), fewer cache misses and fewer branch mispredictions.</p>\n<p>A string interning optimization (no. 9) looked like a guaranteed win. It was introduced to eliminate a lot of small string allocations in the data generation (producer) threads. The performance got worse (more on this later in this post) and the change had to be reverted. This strongly reinforces, the importance of measurements and profiling data for knowing unambiguously if a code optimization made an improvement or did the opposite.</p>\n<p>All benchmarks were run on a Linux machine with the following configuration:</p>\n<ul>\n<li>Ubuntu 24.04.2 LTS (Kernel 6.8)</li>\n<li>AMD Ryzen 7 PRO 8700GE (8 Cores, 16 Threads)</li>\n<li>64 GB of DDR5-5600 ECC RAM</li>\n<li>512 GB NVMe SSDs.</li>\n</ul>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#background\">Background</a></li>\n    <li>\n        <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-1:-getting-started\">Phase 1: Getting Started</a>\n        <ul>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#01:-use-a-dictionary-data-type\">01: Use a Dictionary Data Type</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#02:-eliminate-intermediate-vector-allocation\">02: Eliminate Intermediate Vector Allocation</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#03:-preallocate-a-string-buffer\">03: Preallocate a String Buffer</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#04:-preallocate-a-string-buffer-2\">04: Preallocate a String Buffer 2</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#why-is-the-runtime-unchanged\">Why is the Runtime Unchanged?</a></li>\n        </ul>\n    </li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-2:-architectural-changes\">Phase 2: Architectural Changes</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#05:-increase-parquet-encoding-bandwidth\">05: Increase Parquet Encoding Bandwidth</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#06:-make-data-generation-lightweight\">06: Make Data Generation Lightweight</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#07:-increase-parquet-writer-threads\">07: Increase Parquet Writer Threads</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#08:-introduce-thread-local-state\">08: Introduce Thread-Local State</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#measuring-impact\">Measuring Impact</a></li>\n      </ul>\n    </li>\n    <li>\n        <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-3:-a-performance-regression\">Phase 3: A Performance Regression</a>\n        <ul>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#09:-global-string-interning\">09: Global String Interning</a></li>\n            <li><a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#10:-revert\">10: Revert</a></li>\n        </ul>\n    </li>\n    <li>\n        <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#a-back-of-the-envelope-estimation\">A Back of the Envelope Estimation</a>\n    </li>\n    <li>\n        <a href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#fine-tuning-configuration-for-optimal-performance\">Fine-tuning Configuration for Optimal Performance</a>\n    </li>\n  </ol>\n</nav>\n<h2 id=\"background\" tabindex=\"-1\">Background <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#background\" aria-hidden=\"true\"></a></h2>\n<p>The program is a CLI tool for generating a target number of rows of nested data structures and then written to disk in Parquet format.</p>\n<p>Nested data structures do not naturally fit into a flat columnar format. Record shredding is a process which converts the nested data into a flat, columnar format while preserving the original structural hierarchy of the raw data.</p>\n<p>The generated data follows a <a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">Zipfian-like</a> distribution. It is staged in memory as <a href=\"https://arrow.apache.org/rust/parquet/arrow/index.html\">Arrow RecordBatches</a>, before being written to disk as Parquet files.</p>\n<p>The data is generated in parallel using a <a href=\"https://github.com/rayon-rs/rayon\">Rayon</a> thread pool. Then data generator threads (producers) sends the data to a Parquet writer thread (consumer). The number of writers are configurable from the CLI.</p>\n<h2 id=\"phase-1:-getting-started\" tabindex=\"-1\">Phase 1: Getting Started <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-1:-getting-started\" aria-hidden=\"true\"></a></h2>\n<p>First, we build the CLI program in release mode and use that for end to end benchmarking using <a href=\"https://github.com/sharkdp/hyperfine\">hyperfine</a>.</p>\n<p>In <code>Cargo.toml</code> the following section is added for release builds:</p>\n<pre class=\"language-toml\"><code class=\"language-toml\"><span class=\"token punctuation\">[</span><span class=\"token table class-name\">profile.release</span><span class=\"token punctuation\">]</span><br /><span class=\"token key property\">debug</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"line-tables-only\"</span><br /><span class=\"token key property\">strip</span> <span class=\"token punctuation\">=</span> <span class=\"token boolean\">false</span></code></pre>\n<p>This will include just enough debug information in the release binary which will help us trace hotspots back to the exact line of code in Rust. This is necessary when recording the call-graphs of the program's execution using <code>perf</code>.</p>\n<p>When generating flamegraphs, we will use <a href=\"https://github.com/luser/rustfilt\">rustfilt</a> to demangle the symbols for improved readability.</p>\n<p>We will also collect hardware performance counters like - cycles, instructions retired, cache references, cache misses, branch instructions and branch mispredictions.</p>\n<p>The following optimizations from 01 through 04, uses the flamegraph to identify hotspots indicated by tall towers and then attempt to squash it.</p>\n<h3 id=\"01:-use-a-dictionary-data-type\" tabindex=\"-1\">01: Use a Dictionary Data Type <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#01:-use-a-dictionary-data-type\" aria-hidden=\"true\"></a></h3>\n<p>In the baseline version, the <code>PhoneType</code> Rust enum is mapped to a string data type (<code>DataType::Utf8</code>) in the Arrow schema.</p>\n<pre class=\"language-rust\"><code class=\"language-rust\"><span class=\"token keyword\">pub</span> <span class=\"token keyword\">enum</span> <span class=\"token type-definition class-name\">PhoneType</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token class-name\">Mobile</span><span class=\"token punctuation\">,</span><br />    <span class=\"token class-name\">Home</span><span class=\"token punctuation\">,</span><br />    <span class=\"token class-name\">Work</span><span class=\"token punctuation\">,</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Instead, by changing the Arrow field data type to <code>DataType::Dictionary</code>, the expectation is that the total memory footprint of the program, and storage size of the Parquet file will improve.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">pub fn get_contact_phone_fields() -> Vec&lt;Arc&lt;Field>> {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    vec![<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        Arc::from(Field::new(\"number\", DataType::Utf8, true)),<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">        Arc::from(Field::new(\"phone_type\", DataType::Utf8, true)),<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">        Arc::from(Field::new(<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            \"phone_type\",<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            DataType::Dictionary(Box::new(DataType::UInt8), Box::new(DataType::Utf8)),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            true,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        )),<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    ]<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">}</span></span></code></pre>\n<p>After the change, the maximum RSS (resident set size) is reduced by ~1MB in a run for generating 10 million rows. The Parquet storage size improvement is negligible. There is a minor regression in runtime.</p>\n<p>Even though, there are no dramatic gains here like we expected, we will maintain this change because it removes the mismatch between the underlying Rust and Arrow data types. That is definitely a readability improvement.</p>\n<h3 id=\"02:-eliminate-intermediate-vector-allocation\" tabindex=\"-1\">02: Eliminate Intermediate Vector Allocation <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#02:-eliminate-intermediate-vector-allocation\" aria-hidden=\"true\"></a></h3>\n<p>The generate data with a predefined data skew (Zipfian-like), a data template value is first generated. The holes in the templates are filled in to generate the final <code>Contact</code> struct value, which is then converted to an Arrow <code>RecordBatch</code>. The series of value transformations looks like this:</p>\n<p><code>Vec&lt;PartialContact&gt;</code> → <code>Vec&lt;Contact&gt;</code> → <code>RecordBatch</code>.</p>\n<p>Instead of creating the intermediate <code>Vec&lt;Contact&gt;</code>, we can do a late materialization of the final <code>Contact</code> value when building a <code>RecordBatch</code> by directly passing it the instructions within <code>Vec&lt;PartialContact&gt;</code>. After eliminating the intermediate step, the value transformation will look like this:</p>\n<p><code>Vec&lt;PartialContact&gt;</code> → <code>RecordBatch</code>.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">  // Assemble the Vec&lt;Contact> for this small chunk<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">  let contacts_chunk: Vec&lt;Contact> = partial_contacts<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">      .into_iter()<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">      .map(|partial_contact| { ... })<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">      .collect();<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\"><br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">  // Convert the chunk to a RecordBatch and send it to the writer<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">  let record_batch = create_record_batch(parquet_schema.clone(), &amp;contacts_chunk)<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">      .expect(\"Failed to create RecordBatch\");<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">  let record_batch =<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      to_record_batch(<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">           parquet_schema.clone(), &amp;phone_id_counter, partial_contacts)<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">       .expect(\"Failed to create RecordBatch\");<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"></span></span></code></pre>\n<p>After the change, there is no noticeable change in total runtime. On the other hand, there is a noticeable improvement across the board in CPU utilization metrics. Even though the pipeline did not execute any faster, it ran more efficiently.</p>\n<h3 id=\"03:-preallocate-a-string-buffer\" tabindex=\"-1\">03: Preallocate a String Buffer <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#03:-preallocate-a-string-buffer\" aria-hidden=\"true\"></a></h3>\n<p>In the hot loop, where a <code>RecordBatch</code> is being created, a string is allocated in the heap for each generated value. For a run of 10 million rows this is the equivalent of 10 million heap allocations.</p>\n<p>We can eliminate 99% of these allocations by reusing a mutable string buffer within the loop where <code>PartialContact</code> template values are being materialized and appended into the <code>RecordBatch</code>.</p>\n<p>Suppose a <code>RecordBatch</code> is created from a chunk of 1K row values, it now requires only 10K heap allocations.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let mut phone_number_buf = String::with_capacity(16);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    for PartialContact(name, phones) in chunk {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        name_builder.append_option(name);<br /></span></span><br />@@ -155,11 +158,13 @@ fn to_record_batch(<br /><br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    if has_phone_number {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">       let id = phone_id_counter.fetch_add(1, Ordering::Relaxed);<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">       let phone_number = Some(format!(\"+91-99-{id:08}\"));<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">       write!(phone_number_buf, \"+91-99-{id:08}\")?;<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">       struct_builder<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">           .field_builder::&lt;StringBuilder>(PHONE_NUMBER_FIELD_INDEX)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">           .unwrap()<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">           .append_option(phone_number);<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">           .append_value(&amp;phone_number_buf);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">       phone_number_buf.clear();</span></span></code></pre>\n<p>After this change, there is again no noticeable change in the total runtime. But similar to earlier change, all measures point to an overall improvement in the CPU efficiency of the program.</p>\n<h3 id=\"04:-preallocate-a-string-buffer-2\" tabindex=\"-1\">04: Preallocate a String Buffer 2 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#04:-preallocate-a-string-buffer-2\" aria-hidden=\"true\"></a></h3>\n<p>This is a follow up optimization from the previous one. The idea is the same, to eliminate 99% of heap allocations when generating data, by preallocating a mutable string buffer, and reusing it.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">fn name_strategy() -> BoxedStrategy&lt;Option&lt;String>> {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    prop_oneof![<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">        80 => Just(()).prop_map(|_| Some(format!(\"{} {}\", FirstName().fake::&lt;String>(), LastName().fake::&lt;String>()))),<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">        80 => Just(()).prop_map(|_| {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            let mut name_buf = String::with_capacity(32);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            write!(&amp;mut name_buf, \"{} {}\", FirstName().fake::&lt;&amp;str>(), LastName().fake::&lt;&amp;str>()).unwrap();<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            Some(name_buf)<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">         }),<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        20 => Just(None)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    ].boxed()<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">}<br /></span></span></code></pre>\n<p>The results are identical to the previous optimization. No change in the total runtime. But there is considerable improvement in the CPU efficiency of the program.</p>\n<h3 id=\"why-is-the-runtime-unchanged\" tabindex=\"-1\">Why is the Runtime Unchanged? <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#why-is-the-runtime-unchanged\" aria-hidden=\"true\"></a></h3>\n<p>The optimizations so far had little to no effect on the total runtime of the program, which has remained stable.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/hyperfine_boxplot_grid_phase1.png\" alt=\"Hyperfine box plots for baseline version up to run 04\" /></p>\n<p>The flamegraph profiles taken after each optimization also display a similar consistency.</p>\n<p>We have not seen a speedup in the underlying program despite the optimizations is related to <a href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\">Amdhal's law</a>. The pipeline execution spent only a small fraction of its total execution time in the hot loops which were optimized. This is characterized by tall but narrow towers in the flamegraph profile. To achieve a runtime speedup, we have to focus on the widest towers, as they indicate where the most amount of time is spend.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/flamegraph_montage_phase1.png\" alt=\"Flamegraph montage for baseline version up to run 04\" /></p>\n<p>The CPU efficiency has improved across most metrics from the baseline version because of eliminating allocations.</p>\n<p>The same program now executes in less CPU cycles, requires less instructions. Reducing heap allocations is particularly noticeable as reduced cache-references, cache-misses, branch-instructions and branch-misses.</p>\n<p>Even though the runtime has not changed, the user time metric shows that we have shaved off ~2s (from 28s to under 26s) with these optimizations.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/perf_stats_phase1.png\" alt=\"Perf stats for baseline version up to run 04 \" /></p>\n<p>The individual performance counter metrics have improved, but the IPC (instructions per cycle) has gone down from 1.20 to 1.18. Even so, we are now executing the workload using less CPU instructions and cycles. That counts as an efficiency improvement.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/ipc_trend_phase1.png\" alt=\"IPC trend for baseline version up to run 04\" /></p>\n<h2 id=\"phase-2:-architectural-changes\" tabindex=\"-1\">Phase 2: Architectural Changes <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-2:-architectural-changes\" aria-hidden=\"true\"></a></h2>\n<p>The lesson learned from the previous optimizations, is that to speed up the pipeline we need to focus our efforts on the most time consuming parts of the execution. The most obvious optimization then, is to increase the write throughput, by adding more writers.</p>\n<p>We can do a lot better here to improve the speed of the pipeline, by exploiting data parallelism. The data generation is parallelized, but the data encoding to Parquet is not. It is great candidate for making parallel because it is also a compute-bound workload which is now single-threaded.</p>\n<p>The size of 10 million rows on disk in Parquet format is ~292MB, and the program takes ~4s to execute. So we know for certain that the writer thread is not I/O bound. We need to be writing an order of magnitude more bytes to disk to saturate the NVME I/O write speeds.</p>\n<p>We can also optimize the data generation to speed up the program. It currently depends on <a href=\"https://github.com/proptest-rs/proptest\">proptest</a> (a property testing library). I reused it instead of rolling my own, because the <a href=\"https://docs.rs/proptest/latest/proptest/strategy/trait.Strategy.html\">Strategy trait</a> provides a nice API for defining data skew for the fields of the nested data structure. From the flamegraph profile it is evident that it does a lot more work than which is strictly needed in our case.</p>\n<h3 id=\"05:-increase-parquet-encoding-bandwidth\" tabindex=\"-1\">05: Increase Parquet Encoding Bandwidth <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#05:-increase-parquet-encoding-bandwidth\" aria-hidden=\"true\"></a></h3>\n<p>The simplest possible thing to do here is to increase the number of writers from one to two. For that we can partition the data generator threads into two retaining the MPSC pattern. Each partition is connected to a Parquet writer. This effectively doubles the encoding bandwidth, and it requires only a minimal lines of code to be changed.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">    let (tx, rx) = mpsc::sync_channel::&lt;RecordBatch>(num_threads * 2);<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let (tx1, rx1) = mpsc::sync_channel::&lt;RecordBatch>(num_threads);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let (tx2, rx2) = mpsc::sync_channel::&lt;RecordBatch>(num_threads);<br /></span></span><br /><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let writer_handle_1 = create_writer_thread(\"contacts_1.parquet\", rx1);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let writer_handle_2 = create_writer_thread(\"contacts_2.parquet\", rx2);<br /></span></span><br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    let chunk_count = target_contacts.div_ceil(BASE_CHUNK_SIZE);<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    let parquet_schema = get_contact_schema();<br /></span></span><br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    (0..chunk_count)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        .into_par_iter()<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">        .for_each_with(tx, |tx, chunk_index| {<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">        .for_each_with((tx1, tx2), |(tx1, tx2), chunk_index| {<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">            let start_index = chunk_index * BASE_CHUNK_SIZE;<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">            let current_chunk_size = std::cmp::min(BASE_CHUNK_SIZE, target_contacts - start_index);<br /></span></span><br />@@ -263,11 +275,17 @@ fn main() -> Result&lt;(), Box&lt;dyn Error + Send + Sync>> {<br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">                to_record_batch(parquet_schema.clone(), &amp;phone_id_counter, partial_contacts)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">                    .expect(\"Failed to create RecordBatch\");<br /></span></span><br /><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">            tx.send(record_batch).unwrap();<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">            if chunk_index % 2 == 0 {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">                tx1.send(record_batch).unwrap();<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            } else {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">                tx2.send(record_batch).unwrap();<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            }<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        });<br /></span></span><br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    // Teardown<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">    writer_handle.join().unwrap()?;<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    writer_handle_1.join().unwrap()?;<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    writer_handle_2.join().unwrap()?;<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"></span></span></code></pre>\n<p>This shaved off ~1s (3.6s to 2.7s) from execution time. This is significant, and I need to now find if adding more writers will further reduce the runtime. The major gains are not going to come from this it.</p>\n<h3 id=\"06:-make-data-generation-lightweight\" tabindex=\"-1\">06: Make Data Generation Lightweight <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#06:-make-data-generation-lightweight\" aria-hidden=\"true\"></a></h3>\n<p>The flamegraph profile shows that over 80% of the pipeline execution time is spend in the data generation methods. It is the most dominant factor we need to focus on. Around 20% of that time is spend in <a href=\"https://docs.rs/proptest/latest/proptest/strategy/trait.Strategy.html#tymethod.new_tree\">Strategy::new_tree</a> alone, which is the entry point for data generation.</p>\n<p>There is a lot of performed here which does not contribute to data generation, but is necessary for a test runner. We can eliminate this extra work by implementing light-weight functions, but keeping the ergonomic API design.</p>\n<p>Maybe, I could have done better at the beginning by rolling my own implementation. But the goal at the beginning was to have a correct, simple working implementation. Performance is important, but it will have been pure guesswork if I had predicted that this will so dominant in the runtime. The other reason is I like the ergonomic API design, which I can copy in the light-weight implementation.</p>\n<p>The diff below shows the implementation for the <code>phone_type</code> field in the nested data structure. You can see the structural similarities between the old and new versions of the code.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">// A Zipfian-like categorical distribution for `phone_type`<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">//<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// | Phone Type | Probability |<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// |------------|-------------|<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// | Mobile     | 0.55        |<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// | Work       | 0.35        |<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">// | Home       | 0.10        |<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">//<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">fn phone_type_strategy() -> BoxedStrategy&lt;PhoneType> {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    prop_oneof![<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        55 => Just(PhoneType::Mobile),<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        35 => Just(PhoneType::Work),<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        10 => Just(PhoneType::Home),<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    ]<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    .boxed()<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">fn generate_phone_type(rng: &amp;mut impl Rng) -> PhoneType {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    match rng.random_range(0..100) {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        0..=54 => PhoneType::Mobile, // 0.55<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        55..=89 => PhoneType::Work,  // 0.35<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        _ => PhoneType::Home,        // 0.10<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    }<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">}<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"></span></span></code></pre>\n<p>This refactoring has to be applied uniformly for every field and methods which compose nested fields. The diff below shows how the property-testing runner is replaced with a simple for loop. In this refactoring we completely eliminate the <code>proptest</code> dependency.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\">diff --git a/Cargo.lock b/Cargo.lock<br />index 626375c..a6b06a8 100644<br /><span class=\"token coord\">--- a/Cargo.lock</span><br /><span class=\"token coord\">+++ b/Cargo.lock</span><br />@@ -2322,7 +2322,7 @@ dependencies = [<br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\"> \"log\",<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\"> \"parquet\",<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\"> \"parquet-common\",<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\"> \"proptest\",<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\"> \"rand 0.9.1\",<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\"> \"rayon\",<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">]<br /></span></span><br />diff --git a/crates/parquet-parallel-nested/src/main.rs b/crates/parquet-parallel-nested/src/main.rs<br />index f6d8610..0fa2083 100644<br /><span class=\"token coord\">--- a/crates/parquet-parallel-nested/src/main.rs</span><br /><span class=\"token coord\">+++ b/crates/parquet-parallel-nested/src/main.rs</span><br /><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">fn generate_contacts_chunk(size: usize, seed: u64) -> Vec&lt;PartialContact> {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let mut rng = StdRng::seed_from_u64(seed);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let mut contacts = Vec::with_capacity(size);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let mut name_buf = String::with_capacity(32);<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    for _ in 0..size {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        contacts.push(generate_partial_contact(&amp;mut rng, &amp;mut name_buf));<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    }<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    contacts<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">}<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\"><br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">fn generate_contacts_chunk(size: usize, seed: u64) -> Vec&lt;PartialContact> {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    let mut runner = TestRunner::new(Config {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        rng_seed: RngSeed::Fixed(seed),<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        ..Config::default()<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    });<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\"><br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    let strategy = proptest::collection::vec(contact_strategy(), size);<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    strategy<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        .new_tree(&amp;mut runner)<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        .expect(\"Failed to generate chunk of partial contacts\")<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        .current()<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">}</span></span></code></pre>\n<p>The pipeline is now 2.3x faster. The total runtime decreased from 2.70s to 1.18s (~1.5s). The IPC (instructions per cycle) nearly doubled, from 1.05 to 2.02. Every other stat shows similar improvements.</p>\n<p>This is a strong result. The program speed increased, and it is also now more efficient in core utilization.</p>\n<h3 id=\"07:-increase-parquet-writer-threads\" tabindex=\"-1\">07: Increase Parquet Writer Threads <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#07:-increase-parquet-writer-threads\" aria-hidden=\"true\"></a></h3>\n<p>The Parquet encoding step remains a bottleneck as the data generators outpace the two writer threads. A simple test is to see the effect of doubling the writers again.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">    if chunk_index % 2 == 0 {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">            tx1.send(record_batch).unwrap();<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        } else {<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">            tx2.send(record_batch).unwrap();<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">        }<br /></span><span class=\"token prefix deleted\">-</span><span class=\"token line\">    });<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">    let record_batch =<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">       to_record_batch(parquet_schema.clone(), &amp;phone_id_counter, partial_contac<br /></span></span>ts)<br /><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">       .expect(\"Failed to create RecordBatch\");<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    match chunk_index % 4 {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        0 => s1.send(record_batch).expect(\"Failed to send to rx1\"),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        1 => s2.send(record_batch).expect(\"Failed to send to rx2\"),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        2 => s3.send(record_batch).expect(\"Failed to send to rx3\"),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">        _ => s4.send(record_batch).expect(\"Failed to send to rx4\"),<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    }</span></span></code></pre>\n<p>Another ~1.5x speedup in runtime. The total runtime dropped below a second (800ms) for the first time. On the other hand, the IPC dropped to 1.76 from the previous high.</p>\n<h3 id=\"08:-introduce-thread-local-state\" tabindex=\"-1\">08: Introduce Thread-Local State <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#08:-introduce-thread-local-state\" aria-hidden=\"true\"></a></h3>\n<p>The flamegraph profile now shows that around 20% of the time is spend in resizing vectors, and cloning strings in data generation.</p>\n<p>The current data generation is stateless. As soon as a chunk of nested records are created, a <code>RecordBatch</code> is created. And this is immediately send to the writer thread. The chunk size setting is hard-coded as 256. It creates ~39K <code>RecordBatches</code> for 10 million records. We could increase chunk size, but a better thing to do here is decouple the chunk size from the row count at which we finalize a <code>RecordBatch</code>, so that they can be tuned separately.</p>\n<p>For example, if the chunk size is 256, we can configure a <code>RecordBatch</code> to be finalized when we have 5K nested records. Now only ~2K <code>RecordBatch</code>es are created for a run of 10 million records. For this we introduce <code>GeneratorState</code> struct which contains the <code>RecordBatch</code> fields to which we are appending the chunk values.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">struct GeneratorState {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    schema: SchemaRef,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    name: StringBuilder,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    phone_number_buf: String,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    counter: Arc&lt;AtomicUsize>,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    phones: ListBuilder&lt;StructBuilder>,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    current_chunks: usize,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">}<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">enum GeneratorStateError {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    NotEnoughChunks { current: usize, required: usize },<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">    TryFlushZeroChunks,<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">}<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span></span>@@ -411,38 +351,95 @@ fn main() -> Result&lt;(), Box&lt;dyn Error + Send + Sync>> {<br /><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        .num_threads(num_producers)<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        .build()<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        .unwrap();<br /></span></span><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\"><br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">    pool.install(|| {<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        let chunk_count = target_contacts.div_ceil(BASE_CHUNK_SIZE);<br /></span><span class=\"token prefix unchanged\"> </span><span class=\"token line\">        let parquet_schema = get_contact_schema();<br /></span></span><br /><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">        (0..num_producers).into_par_iter().for_each(|producer_id| {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            // Each thread gets its own state and a clone of the senders.<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">            let mut generator_state =<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">                GeneratorState::new(parquet_schema.clone(), phone_id_counter.clone());</span></span></code></pre>\n<p>This reduces the runtime by another 25% (~800ms to ~600ms). We have also regained IPC and it is at an all time high of 2.21. Every efficiency parameter has improved.</p>\n<p>Yet another significant improvement both in speedup and efficiency of the program's execution.</p>\n<h3 id=\"measuring-impact\" tabindex=\"-1\">Measuring Impact <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#measuring-impact\" aria-hidden=\"true\"></a></h3>\n<p>We improved the total pipeline throughput by increasing the writers. The data generator got lighter, and faster. Finally, we increased the <code>RecordBatch</code> size.</p>\n<p>All of the above optimizations to the pipeline has resulted in a 6X speedup, with the runtime dropping from 3.61s to 0.58s.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/hyperfine_boxplot_grid_phase2.png\" alt=\"Hyperfine box plots from run 04 to run 08\" /></p>\n<p>The cores are now being utilized more efficiently, with every stats we tracked improving significantly.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/perf_stats_phase2.png\" alt=\"Perf stats from run 04 to run 08\" /></p>\n<p>The IPC improved from 1.18 to 2.21 (an 87% increase).</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/ipc_trend_phase2.png\" alt=\"IPC trend from run 04 to run 08\" /></p>\n<p>The final flamegraph shows a concentrated workload which is evenly divided. The data generator (producer) threads profile occupies the left side, while the parquet writer (consumer) threads profile occupies to the right side. The transition to the final flamegraph, shows a clear improvement from a fragmented hotspots to more efficient pipeline execution.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/flamegraph_montage_phase2.png\" alt=\"Flamegraph montage from run 04 to run 08\" /></p>\n<h2 id=\"phase-3:-a-performance-regression\" tabindex=\"-1\">Phase 3: A Performance Regression <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#phase-3:-a-performance-regression\" aria-hidden=\"true\"></a></h2>\n<p>The generated <code>name</code> field is <code>NULL</code> 20% of the time. When generating 10 million rows, we therefore expect the <code>name</code> column to contain roughly 8 million name strings. To my surprise, the total no. of unique names were only ~1.4 million.</p>\n<p>When generating millions of <code>fake</code> names, the chance of a collision becomes very high. The <code>fake</code> implementation is most likely sampling first name, last name pairing with replacement. Since we are generating a large number of names, the collisions become more frequent. This is also known as the <a href=\"https://en.wikipedia.org/wiki/Birthday_problem\">Birthday problem</a>.</p>\n<p>It looks like we can minimize string allocations by 82.5% because only ~1.4 million unique names are generated for a run of total size 10 million.</p>\n<h3 id=\"09:-global-string-interning\" tabindex=\"-1\">09: Global String Interning <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#09:-global-string-interning\" aria-hidden=\"true\"></a></h3>\n<p>When a new name is generated, we check to see if it is unique in a global hashmap. If it is new and unique it is added to the hashmap. If it already exists in the hashmap, we reuse the allocated string stored within the hashmap.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">  let name = Some(name_buf.clone());<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">  let name = if let Some(interned) = interner.get(name_buf) {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      interned.clone()<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">  } else {<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      let new_arc = Arc::new(name_buf.clone());<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      interner.insert(name_buf.clone(), new_arc.clone());<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">      new_arc<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">  };<br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\"><br /></span><span class=\"token prefix inserted\">+</span><span class=\"token line\">  // clear the name buffer for next use<br /></span></span><span class=\"token unchanged\"><span class=\"token prefix unchanged\"> </span><span class=\"token line\">  name_buf.clear();<br /></span></span><br /><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">  name<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">  Some(name)</span></span></code></pre>\n<p>The savings are realized when we finally add the generated name when constructing the <code>RecordBatch</code>.</p>\n<pre class=\"language-diff\"><code class=\"language-diff\"><span class=\"token deleted-sign deleted\"><span class=\"token prefix deleted\">-</span><span class=\"token line\">            name.append_option(generate_name(rng, &amp;mut name_buf));<br /></span></span><span class=\"token inserted-sign inserted\"><span class=\"token prefix inserted\">+</span><span class=\"token line\">            name.append_option(generate_name(rng, &amp;mut name_buf, &amp;self.interner).as_deref());</span></span></code></pre>\n<p>An extra allocation is avoided by directly passing a reference to the interned string.</p>\n<h3 id=\"10:-revert\" tabindex=\"-1\">10: Revert <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#10:-revert\" aria-hidden=\"true\"></a></h3>\n<p>The benchmarks shows a runtime regression of 65%, from 583ms to 965ms. The IPC halved from 2.21 to 1.10. The cache misses increased to 22% from 10%. The measurements leaves no doubt, this is a clear performance regression.</p>\n<p>But why did string interning not work?</p>\n<p>The data generator threads are generating at a rapid pace, and the names are being inserted into the same internal buckets. These may fit into a cache line, and shared across cores. But the high volume writes are constantly invalidating the cache lines, and this points to a cache coherency issue. A data generator thread can invalidate the cache line when another data generator thread is attempting to access the same cache line to get a reference to the interned string. This causes the CPU to stall, and wait for the new cache line to be fetched. Both problems are visible in the lower IPC and higher cache miss rate.</p>\n<p>While reverting the code I noticed that the earlier version, was cloning the name string buffer before passing it the <code>name</code> field builder. This is not necessary, as we can pass the reference directly without cloning, as Arrow will make a copy internally. So an extra clone was removed in the end.</p>\n<p>The median runtime is now 533ms from 584ms, shaving off another 51ms from the final runtime.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/hyperfine_boxplot_grid_phase3.png\" alt=\"Hyperfine box plots from run 08 to run 10\" /></p>\n<p>The changes in hardware performance counters are negligible in most cases, but the total instructions, and branch instructions have reduced significantly. This could be attributed to removing the unnecessary cloning of the mutable buffer.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/perf_stats_phase3.png\" alt=\"Perf stats from run 08 to run 10\" /></p>\n<h2 id=\"a-back-of-the-envelope-estimation\" tabindex=\"-1\">A Back of the Envelope Estimation <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#a-back-of-the-envelope-estimation\" aria-hidden=\"true\"></a></h2>\n<p>The pipeline throughput is dependent on its slowest stage. In micro-benchmarks, where the data generator and writer throughput is measured on a single core, the performance is comparable.</p>\n<table>\n<thead>\n<tr>\n<th>Record Batch Size</th>\n<th>Data Generation Time per Record (ns)</th>\n<th>Write Time per Record (ns)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1024</td>\n<td>140.89</td>\n<td>135.30</td>\n</tr>\n<tr>\n<td>2048</td>\n<td>145.76</td>\n<td>133.05</td>\n</tr>\n<tr>\n<td>4096</td>\n<td>149.08</td>\n<td>131.51</td>\n</tr>\n<tr>\n<td>8192</td>\n<td>149.66</td>\n<td>129.82</td>\n</tr>\n</tbody>\n</table>\n<p>Therefore for this workload, the optimal ratio of producers to consumers is close to 1:1. On the 16-core machine used for end-end benchmarking, we should see the best performance when using 8 generator threads paired together with 8 writer threads.</p>\n<p>From the table we can calculate, the theoretical ceiling for throughput which comes to ~7 million records/second/core. Our highest observed throughput value is 21 millions/records/second on 16 cores with 8-writers, which is ~2.6 million records/second/core.</p>\n<p>On 16-cores, the user time is 4.82s and system time is 0.71s for a combined 5.53s. The total wall clock time is 0.51s. This means we are effectively using 11 cores (5.53s / 0.51s) of the total available 16 cores. This is a high-level of parallelism, where for the duration of the pipelines execution 11 cores are fully busy.</p>\n<p>For future improvements, the single-core efficiency has to be improved, but where?</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/flamegraph-8w-8192rb.png\" alt=\"Flamegraph of latest version after all optimizations are applied\" /></p>\n<p>The flamegraph shows a equal split of work distribution between the data generation threads and writer threads. The next bottleneck appears as the overhead of Rayon in dividing the work between the data generation threads. The data generator is extremely fast that the overhead of distributing work is greater.</p>\n<p>For a run of 10 million rows, with a record batch size of 8192, we now generate 1221 small batches for Rayon to distribute in parallel to cores running the data generator threads. A single batch completes in 1.2ms, so Rayon has to do constantly schedule tasks to cores at the rate of 833 tasks/second. The next optimization should target reducing this overhead.</p>\n<h2 id=\"fine-tuning-configuration-for-optimal-performance\" tabindex=\"-1\">Fine-tuning Configuration for Optimal Performance <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/#fine-tuning-configuration-for-optimal-performance\" aria-hidden=\"true\"></a></h2>\n<p>Keeping the target rows constant at 10 million, we can execute the pipeline a range of record batch sizes, writer threads to compare record throughput (million records/second) and total runtime.</p>\n<pre class=\"language-text\"><code class=\"language-text\">$ ./target/release/parquet-nested-parallel --help<br />A tool for generating and writing nested Parquet data in parallel<br /><br />Usage: parquet-nested-parallel [OPTIONS]<br /><br />Options:<br />      --target-records <TARGET_RECORDS><br />          The target number of records to generate [default: 10000000]<br />      --record-batch-size <RECORD_BATCH_SIZE><br />          The size of each record batch [default: 4096]<br />      --num-writers <NUM_WRITERS><br />          The number of parallel writers [default: 4]<br />      --output-dir <OUTPUT_DIR><br />          The output directory for the Parquet files<br />      --output-filename <OUTPUT_FILENAME><br />          The base filename for the output Parquet files<br />      --dry-run<br />          Do not execute the pipeline<br />  -h, --help<br />          Print help<br />  -V, --version<br />          Print version</OUTPUT_FILENAME></OUTPUT_DIR></NUM_WRITERS></RECORD_BATCH_SIZE></TARGET_RECORDS></code></pre>\n<p>In the micro-benchmark comparison from earlier, the single-core performance of the data generator and writer threads are comparable. So a best performance was predicted to come from a 1:1 allocation of CPU cores between the data generation and writer threads.</p>\n<p>Below we can see that 8 writers, 8 producers produces the highest observed throughput of 23 million records/second. A higher record batch size has little to no effect beyond 10K. With 6 writers, 10 producers, the throughput is above 20 million record/second, but is not optimal because of the imbalanced allocation.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/performance_heatmap_10M_record_throughput_m_sec.png\" alt=\"Record Throughput Analysis\" /></p>\n<p>No surprises here as well. The lowest recorded runtime is when we have a 1:1 allocation, with 8 writers and 8 producer threads.</p>\n<p><img src=\"https://jacobsherin.com/posts/2025-09-01-arrow-shredding-pipeline-perf/img/performance_heatmap_10M_total_time_ms.png\" alt=\"Total Runtime Analysis\" /></p>\n<p>To recap, future optimizations should target reducing the Rayon threadpool overhead to improve single-core efficiency. The current pipeline achieves a high-level of parallelism by being able to fully utilize 11 out of 16 available cores. There is close to ~3X headroom remaining for improving the current ~2.6 million records/per/core throughput, to a maximum possible ~7 million records/per/core throughput. Since the bottleneck changes after each optimization, it is therefore important to continue with a data-driven approach.</p>\n",
      "date_published": "2025-09-02T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/",
      "url": "https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/",
      "title": "Practical Hurdles In Crab Latching Concurrency",
      "content_html": "<p>An implementation of the crab latching protocol enforces a strict top-down order for acquiring latches on a B+Tree node. This avoid deadlocks from ever occurring during concurrent operations. This is distinct from deadlock detection and resolution which is a runtime mechanism.</p>\n<p>Deadlock avoidance is guaranteed by design through careful programming of critical sections in the code. Any mistakes here will result in deadlocks. Even worse, a data race which silently corrupts the index.</p>\n<p>The main strength of a B+Tree index (compared to a hash index) is its unique capability to perform range scans. This is possible because all the entries are stored in key lexicographic order in the leaf nodes, and the leaf nodes themselves are connected to each other like a doubly-linked list. So scanning is efficient once you locate the starting leaf node. Scanning in ascending or descending key order is as simple as following the left or right sibling pointers.</p>\n<p>This forwards or backwards movement during index scans violates the strict top-down ordering required for safety and correctness by the crab latching protocol.</p>\n<p>A delete algorithm which implements a symmetrical tree rebalancing procedure requires acquiring an exclusive latch on either a left or right sibling for merging nodes. There is an equal chance of nodes merging left-right and right-left. This too violates the strict ordering requirement.</p>\n<p>Therefore, an implementation has to come up with practical methods to avoid serial execution order and preserve concurrency. There is no formal verification of correctness via proof in these scenarios. We can improve our confidence in the implementation through engineering effort: code reviews, test suites, analyzers (ThreadSanitizer). Though the existence of data races cannot be ruled out, in practice this is sufficient for a robust and reliable implementation as evidenced by major OLTP systems.</p>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#an-overview-of-crab-latching\">An Overview Of Crab Latching</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#the-classic-deadlock-scenario\">The Classic Deadlock Scenario</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#crab-latching-is-efficient\">Crab Latching is Efficient</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#optimistic-concurrency\">Optimistic Concurrency</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#concurrent-index-scans\">Concurrent Index Scans</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#extension-for-concurrent-bi-directional-scans\">Extension For Concurrent Bi-directional Scans</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#deadlock:-lock-order-inversion\">Deadlock: Lock Order Inversion</a></li>\n      </ul>\n    </li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#extension-for-symmetric-deletion\">Extension For Symmetric Deletion</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#concurrent-scans-can-miss-entries\">Concurrent Scans Can Miss Entries</a></li>\n  </ol>\n</nav>\n<h2 id=\"an-overview-of-crab-latching\" tabindex=\"-1\">An Overview Of Crab Latching <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#an-overview-of-crab-latching\" aria-hidden=\"true\"></a></h2>\n<p>A thread can acquire either a shared read-only latch (s-latch) or an exclusive write latch (x-latch). A thread holding an x-latch blocks other threads until it completes its mutation of the node. Multiple reader threads holding an s-latch can concurrently access a node but are prevented from mutating the node itself. This is a basic single writer, multiple readers concurrency pattern.</p>\n<p>In the crab latching protocol, during traversal a thread holding a latch on a B+Tree node attempts to acquire a latch on the next node. It must release the latch on the previous node only after it has acquired the latch on the next node. The name likely comes from how this mimics the latching movement of a crab.</p>\n<h3 id=\"the-classic-deadlock-scenario\" tabindex=\"-1\">The Classic Deadlock Scenario <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#the-classic-deadlock-scenario\" aria-hidden=\"true\"></a></h3>\n<p>A deadlock occurs when two threads attempt to crab latch going in opposing directions. Consider a simple scenario with a parent Node-P and a child Node-C:</p>\n<ol>\n<li>Thread-1 traverses <em>top-down</em> from Node-P to Node-C to insert a new element.</li>\n<li>Thread-2 moves <em>bottom-up</em>, from Node-C to update a pivot key in its parent Node-P.</li>\n</ol>\n<p>Remember that in crab latching protocol, a thread has to first acquire the latch on the next node before it can release the existing latch it holds on the current node. Also, an x-latch allows access to only a single writer thread. Therefore both write operations will block each other indefinitely as they cross paths, creating a deadlock as shown below.</p>\n<figure>\n              <img src=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/img/btree-deadlock-wait.svg\" alt=\"circular wait deadlock\" />\n              <figcaption>Figure 1. Thread-1 has acquired an exclusive write latch (x-latch) on Node-P and it is blocked waiting to acquire an x-latch on Node-C. Thread-2 already holds an x-latch on Node-C and it is blocked waiting to acquire an x-latch on Node P.</figcaption>\n            </figure>\n<p>A simple rule prevents this situation: acquire latches moving in only one direction. Since all B+Tree operations begin with a top-down traversals, enforcing a top-down order is the natural way to prevent deadlocks from occurring by design.</p>\n<h3 id=\"crab-latching-is-efficient\" tabindex=\"-1\">Crab Latching is Efficient <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#crab-latching-is-efficient\" aria-hidden=\"true\"></a></h3>\n<blockquote>\n<p>Latches are held only during a critical section, that is, while a data structure is read or updated. Deadlocks are avoided by appropriate coding disciplines, for example, requesting multiple latches in carefully designed sequences. Deadlock resolution requires a facility to rollback prior actions, whereas deadlock avoidance does not. Thus, deadlock avoidance is more appropriate for latches, which are designed for minimal overhead\nand maximal performance and scalability. Latch acquisition and release may\nrequire tens of instructions only, usually with no additional cache faults since a latch can be embedded in the data structure it protects.</p>\n<p>Goetz Graefe, <a href=\"https://15721.courses.cs.cmu.edu/spring2016/papers/a16-graefe.pdf\">A Survey of B-Tree Locking Techniques (2010)</a></p>\n</blockquote>\n<p>As Graefe notes, latches are designed for performance. By embedding a latch within each B+Tree node and holding it only for the brief duration of the operation, the protocol achieves fine-grained latching. This approach minimizes contention between threads and maximizes throughput for concurrent operations.</p>\n<h3 id=\"optimistic-concurrency\" tabindex=\"-1\">Optimistic Concurrency <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#optimistic-concurrency\" aria-hidden=\"true\"></a></h3>\n<p>Crab latching is a pessimistic concurrency protocol. During a write, a node may split or merge, cascading changes up to the root. In this approach, a thread acquires an x-latch along the entire path from root to leaf. A single writer holding the root's x-latch blocks all other threads, effectively reducing concurrency to sequential execution.</p>\n<p>The optimistic approach takes advantage of the fact that, in practice, node splits and merges are rare. In this &quot;fast path&quot;, the writer thread traverses the tree using s-latches, only upgrading to an x-latch on the final leaf node to perform the modification.</p>\n<p>This requires minimal overhead to check if a child node is &quot;safe&quot; or &quot;unsafe&quot;. A node is unsafe if the write will cause a split or merge. If it encounters an unsafe node, the thread abandons the optimistic approach. It upgrades its latch on the parent to an x-latch and then acquires x-latches for the rest of the path segment. This &quot;slow path&quot; ensures rebalancing operations can proceed safely.</p>\n<p>A notable benefit, even in the slow path, is that the x-latch path segment may be held on only a subsection of the tree, not extending all the way back to the root. This allows other operations to concurrently access different parts of the B+Tree.</p>\n<h2 id=\"concurrent-index-scans\" tabindex=\"-1\">Concurrent Index Scans <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#concurrent-index-scans\" aria-hidden=\"true\"></a></h2>\n<p>The fundamental problem is that the concurrency models do not take into consideration B+Tree iterators. At the leaf node, traversing to a sibling uses the bi-directional links between leaf nodes. An ascending scan moves from left-right, while a descending scan moves from right-left. This conflicts with the safety property for avoiding deadlocks that traversals have a strictly enforced direction. Following the protocol exactly means the implementation can provide only one type of scan, either forward (ascending) or reverse (descending), but never both together.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// A forward index scan</span><br /><span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">auto</span> iter <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">Begin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> iter <span class=\"token operator\">!=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">End</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>iter<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">int</span> key <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">int</span> value <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>second<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// A reverse index scan</span><br /><span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">auto</span> iter <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">RBegin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> iter <span class=\"token operator\">!=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">REnd</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token operator\">--</span>iter<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">int</span> key <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">int</span> value <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>second<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">/**<br /> * index.Begin()  : first element<br /> * index.End()    : one-past-the-last element<br /> * index.RBegin() : last element<br /> * index.REnd()   : one-past-the-first element<br /> */</span></code></pre>\n<h3 id=\"extension-for-concurrent-bi-directional-scans\" tabindex=\"-1\">Extension For Concurrent Bi-directional Scans <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#extension-for-concurrent-bi-directional-scans\" aria-hidden=\"true\"></a></h3>\n<p>A shared (read-only) or exclusive (write) latch blocks execution until the latch is acquired. For scans which are sideways traversals, we do not want to limit the traversal to any one direction. A blocking latch will create deadlocks if two concurrent scans proceed in opposite directions.</p>\n<p>Therefore we need to use a non-blocking latch to prevent blocking and avoid deadlocks. A non-blocking latch will try to acquire a latch, and will return immediately.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;shared_mutex></span></span><br /><br /><span class=\"token keyword\">class</span> <span class=\"token class-name\">SharedLatch</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// Blocking</span><br />  <span class=\"token comment\">//</span><br />  <span class=\"token comment\">// If another thread holds the latch, execution will block</span><br />  <span class=\"token comment\">// until the latch is acquired.</span><br />  <span class=\"token comment\">//</span><br />  <span class=\"token comment\">// Used in insert, delete &amp; search index operations</span><br />  <span class=\"token keyword\">void</span> <span class=\"token function\">LockShared</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />      latch_<span class=\"token punctuation\">.</span><span class=\"token function\">lock_shared</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token comment\">// Non-blocking</span><br />  <span class=\"token comment\">//</span><br />  <span class=\"token comment\">// Tries to acquire a latch. If successful returns `true`,</span><br />  <span class=\"token comment\">// otherwise returns `false`.</span><br />  <span class=\"token comment\">//</span><br />  <span class=\"token comment\">// Used in ascending &amp; descending index scans</span><br />  <span class=\"token keyword\">bool</span> <span class=\"token function\">TryLockShared</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token keyword\">return</span> latch_<span class=\"token punctuation\">.</span><span class=\"token function\">try_lock_shared</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token keyword\">private</span><span class=\"token operator\">:</span><br />    <span class=\"token comment\">// A wrapper around std::shared_mutex</span><br />    std<span class=\"token double-colon punctuation\">::</span>shared_mutex latch_<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Using <code>TryLockShared()</code> forces us rethink how a scan implementation should work if latch acquisition fails. In contrast, the concurrent insert, delete and search implementations are always expected to return a result matching its output type in the signature.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Returns `std::nullopt` if the key is not found in the index</span><br />std<span class=\"token double-colon punctuation\">::</span>optional<span class=\"token operator\">&lt;</span>KeyType<span class=\"token operator\">></span> <span class=\"token function\">MaybeGet</span><span class=\"token punctuation\">(</span>KeyType key<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token comment\">// Returns `false` if key is a duplicate.</span><br /><span class=\"token comment\">//</span><br /><span class=\"token comment\">// Note: This prevents overwriting an existing key. The handling of</span><br /><span class=\"token comment\">// duplicate keys is an implementation specific detail.</span><br /><span class=\"token keyword\">bool</span> <span class=\"token function\">Insert</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> KeyValuePair element<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token comment\">// Returns `false` if the key does not exist.</span><br /><span class=\"token keyword\">bool</span> <span class=\"token function\">Delete</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> KeyType key_to_remove<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>We can introduce a failure mode, where if latch acquisition fails during a scan we set its internal state to <code>RETRY</code>.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">enum</span> <span class=\"token class-name\">IteratorState</span> <span class=\"token punctuation\">{</span><br />    VALID<span class=\"token punctuation\">,</span> INVALID<span class=\"token punctuation\">,</span> END<span class=\"token punctuation\">,</span> REND<span class=\"token punctuation\">,</span> RETRY<br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre>\n<p>The implementation for forward scan which uses a non-blocking latch and retriable iterator looks like this:</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Move forward one element at a time. If latch acquisition</span><br /><span class=\"token comment\">// failed, then set internal state to `RETRY`.</span><br /><span class=\"token keyword\">void</span> <span class=\"token keyword\">operator</span><span class=\"token operator\">++</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// ...</span><br /><br />  <span class=\"token comment\">// The `TrySharedLock()` is a non-blocking read-only latch</span><br />  <span class=\"token comment\">// which returns `true` or `false` immediately.</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">!</span><span class=\"token punctuation\">(</span>current_node_<span class=\"token operator\">-></span><span class=\"token function\">TrySharedLock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    previous_node<span class=\"token operator\">-></span><span class=\"token function\">ReleaseNodeSharedLatch</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />    <span class=\"token comment\">// Invalidates the iterator.</span><br />    <span class=\"token comment\">// Sets internal state to `RETRY`.</span><br />    <span class=\"token function\">SetRetryIterator</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    <span class=\"token keyword\">return</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token comment\">// ...</span><br /><br /><span class=\"token punctuation\">}</span><br /><br /><br /><span class=\"token keyword\">void</span> <span class=\"token function\">SetRetryIterator</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// Resets internal state</span><br />  current_node_ <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span><br />  current_element_ <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span><br /><br />  state_ <span class=\"token operator\">=</span> RETRY<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>We have a working bi-directional iterator implementation which will avoid deadlocks, but it is not yet free from data races.</p>\n<h3 id=\"deadlock:-lock-order-inversion\" tabindex=\"-1\">Deadlock: Lock Order Inversion <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#deadlock:-lock-order-inversion\" aria-hidden=\"true\"></a></h3>\n<p>The current API introduces a deadlock if the user initializes two iterators within the same scope, within the same thread.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">auto</span> iter_forward <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">Begin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token comment\">// The second iterator creates a lock order inversion.</span><br /><span class=\"token keyword\">auto</span> iter_reverse <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">RBegin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>A deadlock is prevented by enforcing a strict direction for latching. Any concurrent operation must therefore acquire a latch on an ancestor node before acquiring a latch on a descendant node (top-down traversal).</p>\n<p>The iterator here holds a shared (read-only) latch on the leaf it points to. If that iterator remains alive while another operation begins a new top-down traversal from the root, we can get a deadlock.</p>\n<p>(<code>Thread 1</code>): Creates a forward iterator, which holds a shared latch on a leaf node.</p>\n<p>(<code>Thread 2</code>): Begins an insert in the pessimistic path acquiring an exclusive latch beginning at the root node all the way down to the parent of the same leaf node.</p>\n<p>(<code>Thread 2</code>): Now attempts to acquire an exclusive latch on the leaf node but it blocks, waiting for the forward iterator to complete and release its shared latch on the leaf node.</p>\n<p>(<code>Thread 1</code>): Creates a second reverse iterator, which is now blocking to acquire a shared lock on the root. It is waiting for the insert operation to release the exclusive latch.</p>\n<p>This creates a deadlock, even though in implementation we enforced a strict ordering. We can ensure that this does not happen by ensuring that iterator lifetimes do not intersect each other, by introducing a local scope.</p>\n<p>Most importantly, the shared latch is released at the end of the scope and therefore it also enforces a global ordering for latch acquisition and will not result in a deadlock like described above.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">auto</span> iter_forward <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">Begin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// The lifetime of the first iterator ends before this</span><br /><span class=\"token comment\">// scope starts. This guarantees that the shared latch</span><br /><span class=\"token comment\">// on the leaf node is released before we start traversal</span><br /><span class=\"token comment\">// from the root node.</span><br /><span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">auto</span> iter_reverse <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span><span class=\"token function\">RBegin</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>To ensure safety, the latching protocol has to be enforced for concurrent operations within the same thread. Unfortunately, our non-blocking, retriable concurrent scan iterators has introduced an API which is easy for the user to incorrectly implement, and must come with warnings.</p>\n<p>The pattern of creating two iterators in the same scope creates a lock-order-inversion within a single thread. While this does not create a deadlock by itself, because of shared latches, it creates the precondition for a deadlock with any concurrent operation which falls down the pessimistic concurrency path.</p>\n<h2 id=\"extension-for-symmetric-deletion\" tabindex=\"-1\">Extension For Symmetric Deletion <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#extension-for-symmetric-deletion\" aria-hidden=\"true\"></a></h2>\n<p>A symmetric delete algorithm will proceed with a tree rebalancing operation after an underflow by attempting to merge with either the left sibling, and if that doesn't work with the right sibling at any level in the B+Tree. This violates our strict ordering principle for acquiring latches, because a merge can proceed in either direction.</p>\n<p>(<code>Thread 1</code>): operating on Node B, needs to merge with its previous sibling, Node A. It holds an exclusive latch on B and tries to acquire an exclusive latch on A. (left to right)</p>\n<p>(<code>Thread 2</code>): operating on Node A, needs to merge with its next sibling, Node B. It holds an exclusive latch on A and tries to acquire an exclusive latch on B. (right to left)</p>\n<p>The creates a deadlock.</p>\n<p>The fix is to enforce a strict one-way (say left-to-right) latch acquisition order. This is a delicate operation. Before locking the previous sibling A, the exclusive lock on the current node is released. The locks are reacquired in the correct order. First on the previous sibling, then on the current node.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><br />  <span class=\"token keyword\">auto</span> maybe_previous <span class=\"token operator\">=</span> <span class=\"token generic-function\"><span class=\"token function\">static_cast</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span>InnerNode <span class=\"token operator\">*</span><span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span>parent<span class=\"token punctuation\">)</span><span class=\"token operator\">-></span><span class=\"token function\">MaybePreviousWithSeparator</span><span class=\"token punctuation\">(</span>key_to_remove<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>maybe_previous<span class=\"token punctuation\">.</span><span class=\"token function\">has_value</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br /><br />    <span class=\"token comment\">/* ... */</span><br /><br />    <span class=\"token comment\">// Enforcing a strict left-to-right (one-way) ordering</span><br />    right<span class=\"token operator\">-></span><span class=\"token function\">ReleaseNodeExclusiveLatch</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    left<span class=\"token operator\">-></span><span class=\"token function\">GetNodeExclusiveLatch</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    right<span class=\"token operator\">-></span><span class=\"token function\">GetNodeExclusiveLatch</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />    <span class=\"token comment\">/* ... */</span><br />  <span class=\"token punctuation\">}</span></code></pre>\n<h2 id=\"concurrent-scans-can-miss-entries\" tabindex=\"-1\">Concurrent Scans Can Miss Entries <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-21-bplustree-concurrency-challenges/#concurrent-scans-can-miss-entries\" aria-hidden=\"true\"></a></h2>\n<p>Earlier in the case of concurrent scans we saw how local reasoning of strict ordering is not sufficient, because of the extension. A global strict ordering has to be enforced to prevent unintended deadlocks from interaction of scans with other concurrent operations.</p>\n<p>Now let us look at a scenario where both the extensions can interact in a manner which violates strict ordering and introduces lock order inversion bugs in subtle ways.</p>\n<p>(<code>Thread 1</code>): A forward scan iterator has completed scanning entries in Node A, and is next going to process it's right sibling Node B.</p>\n<p>(<code>Thread 2</code>): A delete operation has taken the pessimistic path, because Node B is going to underflow after removing this entry. So it decides to merge with left sibling Node A. It already holds an exclusive latch on Node B and has to acquire an exclusive latch on Node B.</p>\n<p>If the forward scan operator releases its shared latch on Node A, before it acquires a shared latch on Node B, the following sequence of events can happen with the right timing of events.</p>\n<p>(<code>Thread 1</code>): Releases latch on Node A before acquiring a latch on Node B.</p>\n<p>(<code>Thread 2</code>): Release exclusive latch on Node B. Acquires exclusive latch on Node A. Acquires the exclusive latch back on Node B.</p>\n<p>(<code>Thread 1</code>): Is blocked by the exclusive latch held on Node B, by the delete operation in <code>Thread 2</code>.</p>\n<p>(<code>Thread 2</code>): Merges Node A and B together, and modifies the right sibling pointer to Node C.</p>\n<p>(<code>Thread 1</code>): Finally acquires a latch on Node C which is the new right sibling on Node A, not realizing it missed node B completely.</p>\n<p>If latches acquisition is not crafted carefully in the iterator code, the following situation creates a race condition which is very hard to even detect, or reproduce.</p>\n<p>The fix here is to correctly implement crab latching in the iterator code. A shared lock should not be released before <code>TrySharedLock()</code> is attempted on the sibling node. Doing so results in race conditions which are impossible to track down. This requires careful programming discipline.</p>\n",
      "date_published": "2025-08-21T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/",
      "url": "https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/",
      "title": "Cache-Friendly B+Tree Nodes With Dynamic Fanout",
      "content_html": "<p>For a high-performance B+Tree, the memory layout of each node must be a single contiguous block. This improves locality of reference, increasing the likelihood that the node's contents reside in the CPU cache.</p>\n<p>In C++, achieving this means forgoing the use of <code>std::vector</code>, as it introduces a layer of indirection through a separate memory allocation. The solution to this problem though inevitably increases the implementation complexity and is mired with hidden drawbacks. Nevertheless, this is still a necessary trade-off for unlocking high performance.</p>\n<pre class=\"language-text\"><code class=\"language-text\">  +----------------------+<br />  | Node Metadata Header |<br />  +----------------------+<br />  | node_type_           |<-- Inner Node or Leaf Node<br />  | max_size_            |<-- Maximum Capacity (aka Node Fanout)<br />  | node_latch_          |<-- RW-Lock Mutex<br />  | iter_end_            |--------------------+<br />  +----------------------+                    |<br />  | Node Data            |                    |<br />  +----------------------+                    |<br />  | entries_[0]          | <--+               |<br />  | entries_[1]          |    |               |<br />  | entries_[2]          |    + used space    |<br />  | ...                  |    |               |<br />  | entries_[k]          | <--+               |<br />  +----------------------+<-------------------+ iter_end_ points to<br />  |                      |    entries_[k+1], which is one-past-the-last<br />  | (unused space)       |    entry in the node.<br />  | ...                  |<br />  +----------------------+</code></pre>\n<figcaption>Fig 1. Memory Layout of a B+Tree Node as a single contiguous block in heap</figcaption>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#challenges\">Challenges</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#the-struct-hack\">The Struct Hack</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#b+tree-node-declaration\">B+Tree Node Declaration</a></li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#raw-memory-buffer\">Raw Memory Buffer</a></li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#the-price-of-fine-grained-control\">The Price Of Fine-Grained Control</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#manual-handling-of-deallocation\">Manual Handling Of Deallocation</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#adding-new-members-in-a-derived-class\">Adding New Members In A Derived Class</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#reinventing-the-wheel\">Reinventing The Wheel</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#hidden-data-type-assumptions\">Hidden Data Type Assumptions</a></li>\n      </ul>\n    </li>\n    <li><a href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#conclusion\">Conclusion</a></li>\n  </ol>\n</nav>\n<h2 id=\"challenges\" tabindex=\"-1\">Challenges <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#challenges\" aria-hidden=\"true\"></a></h2>\n<p>Using <code>std::vector</code> for a B+Tree node's entries is a non-starter. A <code>std::vector</code> object holds a pointer to its entries which are stored in a separate block of memory on the heap. This indirection fragments the memory layout, forcing us to fall back on C-style arrays for a contiguous layout when storing variable-length node entries.</p>\n<p>This leads to a dilemma. The size of the array must be known at compilation time, yet we need to allow users to configure the fanout (the array's size) at runtime. Furthermore, the implementation should allow inner nodes and leaf nodes to have different fanouts.</p>\n<p>This isn't just a B+Tree problem. It is a common challenge in systems programming whenever an object needs to contain a variable-length payload whose size is only known at runtime. How can you define a class that occupies a single block of memory when a part of the block has a dynamic size?</p>\n<p>The solution isn't obvious, but it's a well-known trick that systems programmers have used for decades, a technique so common it has eventually been standardized in C99.</p>\n<h2 id=\"the-struct-hack\" tabindex=\"-1\">The Struct Hack <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#the-struct-hack\" aria-hidden=\"true\"></a></h2>\n<p>The solution to this problem is a technique originating in C programming known as the struct hack. The variable-length member (array) is placed at the last position in the struct. To satisfy the compiler an array size of one is hard-coded, ensuring the array size is known at compilation time.</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">struct</span> <span class=\"token class-name\">Payload</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">/* Header Section */</span><br />  <span class=\"token comment\">// ...</span><br /><br />  <span class=\"token comment\">/* Data Section */</span><br /><br />  <span class=\"token comment\">// The variable-length member is in last position.</span><br />  <span class=\"token comment\">// The size `1` satisfies the compiler.</span><br />  <span class=\"token keyword\">char</span> elements<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>At runtime, when the required size <code>N</code> is known, you allocate a single block of memory for the struct and the <code>N</code> elements combined. The compiler treats this as an opaque block, and provides no safety guarantees. However, accessing the extra allocated space is safe because the variable-length member is the final field in the struct.</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token comment\">// The (N - 1) adjusts for the 1-element array in Payload struct</span><br />Payload <span class=\"token operator\">*</span>item <span class=\"token operator\">=</span> <span class=\"token function\">malloc</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span>Payload<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>N <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">char</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre>\n<p>This pattern was officially standardized in C99, where it is known as a <a href=\"https://en.wikipedia.org/wiki/Flexible_array_member\">flexible array member</a>.</p>\n<p>The C++11 standard formally incorporates the flexible array member, referring to it as an <strong>array of unknown bound</strong> when it is the last member of a struct.</p>\n<blockquote>\n<p><strong>Arrays of unknown bound</strong></p>\n<p>If <code>expr</code> is omitted in the declaration of an array, the type declared is &quot;array of unknown bound of T&quot;, which is a kind of incomplete type, ...</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">extern</span> <span class=\"token keyword\">int</span> x<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>      <span class=\"token comment\">// the type of x is \"array of unknown bound of int\"</span><br /><span class=\"token keyword\">int</span> a<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span> <span class=\"token comment\">// the type of a is \"array of 3 int\"</span></code></pre>\n</blockquote>\n<p>This means that in C++, the size can be omitted from the final array declaration (e.g. <code>entries_[]</code>), and the code will compile, enabling the same pattern.</p>\n<h2 id=\"b+tree-node-declaration\" tabindex=\"-1\">B+Tree Node Declaration <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#b+tree-node-declaration\" aria-hidden=\"true\"></a></h2>\n<p>Using the flexible array member syntax, we can now declare a B+Tree node with a memory layout which is a contiguous single block in the heap.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">template</span> <span class=\"token operator\">&lt;</span><span class=\"token keyword\">typename</span> <span class=\"token class-name\">KeyType</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">typename</span> <span class=\"token class-name\">ValueType</span><span class=\"token operator\">></span><br /><span class=\"token keyword\">class</span> <span class=\"token class-name\">BPlusTreeNode</span> <span class=\"token punctuation\">{</span><br /><span class=\"token keyword\">public</span><span class=\"token operator\">:</span><br />  <span class=\"token keyword\">using</span> KeyValuePair <span class=\"token operator\">=</span> std<span class=\"token double-colon punctuation\">::</span>pair<span class=\"token operator\">&lt;</span>KeyType<span class=\"token punctuation\">,</span> ValueType<span class=\"token operator\">></span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token keyword\">private</span><span class=\"token operator\">:</span><br />  <span class=\"token comment\">// Node Header Members ... (elided)</span><br /><br />  <span class=\"token comment\">// Points to the memory location beyond the last key-value</span><br />  <span class=\"token comment\">// entry in the `entries_` array.</span><br />  KeyValuePair<span class=\"token operator\">*</span> iter_end_<span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// Array containing key-value entries of unknown bound.</span><br />  KeyValuePair entries_<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre>\n<p>Using a <code>std::vector&lt;KeyValuePair&gt;</code> for the node's entries would result in an indirection. This immediately fragments the memory layout. Accessing an entry within a node is slower, and has higher latency because of the pointer indirection. Chasing the pointer increases the probability of a cache miss, which will force the CPU to stall while it waits for the cache line to be fetched from a different region in main memory.</p>\n<p>A cache miss will cost hundreds of CPU cycles compared to just a few cycles for a cache hit. This cumulative latency is unacceptable for any high-performance data structure.</p>\n<p>This technique avoids the pointer indirection and provides fine-grained control over memory layout. The node header and data are co-located in one continuous memory block. This layout is cache-friendly and will result in fewer cache misses.</p>\n<h2 id=\"raw-memory-buffer\" tabindex=\"-1\">Raw Memory Buffer <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#raw-memory-buffer\" aria-hidden=\"true\"></a></h2>\n<p>This is the key step. The construction of the object has to be separate from its memory allocation. We cannot therefore use the standard <code>new</code> syntax which will attempt to allocate storage, and then initialize the object in the same storage.</p>\n<p>Instead, we use the <a href=\"https://en.cppreference.com/w/cpp/language/new.html#Placement_new\">placement new</a> syntax which only constructs an object in a preallocated memory buffer provided by us. We know exactly how much space to allocate, which is information the standard <code>new</code> operator does not have in this scenario because of the flexible array member.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// A static helper to allocate storage for a B+Tree node.</span><br /><span class=\"token keyword\">static</span> BPlusTreeNode <span class=\"token operator\">*</span><span class=\"token function\">Get</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> p_fanout<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// calculate total buffer size</span><br />  size_t buf_size <span class=\"token operator\">=</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span>BPlusTreeNode<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> p_fanout <span class=\"token operator\">*</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span>KeyValuePair<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// allocate raw memory buffer</span><br />  <span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>buf <span class=\"token operator\">=</span> <span class=\"token double-colon punctuation\">::</span><span class=\"token keyword\">operator</span> <span class=\"token keyword\">new</span><span class=\"token punctuation\">(</span>buf_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// construct B+Tree node object in the preallocated buffer</span><br />  <span class=\"token keyword\">auto</span> node <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span><span class=\"token punctuation\">(</span>buf<span class=\"token punctuation\">)</span> <span class=\"token function\">BPlusTreeNode</span><span class=\"token punctuation\">(</span>p_fanout<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token keyword\">return</span> node<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>The result is a cache-friendly B+Tree node with a fanout that can be configured at runtime.</p>\n<h2 id=\"the-price-of-fine-grained-control\" tabindex=\"-1\">The Price Of Fine-Grained Control <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#the-price-of-fine-grained-control\" aria-hidden=\"true\"></a></h2>\n<p>To create an instance of a B+Tree node with a fanout of <code>256</code>, it is not possible to write simple idiomatic code like this: <code>new BPlusTreeNode(256)</code>.</p>\n<p>Instead we use the custom <code>BPlusTreeNode::Get</code> helper which knows how much raw memory to allocate for the object including the data section.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\">BPlusTreeNode <span class=\"token operator\">*</span>root <span class=\"token operator\">=</span> <span class=\"token class-name\">BPlusTreeNode</span><span class=\"token operator\">&lt;</span>KeyValuePair<span class=\"token operator\">></span><span class=\"token double-colon punctuation\">::</span><span class=\"token function\">Get</span><span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<h3 id=\"manual-handling-of-deallocation\" tabindex=\"-1\">Manual Handling Of Deallocation <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#manual-handling-of-deallocation\" aria-hidden=\"true\"></a></h3>\n<p>The destructor code is also not idiomatic anymore. When the lifetime of the B+Tree node ends, the deallocation code has to be carefully crafted to avoid resource or memory leaks.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">BPlusTreeNode</span> <span class=\"token punctuation\">{</span><br /><br />  <span class=\"token keyword\">void</span> <span class=\"token function\">FreeNode</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token comment\">// Call the destructor for each key-value entry.</span><br />    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>KeyValuePair<span class=\"token operator\">*</span> element <span class=\"token operator\">=</span> entries_<span class=\"token punctuation\">;</span> element <span class=\"token operator\">&lt;</span> iter_end_<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>element<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />      element<span class=\"token operator\">-></span><span class=\"token operator\">~</span><span class=\"token function\">KeyValuePair</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    <span class=\"token punctuation\">}</span><br /><br />    <span class=\"token comment\">// Call the node destructor</span><br />    <span class=\"token keyword\">this</span><span class=\"token operator\">-></span><span class=\"token operator\">~</span><span class=\"token function\">BPlusTreeNode</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />    <span class=\"token comment\">// Deallocate the raw memory</span><br />    <span class=\"token double-colon punctuation\">::</span><span class=\"token keyword\">operator</span> <span class=\"token keyword\">delete</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>This carefully ordered cleanup is necessary because we took manual control of memory. The process is the mirror opposite of our <code>Get</code> function. We constructed the object outside in: <em>raw memory buffer -&gt; node object -&gt; individual elements</em>. So we teardown in the opposite direction, from the inside out: <em>individual elements -&gt; node object -&gt; raw memory buffer</em>.</p>\n<h3 id=\"adding-new-members-in-a-derived-class\" tabindex=\"-1\">Adding New Members In A Derived Class <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#adding-new-members-in-a-derived-class\" aria-hidden=\"true\"></a></h3>\n<p>Adding a new member to a derived class will result in data corruption. It is not possible to add new fields to a specialized <code>InnerNode</code> or <code>LeafNode</code> class.</p>\n<pre class=\"language-text\"><code class=\"language-text\">+----------------------+<br />| Node Metadata Header |<br />+----------------------+<br />| ...                  |<br />+----------------------+<br />| Node Data            |<br />+----------------------+<-- offset where the data buffer starts<br />| entries_[0]          |<-- offset where the derived class members<br />| entries_[1]          |    will be written to, overwriting the<br />| ...                  |    entries<br />| entries_[N]          |<br />+----------------------+</code></pre>\n<figcaption>Fig 2. Adding new members in a derived class will overwrite the <code>entries_</code> array in memory.</figcaption>\n<p>The raw memory we manually allocated is opaque to the compiler and it cannot safely reason about where the newly added members to the derived class are physically located. The end result is it will overwrite the data buffer and cause data corruption.</p>\n<p>The workaround is to break encapsulation and add derived members to the base class so that the flexible array member is always in the last position. This is a significant drawback when we begin using flexible array members.</p>\n<pre class=\"language-text\"><code class=\"language-text\">+----------------------+<br />| Node Metadata Header |<br />+----------------------+<br />| ...                  |<br />| low_key_             |<-- `InnerNode`: left-most node pointer<br />| left_sibling_        |<-- `LeafNode` : link to left sibling<br />| right_sibling_       |<-- `LeafNode` : link to right sibling<br />+----------------------+<br />| Node Data            |<br />+----------------------+<-- Flexible array member guaranteed to<br />| entries_[0]          |    be in the last position<br />| entries_[1]          |<br />| ...                  |<br />| entries_[N]          |<br />+----------------------+</code></pre>\n<figcaption>Fig 3. Memory layout of base class with members necessary for the derived <code>InnerNode</code> and <code>LeafNode</code> implementations.</figcaption>\n<h3 id=\"reinventing-the-wheel\" tabindex=\"-1\">Reinventing The Wheel <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#reinventing-the-wheel\" aria-hidden=\"true\"></a></h3>\n<p>By using a raw C-style array, we effectively reinvent parts of <code>std::vector</code>, implementing our own utilities for insertion, deletion and iteration. This not only raises the complexity and maintenance burden but also means we are responsible for ensuring our custom implementation is as performant as the highly-optimized standard library version.</p>\n<p>The engineering cost to make this implementation production-grade is significant.</p>\n<h3 id=\"hidden-data-type-assumptions\" tabindex=\"-1\">Hidden Data Type Assumptions <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#hidden-data-type-assumptions\" aria-hidden=\"true\"></a></h3>\n<p>The <code>BPlusTreeNode</code>'s generic signature implies it will work for any <code>KeyType</code> or <code>ValueType</code>, but this is dangerously misleading. Using a non-trivial type like <code>std::string</code> will cause undefined behavior.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">template</span> <span class=\"token operator\">&lt;</span><span class=\"token keyword\">typename</span> <span class=\"token class-name\">KeyType</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">typename</span> <span class=\"token class-name\">ValueType</span><span class=\"token operator\">></span><br /><span class=\"token keyword\">class</span> <span class=\"token class-name\">BPlusTreeNode</span> <span class=\"token punctuation\">{</span><br /><span class=\"token keyword\">public</span><span class=\"token operator\">:</span><br />  <span class=\"token keyword\">using</span> KeyValuePair <span class=\"token operator\">=</span> std<span class=\"token double-colon punctuation\">::</span>pair<span class=\"token operator\">&lt;</span>KeyType<span class=\"token punctuation\">,</span> ValueType<span class=\"token operator\">></span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// ...</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>To understand why, let's look at how entries are inserted. To make space for a new element, existing entries must be shifted to the right. With our low-level memory layout, this is done using bitwise copy, as the following implementation shows.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">bool</span> <span class=\"token function\">Insert</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">const</span> KeyValuePair <span class=\"token operator\">&amp;</span>element<span class=\"token punctuation\">,</span> KeyValuePair <span class=\"token operator\">*</span>pos<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// The node is currently full so we cannot insert this element.</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token function\">GetCurrentSize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> <span class=\"token function\">GetMaxSize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span> <span class=\"token keyword\">return</span> <span class=\"token boolean\">false</span><span class=\"token punctuation\">;</span> <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token comment\">// Shift elements from `pos` to the right by one to make</span><br />  <span class=\"token comment\">// place for inserting new `element`.</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">distance</span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">,</span> <span class=\"token function\">End</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />      <span class=\"token comment\">// Bitwise copying</span><br />      std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">memmove</span><span class=\"token punctuation\">(</span><br />        <span class=\"token comment\">// Destination Address</span><br />        <span class=\"token generic-function\"><span class=\"token function\">reinterpret_cast</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span><span class=\"token keyword\">void</span> <span class=\"token operator\">*</span><span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span>std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">next</span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><br />        <span class=\"token comment\">// Source Address</span><br />        <span class=\"token generic-function\"><span class=\"token function\">reinterpret_cast</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span><span class=\"token keyword\">void</span> <span class=\"token operator\">*</span><span class=\"token operator\">></span></span></span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><br />        <span class=\"token comment\">// Byte Count</span><br />        std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">distance</span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">,</span> <span class=\"token function\">End</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span>KeyValuePair<span class=\"token punctuation\">)</span><br />      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><br />  <span class=\"token comment\">// Insert element at `pos`.</span><br />  <span class=\"token keyword\">new</span><span class=\"token punctuation\">(</span>pos<span class=\"token punctuation\">)</span> KeyValuePair<span class=\"token punctuation\">{</span>element<span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token comment\">// Bookkeeping</span><br />  std<span class=\"token double-colon punctuation\">::</span><span class=\"token function\">advance</span><span class=\"token punctuation\">(</span>iter_end_<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />  <span class=\"token keyword\">return</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>The use of <code>std::memmove</code> introduces a hidden constraint: <code>KeyValuePair</code> must be trivially copyable. This means the implementation only works correctly for simple, C-style data structures despite its generic-looking interface.</p>\n<p>Using <code>std::memmove</code> on a <code>std::string</code> object creates a shallow copy. We now have two <code>std::string</code> objects whose internal pointers both point to the same character buffer on the heap. When the destructor of the original string is eventually called, it deallocates that buffer. The copied string is now left with a dangling pointer to freed memory, leading to use-after-free errors or a double-free crash when its own destructor runs.</p>\n<h2 id=\"conclusion\" tabindex=\"-1\">Conclusion <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-18-bplustree-struct-hack/#conclusion\" aria-hidden=\"true\"></a></h2>\n<p>We started with a clear goal: a cache-friendly, contiguous B+Tree node with a dynamic, runtime-configurable fanout. The flexible array member proved to be the perfect tool, giving us direct control over memory layout while supporting variable-length entries.</p>\n<p>However, this fine-grained control comes at a steep cost. We must abandon idiomatic C++, manually manage memory, give up inheritance, and enforce hidden data type constraints. This is the fundamental trade-off: we sacrifice simplicity and safety for raw performance.</p>\n",
      "date_published": "2025-08-18T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/",
      "url": "https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/",
      "title": "A B+Tree Node Underflows: Merge or Borrow?",
      "content_html": "<p>A B+Tree's stable algorithmic performance relies on a single invariant: the path from its root to any leaf is always the same length. However, a delete operation can cause a node to underflow (falling below its minimum occupancy), triggering a rebalancing procedure to maintain this critical invariant.</p>\n<p>Modern B+Trees use fast, optimistic latching protocols which operate under the assumption that rebalancing happens rarely. The mere possibility of an underflow can force the rebalancing into the slow, pessimistic path, acquiring exclusive locks that stall other operations.</p>\n<p>How the implementation decides to fix the underflow is therefore a critical design choice: merge with a sibling node to reclaim free space, or borrow keys from a sibling node to minimize the impact on write latency. Simply put, it's a classic trade-off between space and time. In this post, we will also explore how major OLTP systems expertly implement sophisticated strategies which go beyond this classic trade-off.</p>\n<nav class=\"toc\" aria-labelledby=\"toc-heading\">\n  <h2 id=\"toc-heading\">Table of Contents</h2>\n  <ol>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#fixing-node-underflow\">Fixing Node Underflow</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#the-merge-first-approach\">The Merge-First Approach</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#the-borrow-first-approach\">The Borrow-First Approach</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#in-oltp-systems\">In OLTP systems</a>\n      <ul>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#background-merge-in-mysql-innodb\">Background Merge In MySQL InnoDB</a></li>\n        <li><a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#do-nothing-strategy-in-postgresql\">Do Nothing Strategy In PostgreSQL</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#key-takeaways\">Key Takeaways</a>\n    </li>\n  </ol>\n</nav>\n<h2 id=\"fixing-node-underflow\" tabindex=\"-1\">Fixing Node Underflow <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#fixing-node-underflow\" aria-hidden=\"true\"></a></h2>\n<p>A node underflow happens when the used space (or occupancy) within a node falls below a minimum threshold. This is a possibility after removing an entry from the node. A viable solution is to do nothing. By doing nothing, a tree balancing procedure is never activated. The major downside though is index bloat. A failure to garbage collect the unused memory results in the nodes becoming progressively sparse as entries continue to be added and removed.</p>\n<blockquote>\n<p>In contrast, a node overflow will always trigger a tree rebalancing because it creates a new node whose reference needs to be inserted in the parent node. Here, doing nothing is not even an available option.</p>\n</blockquote>\n<p>The two basic strategies for fixing node underflow both involve merging and borrowing. They differ by which operation is executed first: a merge or a borrow. The merge-first approach prioritizes immediate garbage collection of unused space. It trades-off write speed for more efficient utilization of space. In contrast, the borrow-first approach prioritizes write throughput through redistribution of keys within existing nodes avoiding a merge whenever possible, trading-off long term space-efficiency for short-term write speed.</p>\n<h3 id=\"the-merge-first-approach\" tabindex=\"-1\">The Merge-First Approach <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#the-merge-first-approach\" aria-hidden=\"true\"></a></h3>\n<p>For a merge to work, the combined entries in the underflowing node and the target sibling node must fit within a single node. After merging two nodes into one, the memory belonging to the underflowing node can be garbage collected.</p>\n<blockquote>\n<p>An efficient implementation will avoid allocation for a new node, and reuse the memory of the sibling node.</p>\n</blockquote>\n<p>The problem with the merge-first approach is that in the worst-case it can recursively cascade all the way back to the root of the B+Tree. In practice though this should happen rarely. The reason for this behavior is that merge eliminates an existing node, and its reference has to be removed from the parent inner node. Removing an entry from a node, has the potential however small to again cause an underflow.</p>\n<p>What if the combined entries will not fit into a single node?</p>\n<p>Then we fallback to borrowing entries from a sibling to fix the underflow. This will not cause a cascading rebalance, as there is no change in nodes, only a redistribution of keys.</p>\n<blockquote>\n<p><em>Disclaimer</em>: The following is a simplified view of how the B+Tree algorithm and concurrency protocols interleave with each other. An implementation in code is more complex.</p>\n</blockquote>\n<p>In optimistic (crab latching) concurrency, if removing an entry will cause a node to underflow it is categorized as an &quot;unsafe&quot; node. The optimistic approach is abandoned and we restart traversal back from the root. It involves acquiring a chain of exclusive latches along the search path to safely complete the operation. This significantly slower path blocks other readers and writers from accessing the latched nodes until the rebalancing operation completes.</p>\n<p>The merge-first approach compresses nodes and therefore more keys are stored within the minimum amount of nodes. This occupies less space on disk and therefore requires less read I/O to be performed. For range scans then a minimum number of nodes needs to be read from disk for a given key range. The higher node density also results in better cache locality and this further improves compute performance.</p>\n<h3 id=\"the-borrow-first-approach\" tabindex=\"-1\">The Borrow-First Approach <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#the-borrow-first-approach\" aria-hidden=\"true\"></a></h3>\n<p>For borrowing to work, removing the entries from the sibling node must not result in an underflow. Since there is no change in nodes the tree rebalancing is guaranteed not to cascade and therefore completes faster. If borrowing will cause a node underflow, we fallback to merging nodes.</p>\n<p>The concurrency aspects are similar to the merge-first approach. In comparison, it is reasonable to expect that the exclusive latches held on the nodes in the search path segment, will be for a relatively shorter duration. It is still orders of magnitude slower than the optimistic approach.</p>\n<p>This approach prioritizes faster writes and predictable latency by avoiding merging nodes unless strictly necessary. The downside is that node density is lower. The range scans now require more node I/O because the same key range is now spread over a wide span of leaf nodes.</p>\n<h2 id=\"in-oltp-systems\" tabindex=\"-1\">In OLTP Systems <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#in-oltp-systems\" aria-hidden=\"true\"></a></h2>\n<p>The discussion so far is confined to a stand-alone thread-safe B+Tree implementation. We are looking at behavior and performance at the data structure level. In major OLTP database management systems, the B+Tree index is also tightly integrated with the transaction manager, write ahead log and recovery manager. So the scope of the design decisions are not limited to the data structure level, rather how it impacts the overall systems performance.</p>\n<h3 id=\"background-merge-in-mysql-innodb\" tabindex=\"-1\">Background Merge In MySQL InnoDB <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#background-merge-in-mysql-innodb\" aria-hidden=\"true\"></a></h3>\n<p>In MYSQL's InnoDB, node underflow is handling through merging. Rather than performing an online tree balancing, it is offloaded as a separate asynchronous process in the background. The minimum occupancy of a node is configurable through the <a href=\"https://dev.mysql.com/doc/refman/8.4/en/index-page-merge-threshold.html\">MERGE_THRESHOLD</a> parameter.</p>\n<blockquote>\n<p>If the “page-full” percentage for an index page falls below the MERGE_THRESHOLD value when a row is deleted or when a row is shortened by an UPDATE operation, InnoDB attempts to merge the index page with a neighboring index page. The default MERGE_THRESHOLD value is 50, which is the previously hard-coded value. The minimum MERGE_THRESHOLD value is 1 and the maximum value is 50.</p>\n</blockquote>\n<p>You can monitor the background merge by querying the following InnoDB metrics:</p>\n<pre class=\"language-text\"><code class=\"language-text\">mysql> SELECT NAME, COMMENT FROM INFORMATION_SCHEMA.INNODB_METRICS<br />       WHERE NAME like '%index_page_merge%';<br />+-----------------------------+----------------------------------------+<br />| NAME                        | COMMENT                                |<br />+-----------------------------+----------------------------------------+<br />| index_page_merge_attempts   | Number of index page merge attempts    |<br />| index_page_merge_successful | Number of successful index page merges |<br />+-----------------------------+----------------------------------------+</code></pre>\n<p>Merging entries from a sibling reduces the unused space remaining within a node. If new insertions land on this node, it can immediately overflow. The overflow forces a node split which requires a new node allocation and redistribution of entries. Now both nodes are half-full, and a delete from either node can tip another underflow creating a cycle of merging and splitting. This thrashing merge-split behavior is terrible news for index performance.</p>\n<blockquote>\n<p>If both pages are close to 50% full, a page split can occur soon after the pages are merged. If this merge-split behavior occurs frequently, it can have an adverse affect on performance. To avoid frequent merge-splits, you can lower the MERGE_THRESHOLD value so that InnoDB attempts page merges at a lower “page-full” percentage. Merging pages at a lower page-full percentage leaves more room in index pages and helps reduce merge-split behavior.</p>\n</blockquote>\n<p>What happens if you over-tune the <code>MERGE_THRESHOLD</code> knob for your workload?</p>\n<blockquote>\n<p>A MERGE_THRESHOLD setting that is too small could result in large data files due to an excessive amount of empty page space.</p>\n</blockquote>\n<p>This results in index bloat, which directly harms read efficiency. The same key space instead of being densely packed, is now spread over a larger number of sparse nodes, requiring more I/O for range scans, and takes up more buffer pool capacity to keep the index in memory.</p>\n<h3 id=\"do-nothing-strategy-in-postgresql\" tabindex=\"-1\">Do Nothing Strategy In PostgreSQL <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#do-nothing-strategy-in-postgresql\" aria-hidden=\"true\"></a></h3>\n<p>In PostgreSQL deleting an index entry happens in two separate phases: a logical deletion followed by a physical deletion. The logical deletion creates a tombstone marker, and is lightweight. The physical deletion is more expensive, and happens in the background.</p>\n<p>When a node underflows, PostgreSQL does nothing to rectify the situation. The only time it attempts to reclaim space is when all the entries are removed from a leaf node and it becomes empty.</p>\n<blockquote>\n<p>We consider deleting an entire page from the btree only when it's become\ncompletely empty of items. (Merging partly-full pages would allow better\nspace reuse, but it seems impractical to move existing data items left or\nright to make this happen --- a scan moving in the opposite direction\nmight miss the items if so.)</p>\n</blockquote>\n<p>There is an exception to this rule. PostgreSQL will never delete the right-most child of a parent on any given level, even if it becomes empty. Removing the right-most node will have to be followed by transferring the key space used for navigation to the next or previous parent. Since we do not hold latches on those nodes yet, they will have to be freshly acquired. All of this is avoided by sticking to this rule, and it simplifies the implementation.</p>\n<blockquote>\n<p>To preserve consistency on the parent level, we cannot merge the key space\nof a page into its right sibling unless the right sibling is a child of\nthe same parent --- otherwise, the parent's key space assignment changes\ntoo, meaning we'd have to make bounding-key updates in its parent, and\nperhaps all the way up the tree. Since we can't possibly do that\natomically, we forbid this case.</p>\n</blockquote>\n<p>The deletion of a node is also separated into logical and physical phases. A logical deletion happens first which marks a tombstone. A physical deletion, which reclaims the space for reuse is only performed when this node is not visible to any active transactions.</p>\n<blockquote>\n<p>Recycling a page is decoupled from page deletion. A deleted page can only\nbe put in the FSM to be recycled once there is no possible scan or search\nthat has a reference to it; until then, it must stay in place with its\nsibling links undisturbed, as a tombstone that allows concurrent searches\nto detect and then recover from concurrent deletions (which are rather\nlike concurrent page splits to searchers).</p>\n</blockquote>\n<p><a href=\"https://github.com/postgres/postgres/blob/master/src/backend/access/nbtree/README\">Source</a>: <code>src/backend/access/nbtree/README</code></p>\n<h2 id=\"key-takeaways\" tabindex=\"-1\">Key Takeaways <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-08-16-bplustree-compare-borrow-merge/#key-takeaways\" aria-hidden=\"true\"></a></h2>\n<p>Fixing a node underflow is presented as a binary choice between a merge-first or borrow-first approach for tree rebalancing at the data structure level. For a concurrent implementation, when a node underflow happens the pessimistic (slower) path is preferred for correctness and protecting the integrity of the B+Tree. For non-OLTP use cases, neither merge-first nor borrow-first is inherently better than the other.</p>\n<p>In MySQL the rebalancing is offline, and happens in the background. While in PostgreSQL, rebalancing is not undertaken for node underflows. The priority is higher concurrency by avoiding rebalancing. The trade-off in both systems is accumulating index bloat. The burden of managing index bloat now falls upon the operator.</p>\n<p>Two brilliant lessons we can learn from these OLTP systems to improve concurrency are: offline (asynchronous) rebalancing, and separating the deletion of entries, and nodes into logical and physical phases.</p>\n",
      "date_published": "2025-08-16T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/shredding/",
      "url": "https://jacobsherin.com/posts/shredding/",
      "title": "Working title",
      "content_html": "\n<link href=\"https://jacobsherin.com/static/dremel-post.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n<div class=\"mt0 max-w-[800px]\">\n    <p>\n        In 2010, Google published the <a href=\"https://google.com/\">Dremel paper</a> showing how to shred nested\n        data into columnar format, delivering 100x speedups on\n        web-scale analytics. Until then, there was no widely adopted way to\n        efficiently query nested data on large datasets.\n    </p>\n\n    <p>\n        <em>Nested data in the artist's imagination:</em>\n    </p>\n\n    <pre>\n<pre class=\"language-typescript\"><code class=\"language-typescript\"><span class=\"token keyword\">type</span> <span class=\"token class-name\">Contact</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><br />  name<span class=\"token operator\">?</span><span class=\"token operator\">:</span> <span class=\"token builtin\">string</span><span class=\"token punctuation\">;</span><br />  phones<span class=\"token operator\">:</span> <span class=\"token builtin\">Array</span><span class=\"token operator\">&lt;</span><span class=\"token punctuation\">{</span><span class=\"token builtin\">number</span><span class=\"token operator\">:</span> <span class=\"token builtin\">string</span><span class=\"token punctuation\">;</span> phone_type<span class=\"token operator\">?</span><span class=\"token operator\">:</span> <span class=\"token builtin\">string</span><span class=\"token punctuation\">}</span><span class=\"token operator\">></span><br /><span class=\"token punctuation\">}</span></code></pre>\n</pre>\n\n    <p>\n        Even today, you'll find yourselves waiting for hours if you try running\n        aggregations like SUM and GROUP BY on multi-billion row datasets using\n        Postgres or MySQL. It is a design decision; OLTP databases are meant to run thousands of concurrent transactions\n        per second, and be able to instantly read and create entire rows. But that precludes analytical queries that\n        rely on whole-table column data rather than individual rows.\n    </p>\n\n    <p>\n        For such queries, we turn to OLAP databases like DuckDB, DataFusion, and ClickHouse. They\n        store data in a columnar manner. This exploits the physical reality of how bytes are\n        arranged in the disk, so that we can grab large chunks of useful data in one go, without having to read any\n        irrelevant data.\n    </p>\n\n    <p>The challenge with nested data however is that there is no direct representation for it in a columnar format.\n        This is what the Dremel paper solved for. Before we look at Dremel in detail, let's look at a brief\n        visualization of row and columnar storage. </p>\n\n    <h3>\n        Code for the curious\n    </h3>\n\n    <p>If you are familiar with the Dremel landscape, then you can jump directly into a from-scratch educational\n        implementation of the Dremel shredding algorithm here:<a href=\"https://github.com/jcsherin/denester\">github.com/jcsherin/denester</a>.\n        The core is only around 300 lines - <a href=\"https://github.com/jcsherin/denester/blob/8d1ef7ff62627591e8952ef0b2efbcbd386de9ba/src/parser.rs#L733-L1033\">parser.rs</a>.\n    </p>\n\n    <p>\n        There is also\n        <a href=\"https://github.com/jcsherin/datablok/tree/main/crates/parquet-parallel-nested\">parquet-parallel-nested</a> which I\n        wrote to explore the upper limits of shredding performance. It uses the Rust Arrow project and exploits\n        parallelism to generate, shred, and write 10 million nested documents in approximately 450ms (on a 16-core AMD\n        Ryzen 7 Pro).\n    </p>\n\n\n    <h2>\n        Physical layout of data in row and column storages\n    </h2>\n    <p>\n        Consider the following SQL query:\n    </p>\n    <div class=\"mb-8\">\n        <pre>select SUM(salary) from employees</pre>\n    </div>\n\n    <p>\n        To execute this query, we need access to all the values of the `salary` column, and nothing else.\n        This kind of query that operates on the entirety of specific columns is the standard for most business\n        reporting, analytics, and dashboard queries.\n    </p>\n\n    <p>\n        Let us now look at both row and column oriented storages, and how they lay out the bytes physically on the disk.\n        This arrangement decides which workload it supports best.\n    </p>\n\n    <h3>\n        Row-oriented storage\n    </h3>\n    <p>\n        Here all rows are stored one after the other.\n    </p>\n\n    <div class=\"visualization-content\">\n        <div class=\"data-layout\" id=\"row-oriented-layout\">\n            <div class=\"disk-page read-page\">\n                <div class=\"page-caption\">Page 1</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte\">1   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Alice          </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">90000   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eng       </span></div>\n                <div class=\"data-block\"><span class=\"data-byte\">2   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Bob            </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">65000   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Mktg      </span></div>\n                <div class=\"data-block\"><span class=\"data-byte\">3   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Charlie        </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">110000  </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eng       </span></div>\n            </div>\n            <div class=\"disk-page read-page\">\n                <div class=\"page-caption\">Page 2</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte\">4   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">David          </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">80000   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Sales     </span></div>\n                <div class=\"data-block\"><span class=\"data-byte\">5   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eve            </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">70000   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Mktg      </span></div>\n                <div class=\"data-block\"><span class=\"data-byte\">6   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Frank          </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">125000  </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eng       </span></div>\n            </div>\n            <div class=\"disk-page read-page\">\n                <div class=\"page-caption\">Page 3</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte\">7   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Grace          </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">85000   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Sales     </span></div>\n                <div class=\"data-block\"><span class=\"data-byte\">8   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Heidi          </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">60000   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">HR        </span></div>\n                <div class=\"data-block\"><span class=\"data-byte\">9   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Ivan           </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">95000   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eng       </span></div>\n            </div>\n            <div class=\"disk-page read-page\">\n                <div class=\"page-caption\">Page 4</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte\">10  </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Judy           </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">72000   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Mktg      </span></div>\n            </div>\n        </div>\n    </div>\n\n\n    <p>\n        You might have noticed that we've split up the rows into different pages. This is because at a hardware level,\n        disk reads happen only in large chunks, typically 4kb or more, of \"pages\". The byte-wise read APIs are an\n        illusion cast by the operating system. And so to read data from even one row, the database ends up\n        loading the entire page containing that row's data, which will also include other irrelevant data from that\n        page.\n    </p>\n\n    <p>\n        In the above visualization, we have marked every page that is actually read, with an orange border <span class=\"legend-read\" style=\"border-style: solid\"></span>.\n        And useful data - in this case the `salary` value - which we need to execute our query, is marked with a\n        <span class=\"legend-useful\"></span> green background.\n    </p>\n\n    <p>\n        Here <em>all pages</em> have been read, even though we need only a few bytes of the `salary` column from each\n        row in those pages. It is simply a consequence of how the data is laid out in the disk. This layout is optimized\n        for\n        transactional workloads: adding and updating entire rows. But it is deeply inefficient for analytical queries\n        that need to read specific columns across the table.\n    </p>\n\n\n    <h3>\n        Column-oriented storage\n    </h3>\n\n    <p>\n        Column-oriented storage turns the page-based reading of disks into a major advantage.\n\n        Since all values for a single attribute (like `salary`) are stored together, it can read just those pages for\n        the columns it needs, ignoring all the other data in the table.\n    </p>\n\n    <p>\n    In this visualization we can see that only a\n    single page is read (orange border <span class=\"legend-read\" style=\"border-style: solid\"></span>), which contains only useful data (green\n    background\n    <span class=\"legend-useful\"></span>), demonstrating the immense I/O savings.\n    </p>\n\n\n    <div class=\"visualization-content\">\n        <div class=\"data-layout\" id=\"column-oriented-layout\">\n            <div class=\"disk-page\">\n                <div class=\"page-caption\">Page 1</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte\">1   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">2   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">3   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">4   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">5   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">6   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">7   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">8   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">9   </span><span class=\"byte-separator\"></span><span class=\"data-byte\">10  </span></div>\n            </div>\n            <div class=\"disk-page\">\n                <div class=\"page-caption\">Page 2</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte\">Alice          </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Bob            </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Charlie        </span><span class=\"byte-separator\"></span><span class=\"data-byte\">David          </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eve            </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Frank          </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Grace          </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Heidi          </span></div>\n            </div>\n            <div class=\"disk-page\">\n                <div class=\"page-caption\">Page 3</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte\">Ivan           </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Judy           </span></div>\n            </div>\n            <div class=\"disk-page read-page\">\n                <div class=\"page-caption\">Page 4</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte useful\" data-useful=\"true\">90000   </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">65000   </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">110000  </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">80000   </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">70000   </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">125000  </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">85000   </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">60000   </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">95000   </span><span class=\"byte-separator\"></span><span class=\"data-byte useful\" data-useful=\"true\">72000   </span></div>\n            </div>\n            <div class=\"disk-page\">\n                <div class=\"page-caption\">Page 5</div>\n                <div class=\"read-indicator\">\n                    <div class=\"read-indicator-dot\"></div>\n                    <div class=\"read-indicator-text\">Page read</div>\n                </div>\n                <div class=\"data-block\"><span class=\"data-byte\">Eng       </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Mktg      </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eng       </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Sales     </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Mktg      </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eng       </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Sales     </span><span class=\"byte-separator\"></span><span class=\"data-byte\">HR        </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Eng       </span><span class=\"byte-separator\"></span><span class=\"data-byte\">Mktg      </span></div>\n            </div>\n        </div>\n    </div>\n\n    <h3>\n        Motivation for Dremel\n    </h3>\n\n    <p>As we can see from the above visualization, there is a fundamentally physical reason why columnar storage is essential for efficient analytical queries. And early 2000s Google was confronting web-scale data\n    that needed it. But their first solution was MapReduce, which was more of a distributed computing framework and didn't exploit columnar storage. There was one good reason for that: we did not have any widely adopted way to store nested data in columnar format.\n        Due to widespread usage of ProtoBuf inside Google, most of their data was non-relational nested values, and so no existing OLAP database were fit to purpose. This is what led to the creation of Dremel.\n    </p>\n\n    <p> But why wouldn't a straight-forward columnar mapping work?\n        One simple approach might be to flatten the nested data by treating every unique path in the tree as a column.\n        For example, let's consider this nested data:\n    </p>\n\n    <pre>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">[</span><br />  <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"first\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"John\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"last\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Doe\"</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />  <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"first\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Jane\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"last\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Smith\"</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">]</span></code></pre>\n</pre>\n    <p>We can store it in a columnar fashion in directly like this:</p>\n    <table style=\"white-space: pre;\" class=\"font-mono mb-6\">\n    <thead>\n        <tr>\n            <th>name.first</th>\n            <th>name.last</th>\n        </tr>\n        </thead>\n        <tbody>\n        <tr>\n            <td>John</td>\n            <td>Doe</td>\n\n        </tr>\n        <tr>\n            <td>Jane</td>\n            <td>Smith</td>\n        </tr>\n        </tbody>\n    </table>\n\n    <p>But this stops working the moment we have either repeating or optional fields. We will be unable to efficiently and non-ambiguously represent them with this mapping.</p>\n    <p>Consider this:</p>\n\n    <pre>\n        <pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"profile\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"age\"</span><span class=\"token operator\">:</span> <span class=\"token number\">30</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// Record 2</span><br /><span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"profile\"</span><span class=\"token operator\">:</span> <span class=\"token null keyword\">null</span>  <span class=\"token comment\">// Missing profile entirely!</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// Record 3</span><br /><span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"profile\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Charlie\"</span><br />    <span class=\"token comment\">// Missing age!</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">}</span></code></pre>\n    </pre>\n\n    <p>Here is its naive columnar representation:</p>\n\n    <table style=\"white-space: pre;\" class=\"font-mono mb-6\">\n        <tbody>\n        <tr>\n            <td>profile.name</td>\n            <td>[\"Alice\", null, \"Charlie\"]</td>\n        </tr>\n        <tr>\n            <td>profile.age</td>\n            <td>[30, null, null]</td>\n        </tr>\n        </tbody>\n    </table>\n\n    <p>\n        So we dealt with empty values by putting `null` where the values are missing.\n        That however is a lossy transformation that destroys information.\n    </p>\n\n    <p>\n        How do we distinguish between the leaf value missing vs the\n        entire node missing? For example, in the second record, It is not that\n        profile.name is null or profile.age is null, but there is no value at all\n        for the `profile` field. That is impossible to know from this representation.\n    </p>\n\n    <p>\n        And for the third record, the `profile` node exists, but the leaf value\n        `age` is missing. However, that information cannot be inferred by\n        looking at just the values of the age column. Since its value is `null`,\n        we could infer that either the `age` leaf value is missing, or its\n        entire parent node is missing. Note that in OLAP we only work with\n        individual columns and we should be able to query based on structural\n        information with just that. We cannot reconstruct row information - that\n        would defeat the very purpose of efficiency with columnar storage.\n    </p>\n\n    <p>\n        It doesn't stop there. The introduction of repeating elements (arrays) makes the ambiguity worse:\n    </p>\n\n    <pre>\n        <pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"doc_id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">10</span><span class=\"token punctuation\">,</span><br />  <span class=\"token property\">\"links\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"forward\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"http://A\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"http://B\"</span><span class=\"token punctuation\">]</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// Record 2</span><br /><span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"doc_id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">20</span><span class=\"token punctuation\">,</span><br />  <span class=\"token property\">\"links\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"forward\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"http://C\"</span><span class=\"token punctuation\">]</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">}</span></code></pre>\n    </pre>\n\n    <p>\nLet's look at only the `forward` column.\n    </p>\n\n    <table style=\"white-space: pre;\" class=\"font-mono mb-6\">\n        <tbody>\n        <tr>\n            <td>links.forward</td>\n            <td>[\"http://A\", \"http://B\", \"http://C\"]</td>\n        </tr>\n        </tbody>\n    </table>\n\n\n    <p>\n        But now we've lost the record boundaries. How do we know that \"http://A\" and \"http://B\" belong to record 1, and\n        \"http://C\" belongs to record 2?\n    </p>\n\n    <p>\n        When nested values have both optional and repeated elements, then this\n        naive mapping introduces so much ambiguity that it is impossible to\n        reconstruct the original record. While OLAP doesn't require row wise\n        iteration, it is important that the original value can be reconstructed\n        to ensure the correctness of the mapping.\n    </p>\n\n    <p>\n        The Dremel paper was Google's solution to this exact problem - it lets\n        us accurately and efficiently represent nested data with repeated and\n        optional values in a columnar structure. They were dealing with billions\n        of rows of hierarchical web data that broke traditional columnar\n        assumptions. The introduction of Dremel and OLAP produced dramatic\n        results: a MapReduce job processing 85 billion records that previously\n        took over an hour was now completed in under 10 seconds.\n    </p>\n\n    <p>\n        Although it hasn't captured popular imagination like its peer the MapReduce, it has had a profound impact on all\n        modern analytical SQL databases. It is what enables today's OLAP databases to query billions of rows of nested data with\n        almost the same speed as flat relational data. It has stood the test of time; literally - in 2020 the Dremel paper was given the\n        <a href=\"https://www.vldb.org/awards_10year.html%22\">VLDB Test of Time award</a>.\n    </p>\n</div>\n",
      "date_published": "2025-08-15T00:00:00Z"
    },{
      "id": "https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/",
      "url": "https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/",
      "title": "The Layman&#39;s Guide to Nested Record Shredding",
      "content_html": "<h3 id=\"intro-1\" tabindex=\"-1\">Intro 1 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#intro-1\" aria-hidden=\"true\"></a></h3>\n<p>In modern analytical query engines can compute complex aggregates by scanning billions of nested data structures in an instant.</p>\n<p>This is the performance available on a single node (your laptop).</p>\n<p>This works for SQL queries which contains many aggregates, groups and sort orders.</p>\n<p>The performance gap between querying nested data and flat relational data is often negligible.</p>\n<h3 id=\"intro-2\" tabindex=\"-1\">Intro 2 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#intro-2\" aria-hidden=\"true\"></a></h3>\n<p>The magic lies in a process known as &quot;shredding&quot;.</p>\n<p>At its core shredding is a process for breaking up nested data structures into a flat, columnar format.</p>\n<ul>\n<li>[x] <s>Insert diagram of a nested schema showing shredded column values collected at the leaf nodes. The user should be able to visualize that a path in the schema maps to a shredded column.</s></li>\n</ul>\n<figure>\n  <img src=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/img/figure-1.svg\" alt=\"Visualization of shredded column values of\na nested Contact schema\" />\n  <figcaption>\n  \tFigure 1: A high-level view of <code>Contact</code> nested data instances shredded into columnar format. Each shredded column corresponds to a unique path in the <code>Contact</code> schema: <code>name</code>, <code>phones.number</code> and <code>phones.phone_type</code>.\n  </figcaption>\n</figure>\n<p>The development of &quot;shredding&quot; was driven by the need for interactive analytics on very large (&gt; 1 trillion rows) datasets containing nested data structures in the Dremel query engine at Google in the late 2000s.</p>\n<p>The massive map-reduce jobs used to take multiple hours to complete even after running compute over thousands of servers in parallel.</p>\n<p>The same computation now finished in under 10s when the nested data structure was encoded using the Dremel shredding technique.</p>\n<p>It is an understatement to state that the performance impact was significant.</p>\n<h3 id=\"intro-3\" tabindex=\"-1\">Intro 3 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#intro-3\" aria-hidden=\"true\"></a></h3>\n<p>The novelty of Dremel record shredding lay in directly representing nested data structures in a flat, columnar format.</p>\n<p>This is achieved by introducing two key concepts: <strong>definition levels</strong> and <strong>repetition levels</strong>.</p>\n<p>This is an ingenious representation which manages to encode the original heirarchical structure of the nested data as two integer values.</p>\n<ul>\n<li>[ ] <s>Show a shredded column, def, rep values and the values they map to. I am thinking it can show the encoding at the top and the values in a sequence below. An example which will fit this type of illustration well is a nested list which contains a list of numbers. They encoding representation is also terse and we can show the mapping from encoding to each inner list in the diagram.</s></li>\n</ul>\n<figure>\n\t<img src=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/img/figure-2.svg\" alt=\"Side by side view of nested data and its\nfinal shredded representation with complete definition and repetition level values\" />\n\t<figcaption>Figure 2: Shows three nested list of integers and its final representation after shredding is completed. The structure of the original nested records are encoded in the definition and repetition levels corresponding to the list item in the value column.</figcaption>\n</figure>\n<h3 id=\"intro-4\" tabindex=\"-1\">Intro 4 <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#intro-4\" aria-hidden=\"true\"></a></h3>\n<p>The Dremel encoding technique influenced the design of the next generation of file formats and proprietary storage engines of analytical systems which were being built in the early 2010s.</p>\n<p>Apache Parquet is the most well known and open source implementation to directly adopt the principles of definition levels and repetition levels to represent nested data structure in its file format.</p>\n<p>In Apache Arrow in-memory columnar format the validity bitmap and offset buffers are conceputally similar to the definition and repetition levels in Dremel encoding.</p>\n<p>The widespread adoption of the principles from the Dremel encoding for nested data structures proves its effectiveness for analytical workloads.</p>\n<h3 id=\"intro-exit\" tabindex=\"-1\">Intro Exit <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#intro-exit\" aria-hidden=\"true\"></a></h3>\n<p>Before we dive into shredding let us first see the problems which come up when we try to store and retrieve nested data without shredding.</p>\n<h3 id=\"the-opaque-binary-blob-strawman\" tabindex=\"-1\">The Opaque Binary Blob Strawman <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#the-opaque-binary-blob-strawman\" aria-hidden=\"true\"></a></h3>\n<p>The direct method is to serialize a nested data structure and then store it as a sequence of bytes.</p>\n<p>But now, every time you want to read it back you first load the entire bytes back into memory and deserialize it.</p>\n<p>The cost of deserialization alone will dominate query execution time.</p>\n<p>This is a query to find the top ten most popular tags.</p>\n<p>Each row value is a nested data structure, and the absolute path to a list of tags is <code>user.post.tags</code>.</p>\n<ul>\n<li><s>[x] Show the example values of this schema, instead of describing it to the reader.</s></li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">WITH all_tags(tag) AS (<br />\t-- Defines a CTE (common-table expression) which runs exactly once<br />\t-- and returns the unnested tags from every `user` nested data.<br />\tSELECT UNNEST(user.post.tags) from users<br />)<br />SELECT tag, COUNT(*) as tag_count<br />  FROM all_tags<br />  GROUP BY tag<br />  ORDER BY tag_count DESC<br />  LIMIT 10<br />;<br />┌────────────────────┬───────────┐<br />│        tag         │ tag_count │<br />│      varchar       │   int64   │<br />├────────────────────┼───────────┤<br />│ javascript         │    120000 │<br />│ python             │     60000 │<br />│ sql                │     40000 │<br />│ java               │     30000 │<br />│ css                │     24000 │<br />│ aws                │     20000 │<br />│ docker             │     17142 │<br />│ data-science       │     15000 │<br />│ machine-learning   │     13333 │<br />│ kubernetes         │     12000 │<br />└────────────────────┴───────────┘</code></pre>\n<p>This a full table scan which has to deserialize every nested data structure just to access the <code>tags</code> list for computing the result.</p>\n<p>There is no direct way to access the <code>tags</code> field and slice only the required list values from the byte array.</p>\n<p>The binary blob column can be compressed using a general purpose compression algorithm like snappy, zstd to reduce the disk storage size.</p>\n<p>The total throughput for decompressing the column values, deserializing it, and computing the count aggregate will not exceed the speed at which a fast NVME SSD can read data from disk.</p>\n<p>The bottleneck here is never going to be disk I/O.</p>\n<p>The size of each nested data structure may fall anywhere between a few kilobytes to a few megabytes.</p>\n<p>When data structures are several megabytes, they exceed the size of the CPU's L3 cache.</p>\n<p>Processing a large value will trigger a cache miss that stalls the processor, forcing it to wait for data from main memory which is orders of magnitude slower, leaving the CPU idle instead of performing actual work.</p>\n<p>There is no option to selectively read only the parts required for computing the results of a query which almost certainly will fit into the lower L1/L2 cpu caches.</p>\n<p>There is a deserialization cost associated with brining each value into memory before being able to begin performing any computation relevant to the query.</p>\n<p>The majority of the work performed is not in service of computing the results of the query, but rather for accessing the values.</p>\n<p>However the nested representation produced by shredding using the Dremel encoding technique manages to successfully avoid the pitfalls of the opaque binary blob representation.</p>\n<h3 id=\"nested-schema\" tabindex=\"-1\">Nested Schema <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#nested-schema\" aria-hidden=\"true\"></a></h3>\n<p>Shredding depends on the schema to encode nested data into column values.</p>\n<p>Here are a few valid instances of a <code>contact</code> nested data structure:</p>\n<pre class=\"language-protobuf\"><code class=\"language-protobuf\"><span class=\"token comment\">// All properties are defined</span><br /><span class=\"token punctuation\">{</span> name<span class=\"token punctuation\">:</span> <span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span><br />  phones<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><br />\t<span class=\"token punctuation\">{</span> number<span class=\"token punctuation\">:</span> <span class=\"token string\">\"555-1234\"</span><span class=\"token punctuation\">,</span> phone_type<span class=\"token punctuation\">:</span> <span class=\"token string\">\"Home\"</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />\t<span class=\"token punctuation\">{</span> number<span class=\"token punctuation\">:</span> <span class=\"token string\">\"555-5678\"</span><span class=\"token punctuation\">,</span> phone_type<span class=\"token punctuation\">:</span> <span class=\"token string\">\"Work\"</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// The `phones` list is empty.</span><br /><span class=\"token punctuation\">{</span> name<span class=\"token punctuation\">:</span> <span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> phones<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// The `phones` property is missing.</span><br /><span class=\"token punctuation\">{</span> name<span class=\"token punctuation\">:</span> <span class=\"token string\">\"Charlie\"</span> <span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// The `names` property is missing.</span><br /><span class=\"token punctuation\">{</span> phones<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span>number<span class=\"token punctuation\">:</span> null<span class=\"token punctuation\">,</span> phone_type<span class=\"token punctuation\">:</span> <span class=\"token string\">\"Home\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">}</span></code></pre>\n<p>This is the schema used for shredding the above values for storage in Parquet format.</p>\n<pre class=\"language-text\"><code class=\"language-text\">message contact {<br />  OPTIONAL BINARY name (STRING);<br />  OPTIONAL group phones (LIST) {<br />    REPEATED group list {<br />      OPTIONAL group item {<br />        OPTIONAL BINARY number (STRING);<br />        OPTIONAL BINARY phone_type (STRING);<br />      }<br />    }<br />  }<br />}</code></pre>\n<blockquote>\n<p>Parquet is a self-describing file format which also stores the schema within the metadata located at the footer of the Parquet file.</p>\n</blockquote>\n<p>An <code>OPTIONAL</code> field can be present or missing.</p>\n<p>A <code>REPEATED</code> field is a list with zero or more elements.</p>\n<p>Nesting is added using a <code>group</code> which contains one or more child fields.</p>\n<p>The <code>LIST</code> is a nested data type which contains a nested field which defines the elements of the list.</p>\n<p>Then there are the primitive data types which represents the valeus at the leaf nodes of the nested data.</p>\n<p>As per the above schema there are three shredded columns in it which are:</p>\n<ul>\n<li><code>name</code></li>\n<li><code>phones.list.item.number</code></li>\n<li><code>phones.list.item.phone_type</code></li>\n</ul>\n<p>This is a direct physical representation of shredded columns stored in the Parquet file.</p>\n<p>The intermediate <code>list.item</code> is encapsulated from the user who queries this Parquet file as an internal implementation detail.</p>\n<blockquote>\n<p>Parquet adds the intermediate fields for list data structures to help remove any ambiguity between physical representation of data states in Dremel encoding like:</p>\n<ul>\n<li>an empty list</li>\n<li>a missing list</li>\n<li>first element of list is <code>NULL</code></li>\n</ul>\n</blockquote>\n<p>The user who writes or reads the <code>contact</code> nested data to Parquet has the following logical view of paths in the <code>contact</code> nested data for data access:</p>\n<ul>\n<li><code>name</code></li>\n<li><code>phones.number</code></li>\n<li><code>phones.phone_type</code></li>\n</ul>\n<blockquote>\n<p>The decoupling of the logical and physical representations is a common technique used in database engineering to reduce code complexity and improve the internal implementation without introducing breaking changes to the public interface.</p>\n</blockquote>\n<h3 id=\"definition-levels\" tabindex=\"-1\">Definition Levels <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#definition-levels\" aria-hidden=\"true\"></a></h3>\n<p>The definition level encodes the path structure of the nested data.</p>\n<p>Let us see this in action for a path in the schema which contains both <code>OPTIONAL</code> and <code>REPEATED</code> fields like <code>phones.list.item.number</code>.</p>\n<pre class=\"language-text\"><code class=\"language-text\">message contact {<br />  OPTIONAL BINARY name (STRING);<br />  OPTIONAL group phones (LIST) {<br />    REPEATED group list {<br />      OPTIONAL group item {<br />        OPTIONAL BINARY number (STRING);<br />        OPTIONAL BINARY phone_type (STRING);<br />      }<br />    }<br />  }<br />}</code></pre>\n<p>The definition level count starts at zero for every path.</p>\n<p>For each <code>OPTIONAL</code> and <code>REPEATED</code> field we encounter in the path from the root, we will increment the count by one.</p>\n<table>\n<thead>\n<tr>\n<th>Multiplicity</th>\n<th>Field Name</th>\n<th>Δ</th>\n<th>Definition Level</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>OPTIONAL</td>\n<td>phones</td>\n<td>+1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>REPEATED</td>\n<td>list</td>\n<td>+1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>OPTIONAL</td>\n<td>item</td>\n<td>+1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>OPTIONAL</td>\n<td>number</td>\n<td>+1</td>\n<td>4</td>\n</tr>\n</tbody>\n</table>\n<p>The maximum definition level value for this path is four.</p>\n<p>Now let us see how this encodes various data states which are possible for this path.</p>\n<table>\n<thead>\n<tr>\n<th>Value</th>\n<th>Definition Level</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>{ phones: null }</td>\n<td>0</td>\n<td><code>phones</code> is missing</td>\n</tr>\n<tr>\n<td>{ phones: [] }</td>\n<td>1</td>\n<td><code>list</code> is empty</td>\n</tr>\n<tr>\n<td>{ phones: [null] }</td>\n<td>2</td>\n<td><code>item</code> is missing</td>\n</tr>\n<tr>\n<td>{ phones: [{ number: null }] }</td>\n<td>3</td>\n<td><code>number</code> is missing</td>\n</tr>\n<tr>\n<td>{ phones: [{ number: &quot;555-1234&quot;}] }</td>\n<td>4</td>\n<td>all fields are present</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"repetition-levels\" tabindex=\"-1\">Repetition Levels <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#repetition-levels\" aria-hidden=\"true\"></a></h3>\n<p>The repetition level encodes the list boundaries and the offset of an item in a list.</p>\n<p>The repetition level count starts at zero for every path.</p>\n<p>It is incremented only for <code>REPEATED</code> paths in a field.</p>\n<table>\n<thead>\n<tr>\n<th>Multiplicity</th>\n<th>Field Name</th>\n<th>Δ</th>\n<th>Repetition Level</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>OPTIONAL</td>\n<td>phones</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>REPEATED</td>\n<td>list</td>\n<td>+1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>OPTIONAL</td>\n<td>item</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>OPTIONAL</td>\n<td>number</td>\n<td>0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p>The maximum repetition level for this path is therefore one.</p>\n<pre class=\"language-text\"><code class=\"language-text\">// Record [0]<br />{ phones: [<br />\t{ number: \"555-1234\" },<br />\t{ number: \"555-5678\" }]}<br /><br />// Record [1]<br />{ phones: [] }<br /><br />// Record [2]<br />{ phones: null }<br /><br />// Record [3]<br />{ phones: [ { number: null } ]}</code></pre>\n<p>Let us see how the repetition levels are computed for the following values.</p>\n<table>\n<thead>\n<tr>\n<th>Number (String)</th>\n<th>Repetition Level</th>\n<th>Definition Level</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>&quot;555-1234&quot;</td>\n<td>0</td>\n<td>4</td>\n<td>Record 0:<br /> Start a new list</td>\n</tr>\n<tr>\n<td>&quot;555-5678&quot;</td>\n<td>1</td>\n<td>4</td>\n<td>Record 0:<br /> Continuation of previous list</td>\n</tr>\n<tr>\n<td>null</td>\n<td>0</td>\n<td>1</td>\n<td>Record 1: <br /> Start a new list <br /> Definition level shows the list is empty</td>\n</tr>\n<tr>\n<td>null</td>\n<td>0</td>\n<td>0</td>\n<td>Record 2: <br /> Start a new list <br /> Definition level shows the <code>phones</code> field is missing</td>\n</tr>\n<tr>\n<td>null</td>\n<td>0</td>\n<td>3</td>\n<td>Record 3: <br /> Start a new list <br /> Definition level shows the <code>number</code> is missing</td>\n</tr>\n</tbody>\n</table>\n<p>The repetition level zero is a special case which always identifies the first list item and also the start of a new record.</p>\n<p>The remaining items of this list will all have the same repetition level.</p>\n<p>Therefore the repetition level zero also marks the record boundaries.</p>\n<blockquote>\n<p>Note: The start of a new list can have a non-zero repetition level. This occurs in a schema which contains more than one <code>REPEATED</code> field in a path.</p>\n</blockquote>\n<p>The last three values are NULL and all of them have a zero repetition level.</p>\n<p>We know a new record started, nothing more.</p>\n<p>But by inspecting the definition levels we know exactly at which point the path terminated, and what the data state it represents.</p>\n<h3 id=\"data-layout\" tabindex=\"-1\">Data Layout <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#data-layout\" aria-hidden=\"true\"></a></h3>\n<p>After shredding, the <code>NULL</code> values are not stored in physical storage.</p>\n<p>This is primarily done to save space and an optimization for large nested schemas which are sparesely populated.</p>\n<p>Missing values do not take up additional storage space.</p>\n<p>The shredded column <code>phones.number</code> has the following physical layout.</p>\n<pre><code>values: [&quot;555-1234&quot;, &quot;555-5678&quot;]\ndef: \t[4, 4, 1, 0, 3]\nrep: \t[0, 1, 0, 0, 0]\n</code></pre>\n<p>The maximum definition level of this path is four.</p>\n<p>Therefore we can infer that the last three values are null because the definition level is a value less than four.</p>\n<h3 id=\"comparison-with-opaque-binary-blob\" tabindex=\"-1\">Comparison with Opaque Binary Blob <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#comparison-with-opaque-binary-blob\" aria-hidden=\"true\"></a></h3>\n<p>Can we selectively read only those columns which are required for the query in the shredded representation?</p>\n<p>Yes.</p>\n<p>Do we have to deserialize column values?</p>\n<p>No, they are directly stored as a primitive type. There is no need to deserialize.</p>\n<p>Will the column values, definition levels and repetition levels fit into the lower level L1/L2/L3 caches of the CPU?</p>\n<p>Yes.</p>\n<p>In the binary blob representation a lot of preprocessing work is required to access the right values before the query processing could even begin.</p>\n<p>The nested representation using Dremel encoding is already in a columnar format, which allows direct access to columns which are used in the query.</p>\n<p>The query execution can begin as soon as the column values are read from disk and materialized into memory.</p>\n<h3 id=\"limitation:-point-lookup-queries\" tabindex=\"-1\">Limitation: Point Lookup Queries <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#limitation:-point-lookup-queries\" aria-hidden=\"true\"></a></h3>\n<p>A point lookup query is a highly-selective query which returns only a small set of rows.</p>\n<p>Say we want to access the 55th non-null value in the nested representation of <code>phones.phone_number</code> which also contains <code>NULL</code> values.</p>\n<p>To find the 55th non-null value we have to sequentially scan the definition levels array to skip the <code>NULL</code> values and map it to the correct physical row offset.</p>\n<p>This sequential scan is inherently unavoidable.</p>\n<p>We cannot jump directly to the value without first counting all the preceding non-null entries.</p>\n<p>Neverthless though slightly inefficient this is not disastrous for query performance.</p>\n<p>Columnar file formats like Parquet are capable of high-level pruning using metadata statistics which reduces the linear scanning to small chunks of the stored nested data.</p>\n<h4 id=\"limitation:-heterogenous-column-values\" tabindex=\"-1\">Limitation: Heterogenous Column Values <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#limitation:-heterogenous-column-values\" aria-hidden=\"true\"></a></h4>\n<p>The data type of the leaf node of a path, and therefore a shredded column is fixed.</p>\n<p>If the value contains a different data type it is runtime error which is schema data type mismatch error.</p>\n<p>This makes the Dremel encoding unsuitable for direct use with semi-structured data like JSON where often the same path can store values as different data types depending on some context.</p>\n<ul>\n<li>[x] <s>Show examples of JSON value where a key has a string type in some cases, and is an integer is few other cases.</s></li>\n</ul>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">[</span><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"phones\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span> <span class=\"token property\">\"number\"</span><span class=\"token operator\">:</span> <span class=\"token number\">5551234</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"phone_type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Home\"</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span><br />  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Diana\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"phones\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span> <span class=\"token property\">\"number\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"555-5678\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"phone_type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Work\"</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">]</span></code></pre>\n<p>In this JSON list the first <code>phones.number</code> value is an <code>Integer</code> data type and the second <code>phones.number</code> values is <code>String</code> data type.</p>\n<p>The first value will produce a schema mismatch error and cannot be shredded.</p>\n<p>A core limitation of shredding is that it only works with values which conforms to the defined schema.</p>\n<p>A workaround here is to write custom code which will coerce the <code>Integer</code> value 5551234 into a formatted <code>String</code> value like &quot;555-1234&quot;.</p>\n<h4 id=\"limitation:-no-selective-shredding-support\" tabindex=\"-1\">Limitation: No Selective Shredding Support <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#limitation:-no-selective-shredding-support\" aria-hidden=\"true\"></a></h4>\n<p>The clickstream data contains fields which are strongly-typed: <code>event</code>, <code>user_id</code> &amp; <code>timestamp</code> and also a user-generated <code>tags</code> field which is dynamically typed.</p>\n<p>Defining a rigid schema for <code>tags</code> is impractical because it removes the flexibility available to the user to add custom properties to the event log without any code changes.</p>\n<ul>\n<li>[x] <s>Show examples of an analytics telemetry data like mixpanel where the user can define JSON tags in the client side to dispatch with the telemetry.</s></li>\n</ul>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">[</span><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"event\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Login\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"user_id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">123</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"timestamp\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"2025-07-31T17:40:00Z\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"tags\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"method\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"password\"</span> <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br /><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"event\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"ViewItem\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"user_id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">123</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"timestamp\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"2025-07-31T17:41:15Z\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"tags\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"item_id\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"abc-987\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"price\"</span><span class=\"token operator\">:</span> <span class=\"token number\">19.95</span> <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br /><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"event\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Purchase\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"user_id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">123</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"timestamp\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"2025-07-31T17:45:30Z\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"tags\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"order_id\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"550e8400\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"total\"</span><span class=\"token operator\">:</span> <span class=\"token number\">52.85</span> <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">]</span></code></pre>\n<p>This makes it a poor fit for shredding which requires a pre-defined schema.</p>\n<p>Therefore it is not possible to apply shredding for the <code>tags</code> field.</p>\n<p>The best option here is to treat <code>tags</code> as an opaque binary blob field in the clickstream data.</p>\n<h3 id=\"a-practical-implementation\" tabindex=\"-1\">A Practical Implementation <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#a-practical-implementation\" aria-hidden=\"true\"></a></h3>\n<p>To see these concepts translated into code, I built <a href=\"https://github.com/jcsherin/denester\">denester</a>, a project that implements the Dremel shredding algorithm.</p>\n<p>The core shredding implementation is roughly 300 SLOC and can be directly found in <a href=\"https://github.com/jcsherin/denester/blob/8d1ef7ff62627591e8952ef0b2efbcbd386de9ba/src/parser.rs#L733-L1033\">src/parser.rs</a>.</p>\n<h3 id=\"a-parallel-shredding-implementation\" tabindex=\"-1\">A Parallel Shredding Implementation <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#a-parallel-shredding-implementation\" aria-hidden=\"true\"></a></h3>\n<p>To find out the upper limits of shredding performance, I built a second parallel implementation: <a href=\"https://github.com/jcsherin/datablok/tree/main/crates/parquet-parallel-nested\">parquet-parallel-nested</a>.</p>\n<p>This version is optimized for speed and uses the Rust Arrow (<a href=\"https://docs.rs/arrow/latest/arrow/index.html\">https://docs.rs/arrow/latest/arrow/index.html</a>) library to directly write to Parquet format.</p>\n<p>It also includes custom tooling for generating synthetic, skewed nested data for more realistic benchmarking.</p>\n<p>On a 16-core AMD Ryzen 7 Pro, it can generate, shred and write 10 million nested documents in approximately 450ms.</p>\n<h3 id=\"key-takeaways\" tabindex=\"-1\">Key Takeaways <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#key-takeaways\" aria-hidden=\"true\"></a></h3>\n<p>The genius of the Dremel encoding lies in its ability to encode the structure of nested data into two integer value arrays.</p>\n<p>This eliminates the penalty of deserialization when reading data from storage and also provide an incredibly CPU-friendly cache layout for maximizing query performance.</p>\n<p>While there are limitations and trade-offs it remains an integral technique in modern file formats like Apache Parquet for the modern data stack for analyzing nested data at scale.</p>\n<h3 id=\"references\" tabindex=\"-1\">References <a class=\"direct-link\" href=\"https://jacobsherin.com/posts/2025-07-23-shredding-nested-data-in-parquet/#references\" aria-hidden=\"true\"></a></h3>\n<ul>\n<li>\n<p><a href=\"https://research.google/pubs/pub36632/\">Dremel: Interactive Analysis of Web-Scale Datasets</a> - The original 2010 paper that introduced the Dremel shredding algorithm.</p>\n</li>\n<li>\n<p><a href=\"https://parquet.apache.org/docs/overview/motivation/\">The Parquet Format Specification</a> - The Dremel shredding algorithm had support built-in from the ground up from day one.</p>\n</li>\n<li>\n<p><a href=\"https://arrow.apache.org/docs/format/Columnar.html\">Apache Arrow Columnar Specification</a> - The specification for Arrow's in-memory format, which uses conceptually similar validity bitmaps and offset buffers to represent nested data.</p>\n</li>\n</ul>\n",
      "date_published": "2025-07-30T00:00:00Z"
    }
  ]
}
