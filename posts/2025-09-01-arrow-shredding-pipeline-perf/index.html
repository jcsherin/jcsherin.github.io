<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speeding Up a Parquet Shredding Pipeline by ~8x</title>
    <meta name="description" content="On database building blocks.">
    <meta name="generator" content="Eleventy v1.0.1">
    <link rel="stylesheet" href="/css/prism-okaidia.css">
    <link rel="stylesheet" href="/css/custom.css?v=1757064515324">
    <link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="">
    <link rel="alternate" href="/feed/feed.json" type="application/json" title="">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inria+Serif:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap" rel="stylesheet">
    <script>document.documentElement.classList.remove("no-js");</script>
  </head>
  <body>
    <div class="page-wrapper">
      <header>
        <hgroup>
        <h3 class="home"><a href="/">Jacob&#39;s blog</a></h3>
        <p>On database building blocks.</p>
      </hgroup>
        <ul class="nav">
          <li class="nav-item"><a href="/">posts</a></li>
          <li class="nav-item"><a href="/about/">whoami</a></li>
        </ul>
      </header>

      <main class="tmpl-post">
        
  <style>
    .draft-notice {
      padding: 0.75rem 0; /* Adds needed spacing */      
      border-block: 2px dashed; 
      font-style: italic;
    }
    .draft-notice p {
      margin: 0; /* Removes default paragraph margin */
    }
  </style>

  <aside class="draft-notice">
    <p>This is a draft preview. To publish, remove <code>draft: true</code> from the front matter.</p>
  </aside>


<h1>Speeding Up a Parquet Shredding Pipeline by ~8x</h1>

<time datetime="2025-09-02">02 Sep 2025</time>

<p>Lately, I've been poking around record shredding and needed a dataset of nested data structures for tracing query execution of shredded data. For this, I implemented a data generator which follows a <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipfian-like</a> distribution. The generated data is staged in-memory as <a href="https://arrow.apache.org/rust/parquet/arrow/index.html">Arrow RecordBatches</a>, and then written to disk as Parquet files.</p>
<p>The baseline version I wrote is a simple pipeline using Rust MPSC which connects multiple data generation (producer) threads to a single Parquet writer (consumer) thread. For a nested dataset of 10 million rows, it ~3.7s to complete. In this post, we'll see how a sequence of performance optimizations, reduced the total runtime to ~440ms (8x speedup).</p>
<p><img src="img/hyperfine_trend_plot.png" alt="Performance Trend Across Runs"></p>
<figcaption>Fig 1. The performance trend across a sequence of code optimizations, measured using <code>hyperfine</code>. The black line indicates the median runtime in seconds, while the shaded area indicates the range between min and max runtime.</figcaption>
<p><img src="img/ipc_trend_plot.png" alt="IPC Trend Across Runs"></p>
<p>This chart displays only improvements in total runtime, which does not tell the whole story. While some optimizations here show no difference in the total runtime, the improvements came from higher IPC (instructions per cycle), fewer cache misses and fewer branch mispredictions.</p>
<p>A string interning optimization (no. 9) looked like a guaranteed win. It was introduced to eliminate a lot of small string allocations in the data generation (producer) threads. The performance got worse (more on this later in this post) and the change had to be reverted. This strongly reinforces, the importance of measurements and profiling data for knowing unambiguously if a code optimization made an improvement or did the opposite.</p>
<p>All benchmarks were run on a Linux machine with the following configuration:</p>
<ul>
<li>Ubuntu 24.04.2 LTS (Kernel 6.8)</li>
<li>AMD Ryzen 7 PRO 8700GE (8 Cores, 16 Threads)</li>
<li>64 GB of DDR5-5600 ECC RAM</li>
<li>512 GB NVMe SSDs.</li>
</ul>
<nav class="toc" aria-labelledby="toc-heading">
  <h2 id="toc-heading">Table of Contents</h2>
  <ol>
    <li><a href="#background">Background</a></li>
    <li>
        <a href="#phase-1:-getting-started">Phase 1: Getting Started</a>
        <ul>
            <li><a href="#01:-use-a-dictionary-data-type">01: Use a Dictionary Data Type</a></li>
            <li><a href="#02:-eliminate-intermediate-vector-allocation">02: Eliminate Intermediate Vector Allocation</a></li>
            <li><a href="#03:-preallocate-a-string-buffer">03: Preallocate a String Buffer</a></li>
            <li><a href="#04:-preallocate-a-string-buffer-2">04: Preallocate a String Buffer 2</a></li>
            <li><a href="#why-is-the-runtime-unchanged">Why is the Runtime Unchanged?</a></li>
        </ul>
    </li>
    <li>
        <a href="#phase-2:-architectural-changes">Phase 2: Architectural Changes</a>
    </li>
    <li>
        <a href="#phase-3:-a-performance-regression">Phase 3: A Performance Regression</a>
    </li>
    <li>
        <a href="#phase-4:-micro-optimizations">Phase 4: Micro-Optimizations</a>
    </li>
</ol>
</nav>
<h2 id="background" tabindex="-1">Background <a class="direct-link" href="#background" aria-hidden="true">#</a></h2>
<p>The program is a CLI tool for generating a target number of rows of nested data structures and then written to disk in Parquet format.</p>
<p>Nested data structures do not naturally fit into a flat columnar format. Record shredding is a process which converts the nested data into a flat, columnar format while preserving the original structural hierarchy of the raw data.</p>
<p>The generated data follows a <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipfian-like</a> distribution. It is staged in memory as <a href="https://arrow.apache.org/rust/parquet/arrow/index.html">Arrow RecordBatches</a>, before being written to disk as Parquet files.</p>
<p>The data is generated in parallel using a <a href="https://github.com/rayon-rs/rayon">Rayon</a> thread pool. Then data generator threads (producers) sends the data to a Parquet writer thread (consumer). The number of writers are configurable from the CLI.</p>
<h2 id="phase-1:-getting-started" tabindex="-1">Phase 1: Getting Started <a class="direct-link" href="#phase-1:-getting-started" aria-hidden="true">#</a></h2>
<p>First, we build the CLI program in release mode and use that for end to end benchmarking using <a href="https://github.com/sharkdp/hyperfine">hyperfine</a>.</p>
<p>In <code>Cargo.toml</code> the following section is added for release builds:</p>
<pre class="language-toml"><code class="language-toml"><span class="token punctuation">[</span><span class="token table class-name">profile.release</span><span class="token punctuation">]</span><br><span class="token key property">debug</span> <span class="token punctuation">=</span> <span class="token string">"line-tables-only"</span><br><span class="token key property">strip</span> <span class="token punctuation">=</span> <span class="token boolean">false</span></code></pre>
<p>This will include just enough debug information in the release binary which will help us trace hotspots back to the exact line of code in Rust. This is necessary when recording the call-graphs of the program's execution using <code>perf</code>.</p>
<p>When generating flamegraphs, we will use <a href="https://github.com/luser/rustfilt">rustfilt</a> to demangle the symbols for improved readability.</p>
<p>We will also collect hardware performance counters like - cycles, instructions retired, cache references, cache misses, branch instructions and branch mispredictions.</p>
<p>The following optimizations from 01 through 04, uses the flamegraph to identify hotspots indicated by tall towers and then attempt to squash it.</p>
<h3 id="01:-use-a-dictionary-data-type" tabindex="-1">01: Use a Dictionary Data Type <a class="direct-link" href="#01:-use-a-dictionary-data-type" aria-hidden="true">#</a></h3>
<p>In the baseline version, the <code>PhoneType</code> Rust enum is mapped to a string data type (<code>DataType::Utf8</code>) in the Arrow schema.</p>
<pre class="language-rust"><code class="language-rust"><span class="token keyword">pub</span> <span class="token keyword">enum</span> <span class="token type-definition class-name">PhoneType</span> <span class="token punctuation">{</span><br>    <span class="token class-name">Mobile</span><span class="token punctuation">,</span><br>    <span class="token class-name">Home</span><span class="token punctuation">,</span><br>    <span class="token class-name">Work</span><span class="token punctuation">,</span><br><span class="token punctuation">}</span></code></pre>
<p>Instead, by changing the Arrow field data type to <code>DataType::Dictionary</code>, the expectation is that the total memory footprint of the program, and storage size of the Parquet file will improve.</p>
<pre class="language-diff"><code class="language-diff"><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">pub fn get_contact_phone_fields() -> Vec&lt;Arc&lt;Field>> {<br></span><span class="token prefix unchanged"> </span><span class="token line">    vec![<br></span><span class="token prefix unchanged"> </span><span class="token line">        Arc::from(Field::new("number", DataType::Utf8, true)),<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">        Arc::from(Field::new("phone_type", DataType::Utf8, true)),<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        Arc::from(Field::new(<br></span><span class="token prefix inserted">+</span><span class="token line">            "phone_type",<br></span><span class="token prefix inserted">+</span><span class="token line">            DataType::Dictionary(Box::new(DataType::UInt8), Box::new(DataType::Utf8)),<br></span><span class="token prefix inserted">+</span><span class="token line">            true,<br></span><span class="token prefix inserted">+</span><span class="token line">        )),<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    ]<br></span><span class="token prefix unchanged"> </span><span class="token line">}</code></pre>
<p>After the change, the maximum RSS (resident set size) is reduced by ~1MB in a run for generating 10 million rows. The Parquet storage size improvement is negligible. There is a minor regression in runtime.</p>
<p>Even though, there are no dramatic gains here like we expected, we will maintain this change because it removes the mismatch between the underlying Rust and Arrow data types. That is definitely a readability improvement.</p>
<h3 id="02:-eliminate-intermediate-vector-allocation" tabindex="-1">02: Eliminate Intermediate Vector Allocation <a class="direct-link" href="#02:-eliminate-intermediate-vector-allocation" aria-hidden="true">#</a></h3>
<p>The generate data with a predefined data skew (Zipfian-like), a data template value is first generated. The holes in the templates are filled in to generate the final <code>Contact</code> struct value, which is then converted to an Arrow <code>RecordBatch</code>. The series of value transformations looks like this:</p>
<p><code>Vec&lt;PartialContact&gt;</code> → <code>Vec&lt;Contact&gt;</code> → <code>RecordBatch</code>.</p>
<p>Instead of creating the intermediate <code>Vec&lt;Contact&gt;</code>, we can do a late materialization of the final <code>Contact</code> value when building a <code>RecordBatch</code> by directly passing it the instructions within <code>Vec&lt;PartialContact&gt;</code>. After eliminating the intermediate step, the value transformation will look like this:</p>
<p><code>Vec&lt;PartialContact&gt;</code> → <code>RecordBatch</code>.</p>
<pre class="language-diff"><code class="language-diff"><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">  // Assemble the Vec&lt;Contact> for this small chunk<br></span><span class="token prefix deleted">-</span><span class="token line">  let contacts_chunk: Vec&lt;Contact> = partial_contacts<br></span><span class="token prefix deleted">-</span><span class="token line">      .into_iter()<br></span><span class="token prefix deleted">-</span><span class="token line">      .map(|partial_contact| { ... })<br></span><span class="token prefix deleted">-</span><span class="token line">      .collect();<br></span><span class="token prefix deleted">-</span><span class="token line"><br></span><span class="token prefix deleted">-</span><span class="token line">  // Convert the chunk to a RecordBatch and send it to the writer<br></span><span class="token prefix deleted">-</span><span class="token line">  let record_batch = create_record_batch(parquet_schema.clone(), &amp;contacts_chunk)<br></span><span class="token prefix deleted">-</span><span class="token line">      .expect("Failed to create RecordBatch");<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">  let record_batch =<br></span><span class="token prefix inserted">+</span><span class="token line">      to_record_batch(<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">           parquet_schema.clone(), &amp;phone_id_counter, partial_contacts)<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">       .expect("Failed to create RecordBatch");<br></span><span class="token prefix inserted">+</span><span class="token line"></code></pre>
<p>After the change, there is no noticeable change in total runtime. On the other hand, there is a noticeable improvement across the board in CPU utilization metrics. Even though the pipeline did not execute any faster, it ran more efficiently.</p>
<h3 id="03:-preallocate-a-string-buffer" tabindex="-1">03: Preallocate a String Buffer <a class="direct-link" href="#03:-preallocate-a-string-buffer" aria-hidden="true">#</a></h3>
<p>In the hot loop, where a <code>RecordBatch</code> is being created, a string is allocated in the heap for each generated value. For a run of 10 million rows this is the equivalent of 10 million heap allocations.</p>
<p>We can eliminate 99% of these allocations by reusing a mutable string buffer within the loop where <code>PartialContact</code> template values are being materialized and appended into the <code>RecordBatch</code>.</p>
<p>Suppose a <code>RecordBatch</code> is created from a chunk of 1K row values, it now requires only 10K heap allocations.</p>
<pre class="language-diff"><code class="language-diff"><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    let mut phone_number_buf = String::with_capacity(16);<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    for PartialContact(name, phones) in chunk {<br></span><span class="token prefix unchanged"> </span><span class="token line">        name_builder.append_option(name);<br></span></span><br>@@ -155,11 +158,13 @@ fn to_record_batch(<br><br><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    if has_phone_number {<br></span><span class="token prefix unchanged"> </span><span class="token line">       let id = phone_id_counter.fetch_add(1, Ordering::Relaxed);<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">       let phone_number = Some(format!("+91-99-{id:08}"));<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">       write!(phone_number_buf, "+91-99-{id:08}")?;<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">       struct_builder<br></span><span class="token prefix unchanged"> </span><span class="token line">           .field_builder::&lt;StringBuilder>(PHONE_NUMBER_FIELD_INDEX)<br></span><span class="token prefix unchanged"> </span><span class="token line">           .unwrap()<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">           .append_option(phone_number);<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">           .append_value(&amp;phone_number_buf);<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span><span class="token prefix inserted">+</span><span class="token line">       phone_number_buf.clear();</code></pre>
<p>After this change, there is again no noticeable change in the total runtime. But similar to earlier change, all measures point to an overall improvement in the CPU efficiency of the program.</p>
<h3 id="04:-preallocate-a-string-buffer-2" tabindex="-1">04: Preallocate a String Buffer 2 <a class="direct-link" href="#04:-preallocate-a-string-buffer-2" aria-hidden="true">#</a></h3>
<p>This is a follow up optimization from the previous one. The idea is the same, to eliminate 99% of heap allocations when generating data, by preallocating a mutable string buffer, and reusing it.</p>
<pre class="language-diff"><code class="language-diff"><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">fn name_strategy() -> BoxedStrategy&lt;Option&lt;String>> {<br></span><span class="token prefix unchanged"> </span><span class="token line">    prop_oneof![<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">        80 => Just(()).prop_map(|_| Some(format!("{} {}", FirstName().fake::&lt;String>(), LastName().fake::&lt;String>()))),<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        80 => Just(()).prop_map(|_| {<br></span><span class="token prefix inserted">+</span><span class="token line">            let mut name_buf = String::with_capacity(32);<br></span><span class="token prefix inserted">+</span><span class="token line">            write!(&amp;mut name_buf, "{} {}", FirstName().fake::&lt;&amp;str>(), LastName().fake::&lt;&amp;str>()).unwrap();<br></span><span class="token prefix inserted">+</span><span class="token line">            Some(name_buf)<br></span><span class="token prefix inserted">+</span><span class="token line">         }),<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">        20 => Just(None)<br></span><span class="token prefix unchanged"> </span><span class="token line">    ].boxed()<br></span><span class="token prefix unchanged"> </span><span class="token line">}<br></span></span></code></pre>
<p>The results are identical to the previous optimization. No change in the total runtime. But there is considerable improvement in the CPU efficiency of the program.</p>
<h3 id="why-is-the-runtime-unchanged" tabindex="-1">Why is the Runtime Unchanged? <a class="direct-link" href="#why-is-the-runtime-unchanged" aria-hidden="true">#</a></h3>
<p>The optimizations so far had little to no effect on the total runtime of the program, which has remained stable.</p>
<p><img src="img/hyperfine_boxplot_grid_phase1.png" alt="Hyperfine box plots for baseline version up to run 04"></p>
<p>The flamegraph profiles taken after each optimization also display a similar consistency.</p>
<p>We have not seen a speedup in the underlying program despite the optimizations is related to <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdhal's law</a>. The pipeline execution spent only a small fraction of its total execution time in the hot loops which were optimized. This is characterized by tall but narrow towers in the flamegraph profile. To achieve a runtime speedup, we have to focus on the widest towers, as they indicate where the most amount of time is spend.</p>
<p><img src="img/flamegraph_montage_phase1.png" alt="Flamegraph montage for baseline version up to run 04"></p>
<p>The CPU efficiency has improved across most metrics from the baseline version because of eliminating allocations.</p>
<p>The same program now executes in less CPU cycles, requires less instructions. Reducing heap allocations is particularly noticeable as reduced cache-references, cache-misses, branch-instructions and branch-misses.</p>
<p>Even though the runtime has not changed, the user time metric shows that we have shaved off ~2s (from 28s to under 26s) with these optimizations.</p>
<p><img src="img/perf_stats_phase1.png" alt="Perf stats for baseline version up to run 04 "></p>
<p>The individual performance counter metrics have improved, but the IPC (instructions per cycle) has gone down from 1.20 to 1.18. Even so, we are now executing the workload using less CPU instructions and cycles. That counts as an efficiency improvement.</p>
<p><img src="img/ipc_trend_phase1.png" alt="IPC trend for baseline version up to run 04"></p>
<h2 id="phase-2:-architectural-changes" tabindex="-1">Phase 2: Architectural Changes <a class="direct-link" href="#phase-2:-architectural-changes" aria-hidden="true">#</a></h2>
<p><img src="img/flamegraph_montage_phase2.png" alt="Flamegraph montage from run 04 to run 08">
<img src="img/hyperfine_boxplot_grid_phase2.png" alt="Hyperfine box plots from run 04 to run 08">
<img src="img/perf_stats_phase2.png" alt="Perf stats from run 04 to run 08">
<img src="img/ipc_trend_phase2.png" alt="IPC trend from run 04 to run 08"></p>
<h2 id="phase-3:-a-performance-regression" tabindex="-1">Phase 3: A Performance Regression <a class="direct-link" href="#phase-3:-a-performance-regression" aria-hidden="true">#</a></h2>
<p><img src="img/flamegraph_montage_phase3.png" alt="Flamegraph montage from run 08 to run 10">
<img src="img/hyperfine_boxplot_grid_phase3.png" alt="Hyperfine box plots from run 08 to run 10">
<img src="img/perf_stats_phase3.png" alt="Perf stats from run 08 to run 10">
<img src="img/ipc_trend_phase3.png" alt="IPC trend from run 08 to run 10"></p>
<h2 id="phase-4:-micro-optimizations" tabindex="-1">Phase 4: Micro-optimizations <a class="direct-link" href="#phase-4:-micro-optimizations" aria-hidden="true">#</a></h2>
<p><img src="img/flamegraph_montage_phase4.png" alt="Flamegraph montage from run 10 to run 12">
<img src="img/hyperfine_boxplot_grid_phase4.png" alt="Hyperfine box plots from run 10 to run 12">
<img src="img/perf_stats_phase4.png" alt="Perf stats from run 10 to run 12">
<img src="img/ipc_trend_phase4.png" alt="IPC trend from run 10 to run 12"></p>


      </main>

      <footer></footer>

      <!-- Current page: /posts/2025-09-01-arrow-shredding-pipeline-perf/ -->
    </body>
  </div>
</html>
