<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Performance Tuning a Nested Data Generator for Parquet</title>
    <meta name="description" content="On database building blocks.">
    <meta name="generator" content="Eleventy v1.0.1">
    <link rel="stylesheet" href="/css/prism-okaidia.css">
    <link rel="stylesheet" href="/css/custom.css?v=1758525399896">
    <link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="">
    <link rel="alternate" href="/feed/feed.json" type="application/json" title="">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inria+Serif:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap" rel="stylesheet">
    <script>document.documentElement.classList.remove("no-js");</script>
  </head>
  <body>
    <div class="page-wrapper">
      <header>
        <hgroup>
        <h3 class="home"><a href="/">Jacob&#39;s blog</a></h3>
        <p>On database building blocks.</p>
      </hgroup>
        <ul class="nav">
          <li class="nav-item"><a href="/">posts</a></li>
          <li class="nav-item"><a href="/about/">whoami</a></li>
        </ul>
      </header>

      <main class="tmpl-post">
        
  <style>
    .draft-notice {
      padding: 0.75rem 0; /* Adds needed spacing */      
      border-block: 2px dashed; 
      font-style: italic;
    }
    .draft-notice p {
      margin: 0; /* Removes default paragraph margin */
    }
  </style>

  <aside class="draft-notice">
    <p>This is a draft preview. To publish, remove <code>draft: true</code> from the front matter.</p>
  </aside>


<h1>Performance Tuning a Nested Data Generator for Parquet</h1>

<time datetime="2025-09-02">02 Sep 2025</time>

<p>Lately, I've been poking around record shredding and needed a dataset of nested data structures for tracing query execution of shredded data. For this, I implemented a data generator which follows a <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipfian-like</a> distribution. The generated data is staged in-memory as <a href="https://arrow.apache.org/rust/parquet/arrow/index.html">Arrow RecordBatches</a>, and then written to disk as Parquet files.</p>
<p>The baseline version I wrote is a simple pipeline using Rust MPSC which connects multiple data generation (producer) threads to a single Parquet writer (consumer) thread. For a nested dataset of 10 million rows, it ~3.7s to complete. In this post, we'll see how a sequence of performance optimizations, reduced the total runtime to ~533ms (6x speedup).</p>
<p><img src="img/hyperfine_trend_plot.png" alt="Performance Trend Across Runs"></p>
<figcaption>Fig 1. The performance trend across a sequence of code optimizations, measured using <code>hyperfine</code>. The black line indicates the median runtime in seconds, while the shaded area indicates the range between min and max runtime.</figcaption>
<p><img src="img/ipc_trend_plot.png" alt="IPC Trend Across Runs"></p>
<p>This chart displays only improvements in total runtime, which does not tell the whole story. While some optimizations here show no difference in the total runtime, the improvements came from higher IPC (instructions per cycle), fewer cache misses and fewer branch mispredictions.</p>
<p>A string interning optimization (no. 9) looked like a guaranteed win. It was introduced to eliminate a lot of small string allocations in the data generation (producer) threads. The performance got worse (more on this later in this post) and the change had to be reverted. This strongly reinforces, the importance of measurements and profiling data for knowing unambiguously if a code optimization made an improvement or did the opposite.</p>
<p>All benchmarks were run on a Linux machine with the following configuration:</p>
<ul>
<li>Ubuntu 24.04.2 LTS (Kernel 6.8)</li>
<li>AMD Ryzen 7 PRO 8700GE (8 Cores, 16 Threads)</li>
<li>64 GB of DDR5-5600 ECC RAM</li>
<li>512 GB NVMe SSDs.</li>
</ul>
<nav class="toc" aria-labelledby="toc-heading">
  <h2 id="toc-heading">Table of Contents</h2>
  <ol>
    <li><a href="#background">Background</a></li>
    <li>
        <a href="#phase-1:-getting-started">Phase 1: Getting Started</a>
        <ul>
            <li><a href="#01:-use-a-dictionary-data-type">01: Use a Dictionary Data Type</a></li>
            <li><a href="#02:-eliminate-intermediate-vector-allocation">02: Eliminate Intermediate Vector Allocation</a></li>
            <li><a href="#03:-preallocate-a-string-buffer">03: Preallocate a String Buffer</a></li>
            <li><a href="#04:-preallocate-a-string-buffer-2">04: Preallocate a String Buffer 2</a></li>
            <li><a href="#why-is-the-runtime-unchanged">Why is the Runtime Unchanged?</a></li>
        </ul>
    </li>
    <li>
      <a href="#phase-2:-architectural-changes">Phase 2: Architectural Changes</a>
      <ul>
        <li><a href="#05:-increase-parquet-encoding-bandwidth">05: Increase Parquet Encoding Bandwidth</a></li>
        <li><a href="#06:-make-data-generation-lightweight">06: Make Data Generation Lightweight</a></li>
        <li><a href="#07:-increase-parquet-writer-threads">07: Increase Parquet Writer Threads</a></li>
        <li><a href="#08:-introduce-thread-local-state">08: Introduce Thread-Local State</a></li>
        <li><a href="#measuring-impact">Measuring Impact</a></li>
      </ul>
    </li>
    <li>
        <a href="#phase-3:-a-performance-regression">Phase 3: A Performance Regression</a>
        <ul>
            <li><a href="#09:-global-string-interning">09: Global String Interning</a></li>
            <li><a href="#10:-revert">10: Revert</a></li>
        </ul>
    </li>
    <li>
        <a href="#a-back-of-the-envelope-estimation">A Back of the Envelope Estimation</a>
    </li>
    <li>
        <a href="#fine-tuning-configuration-for-optimal-performance">Fine-tuning Configuration for Optimal Performance</a>
    </li>
  </ol>
</nav>
<h2 id="background" tabindex="-1">Background <a class="direct-link" href="#background" aria-hidden="true">#</a></h2>
<p>The program is a CLI tool for generating a target number of rows of nested data structures and then written to disk in Parquet format.</p>
<p>Nested data structures do not naturally fit into a flat columnar format. Record shredding is a process which converts the nested data into a flat, columnar format while preserving the original structural hierarchy of the raw data.</p>
<p>The generated data follows a <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipfian-like</a> distribution. It is staged in memory as <a href="https://arrow.apache.org/rust/parquet/arrow/index.html">Arrow RecordBatches</a>, before being written to disk as Parquet files.</p>
<p>The data is generated in parallel using a <a href="https://github.com/rayon-rs/rayon">Rayon</a> thread pool. Then data generator threads (producers) sends the data to a Parquet writer thread (consumer). The number of writers are configurable from the CLI.</p>
<h2 id="phase-1:-getting-started" tabindex="-1">Phase 1: Getting Started <a class="direct-link" href="#phase-1:-getting-started" aria-hidden="true">#</a></h2>
<p>First, we build the CLI program in release mode and use that for end to end benchmarking using <a href="https://github.com/sharkdp/hyperfine">hyperfine</a>.</p>
<p>In <code>Cargo.toml</code> the following section is added for release builds:</p>
<pre class="language-toml"><code class="language-toml"><span class="token punctuation">[</span><span class="token table class-name">profile.release</span><span class="token punctuation">]</span><br><span class="token key property">debug</span> <span class="token punctuation">=</span> <span class="token string">"line-tables-only"</span><br><span class="token key property">strip</span> <span class="token punctuation">=</span> <span class="token boolean">false</span></code></pre>
<p>This will include just enough debug information in the release binary which will help us trace hotspots back to the exact line of code in Rust. This is necessary when recording the call-graphs of the program's execution using <code>perf</code>.</p>
<p>When generating flamegraphs, we will use <a href="https://github.com/luser/rustfilt">rustfilt</a> to demangle the symbols for improved readability.</p>
<p>We will also collect hardware performance counters like - cycles, instructions retired, cache references, cache misses, branch instructions and branch mispredictions.</p>
<p>The following optimizations from 01 through 04, uses the flamegraph to identify hotspots indicated by tall towers and then attempt to squash it.</p>
<h3 id="01:-use-a-dictionary-data-type" tabindex="-1">01: Use a Dictionary Data Type <a class="direct-link" href="#01:-use-a-dictionary-data-type" aria-hidden="true">#</a></h3>
<p>In the baseline version, the <code>PhoneType</code> Rust enum is mapped to a string data type (<code>DataType::Utf8</code>) in the Arrow schema.</p>
<pre class="language-rust"><code class="language-rust"><span class="token keyword">pub</span> <span class="token keyword">enum</span> <span class="token type-definition class-name">PhoneType</span> <span class="token punctuation">{</span><br>    <span class="token class-name">Mobile</span><span class="token punctuation">,</span><br>    <span class="token class-name">Home</span><span class="token punctuation">,</span><br>    <span class="token class-name">Work</span><span class="token punctuation">,</span><br><span class="token punctuation">}</span></code></pre>
<p>Instead, by changing the Arrow field data type to <code>DataType::Dictionary</code>, the expectation is that the total memory footprint of the program, and storage size of the Parquet file will improve.</p>
<pre class="language-diff"><code class="language-diff"><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">pub fn get_contact_phone_fields() -> Vec&lt;Arc&lt;Field>> {<br></span><span class="token prefix unchanged"> </span><span class="token line">    vec![<br></span><span class="token prefix unchanged"> </span><span class="token line">        Arc::from(Field::new("number", DataType::Utf8, true)),<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">        Arc::from(Field::new("phone_type", DataType::Utf8, true)),<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        Arc::from(Field::new(<br></span><span class="token prefix inserted">+</span><span class="token line">            "phone_type",<br></span><span class="token prefix inserted">+</span><span class="token line">            DataType::Dictionary(Box::new(DataType::UInt8), Box::new(DataType::Utf8)),<br></span><span class="token prefix inserted">+</span><span class="token line">            true,<br></span><span class="token prefix inserted">+</span><span class="token line">        )),<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    ]<br></span><span class="token prefix unchanged"> </span><span class="token line">}</code></pre>
<p>After the change, the maximum RSS (resident set size) is reduced by ~1MB in a run for generating 10 million rows. The Parquet storage size improvement is negligible. There is a minor regression in runtime.</p>
<p>Even though, there are no dramatic gains here like we expected, we will maintain this change because it removes the mismatch between the underlying Rust and Arrow data types. That is definitely a readability improvement.</p>
<h3 id="02:-eliminate-intermediate-vector-allocation" tabindex="-1">02: Eliminate Intermediate Vector Allocation <a class="direct-link" href="#02:-eliminate-intermediate-vector-allocation" aria-hidden="true">#</a></h3>
<p>The generate data with a predefined data skew (Zipfian-like), a data template value is first generated. The holes in the templates are filled in to generate the final <code>Contact</code> struct value, which is then converted to an Arrow <code>RecordBatch</code>. The series of value transformations looks like this:</p>
<p><code>Vec&lt;PartialContact&gt;</code> → <code>Vec&lt;Contact&gt;</code> → <code>RecordBatch</code>.</p>
<p>Instead of creating the intermediate <code>Vec&lt;Contact&gt;</code>, we can do a late materialization of the final <code>Contact</code> value when building a <code>RecordBatch</code> by directly passing it the instructions within <code>Vec&lt;PartialContact&gt;</code>. After eliminating the intermediate step, the value transformation will look like this:</p>
<p><code>Vec&lt;PartialContact&gt;</code> → <code>RecordBatch</code>.</p>
<pre class="language-diff"><code class="language-diff"><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">  // Assemble the Vec&lt;Contact> for this small chunk<br></span><span class="token prefix deleted">-</span><span class="token line">  let contacts_chunk: Vec&lt;Contact> = partial_contacts<br></span><span class="token prefix deleted">-</span><span class="token line">      .into_iter()<br></span><span class="token prefix deleted">-</span><span class="token line">      .map(|partial_contact| { ... })<br></span><span class="token prefix deleted">-</span><span class="token line">      .collect();<br></span><span class="token prefix deleted">-</span><span class="token line"><br></span><span class="token prefix deleted">-</span><span class="token line">  // Convert the chunk to a RecordBatch and send it to the writer<br></span><span class="token prefix deleted">-</span><span class="token line">  let record_batch = create_record_batch(parquet_schema.clone(), &amp;contacts_chunk)<br></span><span class="token prefix deleted">-</span><span class="token line">      .expect("Failed to create RecordBatch");<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">  let record_batch =<br></span><span class="token prefix inserted">+</span><span class="token line">      to_record_batch(<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">           parquet_schema.clone(), &amp;phone_id_counter, partial_contacts)<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">       .expect("Failed to create RecordBatch");<br></span><span class="token prefix inserted">+</span><span class="token line"></code></pre>
<p>After the change, there is no noticeable change in total runtime. On the other hand, there is a noticeable improvement across the board in CPU utilization metrics. Even though the pipeline did not execute any faster, it ran more efficiently.</p>
<h3 id="03:-preallocate-a-string-buffer" tabindex="-1">03: Preallocate a String Buffer <a class="direct-link" href="#03:-preallocate-a-string-buffer" aria-hidden="true">#</a></h3>
<p>In the hot loop, where a <code>RecordBatch</code> is being created, a string is allocated in the heap for each generated value. For a run of 10 million rows this is the equivalent of 10 million heap allocations.</p>
<p>We can eliminate 99% of these allocations by reusing a mutable string buffer within the loop where <code>PartialContact</code> template values are being materialized and appended into the <code>RecordBatch</code>.</p>
<p>Suppose a <code>RecordBatch</code> is created from a chunk of 1K row values, it now requires only 10K heap allocations.</p>
<pre class="language-diff"><code class="language-diff"><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    let mut phone_number_buf = String::with_capacity(16);<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    for PartialContact(name, phones) in chunk {<br></span><span class="token prefix unchanged"> </span><span class="token line">        name_builder.append_option(name);<br></span></span><br>@@ -155,11 +158,13 @@ fn to_record_batch(<br><br><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    if has_phone_number {<br></span><span class="token prefix unchanged"> </span><span class="token line">       let id = phone_id_counter.fetch_add(1, Ordering::Relaxed);<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">       let phone_number = Some(format!("+91-99-{id:08}"));<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">       write!(phone_number_buf, "+91-99-{id:08}")?;<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">       struct_builder<br></span><span class="token prefix unchanged"> </span><span class="token line">           .field_builder::&lt;StringBuilder>(PHONE_NUMBER_FIELD_INDEX)<br></span><span class="token prefix unchanged"> </span><span class="token line">           .unwrap()<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">           .append_option(phone_number);<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">           .append_value(&amp;phone_number_buf);<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span><span class="token prefix inserted">+</span><span class="token line">       phone_number_buf.clear();</code></pre>
<p>After this change, there is again no noticeable change in the total runtime. But similar to earlier change, all measures point to an overall improvement in the CPU efficiency of the program.</p>
<h3 id="04:-preallocate-a-string-buffer-2" tabindex="-1">04: Preallocate a String Buffer 2 <a class="direct-link" href="#04:-preallocate-a-string-buffer-2" aria-hidden="true">#</a></h3>
<p>This is a follow up optimization from the previous one. The idea is the same, to eliminate 99% of heap allocations when generating data, by preallocating a mutable string buffer, and reusing it.</p>
<pre class="language-diff"><code class="language-diff"><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">fn name_strategy() -> BoxedStrategy&lt;Option&lt;String>> {<br></span><span class="token prefix unchanged"> </span><span class="token line">    prop_oneof![<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">        80 => Just(()).prop_map(|_| Some(format!("{} {}", FirstName().fake::&lt;String>(), LastName().fake::&lt;String>()))),<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        80 => Just(()).prop_map(|_| {<br></span><span class="token prefix inserted">+</span><span class="token line">            let mut name_buf = String::with_capacity(32);<br></span><span class="token prefix inserted">+</span><span class="token line">            write!(&amp;mut name_buf, "{} {}", FirstName().fake::&lt;&amp;str>(), LastName().fake::&lt;&amp;str>()).unwrap();<br></span><span class="token prefix inserted">+</span><span class="token line">            Some(name_buf)<br></span><span class="token prefix inserted">+</span><span class="token line">         }),<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">        20 => Just(None)<br></span><span class="token prefix unchanged"> </span><span class="token line">    ].boxed()<br></span><span class="token prefix unchanged"> </span><span class="token line">}<br></span></span></code></pre>
<p>The results are identical to the previous optimization. No change in the total runtime. But there is considerable improvement in the CPU efficiency of the program.</p>
<h3 id="why-is-the-runtime-unchanged" tabindex="-1">Why is the Runtime Unchanged? <a class="direct-link" href="#why-is-the-runtime-unchanged" aria-hidden="true">#</a></h3>
<p>The optimizations so far had little to no effect on the total runtime of the program, which has remained stable.</p>
<p><img src="img/hyperfine_boxplot_grid_phase1.png" alt="Hyperfine box plots for baseline version up to run 04"></p>
<p>The flamegraph profiles taken after each optimization also display a similar consistency.</p>
<p>We have not seen a speedup in the underlying program despite the optimizations is related to <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdhal's law</a>. The pipeline execution spent only a small fraction of its total execution time in the hot loops which were optimized. This is characterized by tall but narrow towers in the flamegraph profile. To achieve a runtime speedup, we have to focus on the widest towers, as they indicate where the most amount of time is spend.</p>
<p><img src="img/flamegraph_montage_phase1.png" alt="Flamegraph montage for baseline version up to run 04"></p>
<p>The CPU efficiency has improved across most metrics from the baseline version because of eliminating allocations.</p>
<p>The same program now executes in less CPU cycles, requires less instructions. Reducing heap allocations is particularly noticeable as reduced cache-references, cache-misses, branch-instructions and branch-misses.</p>
<p>Even though the runtime has not changed, the user time metric shows that we have shaved off ~2s (from 28s to under 26s) with these optimizations.</p>
<p><img src="img/perf_stats_phase1.png" alt="Perf stats for baseline version up to run 04 "></p>
<p>The individual performance counter metrics have improved, but the IPC (instructions per cycle) has gone down from 1.20 to 1.18. Even so, we are now executing the workload using less CPU instructions and cycles. That counts as an efficiency improvement.</p>
<p><img src="img/ipc_trend_phase1.png" alt="IPC trend for baseline version up to run 04"></p>
<h2 id="phase-2:-architectural-changes" tabindex="-1">Phase 2: Architectural Changes <a class="direct-link" href="#phase-2:-architectural-changes" aria-hidden="true">#</a></h2>
<p>The lesson learned from the previous optimizations, is that to speed up the pipeline we need to focus our efforts on the most time consuming parts of the execution. The most obvious optimization then, is to increase the write throughput, by adding more writers.</p>
<p>We can do a lot better here to improve the speed of the pipeline, by exploiting data parallelism. The data generation is parallelized, but the data encoding to Parquet is not. It is great candidate for making parallel because it is also a compute-bound workload which is now single-threaded.</p>
<p>The size of 10 million rows on disk in Parquet format is ~292MB, and the program takes ~4s to execute. So we know for certain that the writer thread is not I/O bound. We need to be writing an order of magnitude more bytes to disk to saturate the NVME I/O write speeds.</p>
<p>We can also optimize the data generation to speed up the program. It currently depends on <a href="https://github.com/proptest-rs/proptest">proptest</a> (a property testing library). I reused it instead of rolling my own, because the <a href="https://docs.rs/proptest/latest/proptest/strategy/trait.Strategy.html">Strategy trait</a> provides a nice API for defining data skew for the fields of the nested data structure. From the flamegraph profile it is evident that it does a lot more work than which is strictly needed in our case.</p>
<h3 id="05:-increase-parquet-encoding-bandwidth" tabindex="-1">05: Increase Parquet Encoding Bandwidth <a class="direct-link" href="#05:-increase-parquet-encoding-bandwidth" aria-hidden="true">#</a></h3>
<p>The simplest possible thing to do here is to increase the number of writers from one to two. For that we can partition the data generator threads into two retaining the MPSC pattern. Each partition is connected to a Parquet writer. This effectively doubles the encoding bandwidth, and it requires only a minimal lines of code to be changed.</p>
<pre class="language-diff"><code class="language-diff"><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    let (tx, rx) = mpsc::sync_channel::&lt;RecordBatch>(num_threads * 2);<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    let (tx1, rx1) = mpsc::sync_channel::&lt;RecordBatch>(num_threads);<br></span><span class="token prefix inserted">+</span><span class="token line">    let (tx2, rx2) = mpsc::sync_channel::&lt;RecordBatch>(num_threads);<br></span></span><br><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    let writer_handle_1 = create_writer_thread("contacts_1.parquet", rx1);<br></span><span class="token prefix inserted">+</span><span class="token line">    let writer_handle_2 = create_writer_thread("contacts_2.parquet", rx2);<br></span></span><br><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    let chunk_count = target_contacts.div_ceil(BASE_CHUNK_SIZE);<br></span><span class="token prefix unchanged"> </span><span class="token line">    let parquet_schema = get_contact_schema();<br></span></span><br><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    (0..chunk_count)<br></span><span class="token prefix unchanged"> </span><span class="token line">        .into_par_iter()<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">        .for_each_with(tx, |tx, chunk_index| {<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        .for_each_with((tx1, tx2), |(tx1, tx2), chunk_index| {<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">            let start_index = chunk_index * BASE_CHUNK_SIZE;<br></span><span class="token prefix unchanged"> </span><span class="token line">            let current_chunk_size = std::cmp::min(BASE_CHUNK_SIZE, target_contacts - start_index);<br></span></span><br>@@ -263,11 +275,17 @@ fn main() -> Result&lt;(), Box&lt;dyn Error + Send + Sync>> {<br><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">                to_record_batch(parquet_schema.clone(), &amp;phone_id_counter, partial_contacts)<br></span><span class="token prefix unchanged"> </span><span class="token line">                    .expect("Failed to create RecordBatch");<br></span></span><br><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">            tx.send(record_batch).unwrap();<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">            if chunk_index % 2 == 0 {<br></span><span class="token prefix inserted">+</span><span class="token line">                tx1.send(record_batch).unwrap();<br></span><span class="token prefix inserted">+</span><span class="token line">            } else {<br></span><span class="token prefix inserted">+</span><span class="token line">                tx2.send(record_batch).unwrap();<br></span><span class="token prefix inserted">+</span><span class="token line">            }<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">        });<br></span></span><br><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    // Teardown<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    writer_handle.join().unwrap()?;<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    writer_handle_1.join().unwrap()?;<br></span><span class="token prefix inserted">+</span><span class="token line">    writer_handle_2.join().unwrap()?;<br></span><span class="token prefix inserted">+</span><span class="token line"></code></pre>
<p>This shaved off ~1s (3.6s to 2.7s) from execution time. This is significant, and I need to now find if adding more writers will further reduce the runtime. The major gains are not going to come from this it.</p>
<h3 id="06:-make-data-generation-lightweight" tabindex="-1">06: Make Data Generation Lightweight <a class="direct-link" href="#06:-make-data-generation-lightweight" aria-hidden="true">#</a></h3>
<p>The flamegraph profile shows that over 80% of the pipeline execution time is spend in the data generation methods. It is the most dominant factor we need to focus on. Around 20% of that time is spend in <a href="https://docs.rs/proptest/latest/proptest/strategy/trait.Strategy.html#tymethod.new_tree">Strategy::new_tree</a> alone, which is the entry point for data generation.</p>
<p>There is a lot of performed here which does not contribute to data generation, but is necessary for a test runner. We can eliminate this extra work by implementing light-weight functions, but keeping the ergonomic API design.</p>
<p>Maybe, I could have done better at the beginning by rolling my own implementation. But the goal at the beginning was to have a correct, simple working implementation. Performance is important, but it will have been pure guesswork if I had predicted that this will so dominant in the runtime. The other reason is I like the ergonomic API design, which I can copy in the light-weight implementation.</p>
<p>The diff below shows the implementation for the <code>phone_type</code> field in the nested data structure. You can see the structural similarities between the old and new versions of the code.</p>
<pre class="language-diff"><code class="language-diff"><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">// A Zipfian-like categorical distribution for `phone_type`<br></span><span class="token prefix deleted">-</span><span class="token line">//<br></span><span class="token prefix deleted">-</span><span class="token line">// | Phone Type | Probability |<br></span><span class="token prefix deleted">-</span><span class="token line">// |------------|-------------|<br></span><span class="token prefix deleted">-</span><span class="token line">// | Mobile     | 0.55        |<br></span><span class="token prefix deleted">-</span><span class="token line">// | Work       | 0.35        |<br></span><span class="token prefix deleted">-</span><span class="token line">// | Home       | 0.10        |<br></span><span class="token prefix deleted">-</span><span class="token line">//<br></span><span class="token prefix deleted">-</span><span class="token line">fn phone_type_strategy() -> BoxedStrategy&lt;PhoneType> {<br></span><span class="token prefix deleted">-</span><span class="token line">    prop_oneof![<br></span><span class="token prefix deleted">-</span><span class="token line">        55 => Just(PhoneType::Mobile),<br></span><span class="token prefix deleted">-</span><span class="token line">        35 => Just(PhoneType::Work),<br></span><span class="token prefix deleted">-</span><span class="token line">        10 => Just(PhoneType::Home),<br></span><span class="token prefix deleted">-</span><span class="token line">    ]<br></span><span class="token prefix deleted">-</span><span class="token line">    .boxed()<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">fn generate_phone_type(rng: &amp;mut impl Rng) -> PhoneType {<br></span><span class="token prefix inserted">+</span><span class="token line">    match rng.random_range(0..100) {<br></span><span class="token prefix inserted">+</span><span class="token line">        0..=54 => PhoneType::Mobile, // 0.55<br></span><span class="token prefix inserted">+</span><span class="token line">        55..=89 => PhoneType::Work,  // 0.35<br></span><span class="token prefix inserted">+</span><span class="token line">        _ => PhoneType::Home,        // 0.10<br></span><span class="token prefix inserted">+</span><span class="token line">    }<br></span><span class="token prefix inserted">+</span><span class="token line">}<br></span><span class="token prefix inserted">+</span><span class="token line"></code></pre>
<p>This refactoring has to be applied uniformly for every field and methods which compose nested fields. The diff below shows how the property-testing runner is replaced with a simple for loop. In this refactoring we completely eliminate the <code>proptest</code> dependency.</p>
<pre class="language-diff"><code class="language-diff">diff --git a/Cargo.lock b/Cargo.lock<br>index 626375c..a6b06a8 100644<br><span class="token coord">--- a/Cargo.lock</span><br><span class="token coord">+++ b/Cargo.lock</span><br>@@ -2322,7 +2322,7 @@ dependencies = [<br><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line"> "log",<br></span><span class="token prefix unchanged"> </span><span class="token line"> "parquet",<br></span><span class="token prefix unchanged"> </span><span class="token line"> "parquet-common",<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line"> "proptest",<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line"> "rand 0.9.1",<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line"> "rayon",<br></span><span class="token prefix unchanged"> </span><span class="token line">]<br></span></span><br>diff --git a/crates/parquet-parallel-nested/src/main.rs b/crates/parquet-parallel-nested/src/main.rs<br>index f6d8610..0fa2083 100644<br><span class="token coord">--- a/crates/parquet-parallel-nested/src/main.rs</span><br><span class="token coord">+++ b/crates/parquet-parallel-nested/src/main.rs</span><br><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line"><br></span><span class="token prefix inserted">+</span><span class="token line">fn generate_contacts_chunk(size: usize, seed: u64) -> Vec&lt;PartialContact> {<br></span><span class="token prefix inserted">+</span><span class="token line">    let mut rng = StdRng::seed_from_u64(seed);<br></span><span class="token prefix inserted">+</span><span class="token line">    let mut contacts = Vec::with_capacity(size);<br></span><span class="token prefix inserted">+</span><span class="token line">    let mut name_buf = String::with_capacity(32);<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span><span class="token prefix inserted">+</span><span class="token line">    for _ in 0..size {<br></span><span class="token prefix inserted">+</span><span class="token line">        contacts.push(generate_partial_contact(&amp;mut rng, &amp;mut name_buf));<br></span><span class="token prefix inserted">+</span><span class="token line">    }<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span><span class="token prefix inserted">+</span><span class="token line">    contacts<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">}<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line"><br></span><span class="token prefix deleted">-</span><span class="token line">fn generate_contacts_chunk(size: usize, seed: u64) -> Vec&lt;PartialContact> {<br></span><span class="token prefix deleted">-</span><span class="token line">    let mut runner = TestRunner::new(Config {<br></span><span class="token prefix deleted">-</span><span class="token line">        rng_seed: RngSeed::Fixed(seed),<br></span><span class="token prefix deleted">-</span><span class="token line">        ..Config::default()<br></span><span class="token prefix deleted">-</span><span class="token line">    });<br></span><span class="token prefix deleted">-</span><span class="token line"><br></span><span class="token prefix deleted">-</span><span class="token line">    let strategy = proptest::collection::vec(contact_strategy(), size);<br></span><span class="token prefix deleted">-</span><span class="token line">    strategy<br></span><span class="token prefix deleted">-</span><span class="token line">        .new_tree(&amp;mut runner)<br></span><span class="token prefix deleted">-</span><span class="token line">        .expect("Failed to generate chunk of partial contacts")<br></span><span class="token prefix deleted">-</span><span class="token line">        .current()<br></span><span class="token prefix deleted">-</span><span class="token line">}</code></pre>
<p>The pipeline is now 2.3x faster. The total runtime decreased from 2.70s to 1.18s (~1.5s). The IPC (instructions per cycle) nearly doubled, from 1.05 to 2.02. Every other stat shows similar improvements.</p>
<p>This is a strong result. The program speed increased, and it is also now more efficient in core utilization.</p>
<h3 id="07:-increase-parquet-writer-threads" tabindex="-1">07: Increase Parquet Writer Threads <a class="direct-link" href="#07:-increase-parquet-writer-threads" aria-hidden="true">#</a></h3>
<p>The Parquet encoding step remains a bottleneck as the data generators outpace the two writer threads. A simple test is to see the effect of doubling the writers again.</p>
<pre class="language-diff"><code class="language-diff"><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    if chunk_index % 2 == 0 {<br></span><span class="token prefix deleted">-</span><span class="token line">            tx1.send(record_batch).unwrap();<br></span><span class="token prefix deleted">-</span><span class="token line">        } else {<br></span><span class="token prefix deleted">-</span><span class="token line">            tx2.send(record_batch).unwrap();<br></span><span class="token prefix deleted">-</span><span class="token line">        }<br></span><span class="token prefix deleted">-</span><span class="token line">    });<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    let record_batch =<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">       to_record_batch(parquet_schema.clone(), &amp;phone_id_counter, partial_contac<br></span></span>ts)<br><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">       .expect("Failed to create RecordBatch");<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span><span class="token prefix inserted">+</span><span class="token line">    match chunk_index % 4 {<br></span><span class="token prefix inserted">+</span><span class="token line">        0 => s1.send(record_batch).expect("Failed to send to rx1"),<br></span><span class="token prefix inserted">+</span><span class="token line">        1 => s2.send(record_batch).expect("Failed to send to rx2"),<br></span><span class="token prefix inserted">+</span><span class="token line">        2 => s3.send(record_batch).expect("Failed to send to rx3"),<br></span><span class="token prefix inserted">+</span><span class="token line">        _ => s4.send(record_batch).expect("Failed to send to rx4"),<br></span><span class="token prefix inserted">+</span><span class="token line">    }</code></pre>
<p>Another ~1.5x speedup in runtime. The total runtime dropped below a second (800ms) for the first time. On the other hand, the IPC dropped to 1.76 from the previous high.</p>
<h3 id="08:-introduce-thread-local-state" tabindex="-1">08: Introduce Thread-Local State <a class="direct-link" href="#08:-introduce-thread-local-state" aria-hidden="true">#</a></h3>
<p>The flamegraph profile now shows that around 20% of the time is spend in resizing vectors, and cloning strings in data generation.</p>
<p>The current data generation is stateless. As soon as a chunk of nested records are created, a <code>RecordBatch</code> is created. And this is immediately send to the writer thread. The chunk size setting is hard-coded as 256. It creates ~39K <code>RecordBatches</code> for 10 million records. We could increase chunk size, but a better thing to do here is decouple the chunk size from the row count at which we finalize a <code>RecordBatch</code>, so that they can be tuned separately.</p>
<p>For example, if the chunk size is 256, we can configure a <code>RecordBatch</code> to be finalized when we have 5K nested records. Now only ~2K <code>RecordBatch</code>es are created for a run of 10 million records. For this we introduce <code>GeneratorState</code> struct which contains the <code>RecordBatch</code> fields to which we are appending the chunk values.</p>
<pre class="language-diff"><code class="language-diff"><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">struct GeneratorState {<br></span><span class="token prefix inserted">+</span><span class="token line">    schema: SchemaRef,<br></span><span class="token prefix inserted">+</span><span class="token line">    name: StringBuilder,<br></span><span class="token prefix inserted">+</span><span class="token line">    phone_number_buf: String,<br></span><span class="token prefix inserted">+</span><span class="token line">    counter: Arc&lt;AtomicUsize>,<br></span><span class="token prefix inserted">+</span><span class="token line">    phones: ListBuilder&lt;StructBuilder>,<br></span><span class="token prefix inserted">+</span><span class="token line">    current_chunks: usize,<br></span><span class="token prefix inserted">+</span><span class="token line">}<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span><span class="token prefix inserted">+</span><span class="token line">enum GeneratorStateError {<br></span><span class="token prefix inserted">+</span><span class="token line">    NotEnoughChunks { current: usize, required: usize },<br></span><span class="token prefix inserted">+</span><span class="token line">    TryFlushZeroChunks,<br></span><span class="token prefix inserted">+</span><span class="token line">}<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span></span>@@ -411,38 +351,95 @@ fn main() -> Result&lt;(), Box&lt;dyn Error + Send + Sync>> {<br><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">        .num_threads(num_producers)<br></span><span class="token prefix unchanged"> </span><span class="token line">        .build()<br></span><span class="token prefix unchanged"> </span><span class="token line">        .unwrap();<br></span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line"><br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">    pool.install(|| {<br></span><span class="token prefix unchanged"> </span><span class="token line">        let chunk_count = target_contacts.div_ceil(BASE_CHUNK_SIZE);<br></span><span class="token prefix unchanged"> </span><span class="token line">        let parquet_schema = get_contact_schema();<br></span></span><br><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        (0..num_producers).into_par_iter().for_each(|producer_id| {<br></span><span class="token prefix inserted">+</span><span class="token line">            // Each thread gets its own state and a clone of the senders.<br></span><span class="token prefix inserted">+</span><span class="token line">            let mut generator_state =<br></span><span class="token prefix inserted">+</span><span class="token line">                GeneratorState::new(parquet_schema.clone(), phone_id_counter.clone());</code></pre>
<p>This reduces the runtime by another 25% (~800ms to ~600ms). We have also regained IPC and it is at an all time high of 2.21. Every efficiency parameter has improved.</p>
<p>Yet another significant improvement both in speedup and efficiency of the program's execution.</p>
<h3 id="measuring-impact" tabindex="-1">Measuring Impact <a class="direct-link" href="#measuring-impact" aria-hidden="true">#</a></h3>
<p>We improved the total pipeline throughput by increasing the writers. The data generator got lighter, and faster. Finally, we increased the <code>RecordBatch</code> size.</p>
<p>All of the above optimizations to the pipeline has resulted in a 6X speedup, with the runtime dropping from 3.61s to 0.58s.</p>
<p><img src="img/hyperfine_boxplot_grid_phase2.png" alt="Hyperfine box plots from run 04 to run 08"></p>
<p>The cores are now being utilized more efficiently, with every stats we tracked improving significantly.</p>
<p><img src="img/perf_stats_phase2.png" alt="Perf stats from run 04 to run 08"></p>
<p>The IPC improved from 1.18 to 2.21 (an 87% increase).</p>
<p><img src="img/ipc_trend_phase2.png" alt="IPC trend from run 04 to run 08"></p>
<p>The final flamegraph shows a concentrated workload which is evenly divided. The data generator (producer) threads profile occupies the left side, while the parquet writer (consumer) threads profile occupies to the right side. The transition to the final flamegraph, shows a clear improvement from a fragmented hotspots to more efficient pipeline execution.</p>
<p><img src="img/flamegraph_montage_phase2.png" alt="Flamegraph montage from run 04 to run 08"></p>
<h2 id="phase-3:-a-performance-regression" tabindex="-1">Phase 3: A Performance Regression <a class="direct-link" href="#phase-3:-a-performance-regression" aria-hidden="true">#</a></h2>
<p>The generated <code>name</code> field is <code>NULL</code> 20% of the time. When generating 10 million rows, we therefore expect the <code>name</code> column to contain roughly 8 million name strings. To my surprise, the total no. of unique names were only ~1.4 million.</p>
<p>When generating millions of <code>fake</code> names, the chance of a collision becomes very high. The <code>fake</code> implementation is most likely sampling first name, last name pairing with replacement. Since we are generating a large number of names, the collisions become more frequent. This is also known as the <a href="https://en.wikipedia.org/wiki/Birthday_problem">Birthday problem</a>.</p>
<p>It looks like we can minimize string allocations by 82.5% because only ~1.4 million unique names are generated for a run of total size 10 million.</p>
<h3 id="09:-global-string-interning" tabindex="-1">09: Global String Interning <a class="direct-link" href="#09:-global-string-interning" aria-hidden="true">#</a></h3>
<p>When a new name is generated, we check to see if it is unique in a global hashmap. If it is new and unique it is added to the hashmap. If it already exists in the hashmap, we reuse the allocated string stored within the hashmap.</p>
<pre class="language-diff"><code class="language-diff"><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">  let name = Some(name_buf.clone());<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">  let name = if let Some(interned) = interner.get(name_buf) {<br></span><span class="token prefix inserted">+</span><span class="token line">      interned.clone()<br></span><span class="token prefix inserted">+</span><span class="token line">  } else {<br></span><span class="token prefix inserted">+</span><span class="token line">      let new_arc = Arc::new(name_buf.clone());<br></span><span class="token prefix inserted">+</span><span class="token line">      interner.insert(name_buf.clone(), new_arc.clone());<br></span><span class="token prefix inserted">+</span><span class="token line">      new_arc<br></span><span class="token prefix inserted">+</span><span class="token line">  };<br></span><span class="token prefix inserted">+</span><span class="token line"><br></span><span class="token prefix inserted">+</span><span class="token line">  // clear the name buffer for next use<br></span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">  name_buf.clear();<br></span></span><br><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">  name<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">  Some(name)</code></pre>
<p>The savings are realized when we finally add the generated name when constructing the <code>RecordBatch</code>.</p>
<pre class="language-diff"><code class="language-diff"><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">            name.append_option(generate_name(rng, &amp;mut name_buf));<br></span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">            name.append_option(generate_name(rng, &amp;mut name_buf, &amp;self.interner).as_deref());</code></pre>
<p>An extra allocation is avoided by directly passing a reference to the interned string.</p>
<h3 id="10:-revert" tabindex="-1">10: Revert <a class="direct-link" href="#10:-revert" aria-hidden="true">#</a></h3>
<p>The benchmarks shows a runtime regression of 65%, from 583ms to 965ms. The IPC halved from 2.21 to 1.10. The cache misses increased to 22% from 10%. The measurements leaves no doubt, this is a clear performance regression.</p>
<p>But why did string interning not work?</p>
<p>The data generator threads are generating at a rapid pace, and the names are being inserted into the same internal buckets. These may fit into a cache line, and shared across cores. But the high volume writes are constantly invalidating the cache lines, and this points to a cache coherency issue. A data generator thread can invalidate the cache line when another data generator thread is attempting to access the same cache line to get a reference to the interned string. This causes the CPU to stall, and wait for the new cache line to be fetched. Both problems are visible in the lower IPC and higher cache miss rate.</p>
<p>While reverting the code I noticed that the earlier version, was cloning the name string buffer before passing it the <code>name</code> field builder. This is not necessary, as we can pass the reference directly without cloning, as Arrow will make a copy internally. So an extra clone was removed in the end.</p>
<p>The median runtime is now 533ms from 584ms, shaving off another 51ms from the final runtime.</p>
<p><img src="img/hyperfine_boxplot_grid_phase3.png" alt="Hyperfine box plots from run 08 to run 10"></p>
<p>The changes in hardware performance counters are negligible in most cases, but the total instructions, and branch instructions have reduced significantly. This could be attributed to removing the unnecessary cloning of the mutable buffer.</p>
<p><img src="img/perf_stats_phase3.png" alt="Perf stats from run 08 to run 10"></p>
<h2 id="a-back-of-the-envelope-estimation" tabindex="-1">A Back of the Envelope Estimation <a class="direct-link" href="#a-back-of-the-envelope-estimation" aria-hidden="true">#</a></h2>
<p>The pipeline throughput is dependent on its slowest stage. In micro-benchmarks, where the data generator and writer throughput is measured on a single core, the performance is comparable.</p>
<table>
<thead>
<tr>
<th>Record Batch Size</th>
<th>Data Generation Time per Record (ns)</th>
<th>Write Time per Record (ns)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1024</td>
<td>140.89</td>
<td>135.30</td>
</tr>
<tr>
<td>2048</td>
<td>145.76</td>
<td>133.05</td>
</tr>
<tr>
<td>4096</td>
<td>149.08</td>
<td>131.51</td>
</tr>
<tr>
<td>8192</td>
<td>149.66</td>
<td>129.82</td>
</tr>
</tbody>
</table>
<p>Therefore for this workload, the optimal ratio of producers to consumers is close to 1:1. On the 16-core machine used for end-end benchmarking, we should see the best performance when using 8 generator threads paired together with 8 writer threads.</p>
<p>From the table we can calculate, the theoretical ceiling for throughput which comes to ~7 million records/second/core. Our highest observed throughput value is 21 millions/records/second on 16 cores with 8-writers, which is ~2.6 million records/second/core.</p>
<p>On 16-cores, the user time is 4.82s and system time is 0.71s for a combined 5.53s. The total wall clock time is 0.51s. This means we are effectively using 11 cores (5.53s / 0.51s) of the total available 16 cores. This is a high-level of parallelism, where for the duration of the pipelines execution 11 cores are fully busy.</p>
<p>For future improvements, the single-core efficiency has to be improved, but where?</p>
<p><img src="img/flamegraph-8w-8192rb.png" alt="Flamegraph of latest version after all optimizations are applied"></p>
<p>The flamegraph shows a equal split of work distribution between the data generation threads and writer threads. The next bottleneck appears as the overhead of Rayon in dividing the work between the data generation threads. The data generator is extremely fast that the overhead of distributing work is greater.</p>
<p>For a run of 10 million rows, with a record batch size of 8192, we now generate 1221 small batches for Rayon to distribute in parallel to cores running the data generator threads. A single batch completes in 1.2ms, so Rayon has to do constantly schedule tasks to cores at the rate of 833 tasks/second. The next optimization should target reducing this overhead.</p>
<h2 id="fine-tuning-configuration-for-optimal-performance" tabindex="-1">Fine-tuning Configuration for Optimal Performance <a class="direct-link" href="#fine-tuning-configuration-for-optimal-performance" aria-hidden="true">#</a></h2>
<p>Keeping the target rows constant at 10 million, we can execute the pipeline a range of record batch sizes, writer threads to compare record throughput (million records/second) and total runtime.</p>
<pre class="language-text"><code class="language-text">$ ./target/release/parquet-nested-parallel --help<br>A tool for generating and writing nested Parquet data in parallel<br><br>Usage: parquet-nested-parallel [OPTIONS]<br><br>Options:<br>      --target-records <TARGET_RECORDS><br>          The target number of records to generate [default: 10000000]<br>      --record-batch-size <RECORD_BATCH_SIZE><br>          The size of each record batch [default: 4096]<br>      --num-writers <NUM_WRITERS><br>          The number of parallel writers [default: 4]<br>      --output-dir <OUTPUT_DIR><br>          The output directory for the Parquet files<br>      --output-filename <OUTPUT_FILENAME><br>          The base filename for the output Parquet files<br>      --dry-run<br>          Do not execute the pipeline<br>  -h, --help<br>          Print help<br>  -V, --version<br>          Print version</code></pre>
<p>In the micro-benchmark comparison from earlier, the single-core performance of the data generator and writer threads are comparable. So a best performance was predicted to come from a 1:1 allocation of CPU cores between the data generation and writer threads.</p>
<p>Below we can see that 8 writers, 8 producers produces the highest observed throughput of 23 million records/second. A higher record batch size has little to no effect beyond 10K. With 6 writers, 10 producers, the throughput is above 20 million record/second, but is not optimal because of the imbalanced allocation.</p>
<p><img src="img/performance_heatmap_10M_record_throughput_m_sec.png" alt="Record Throughput Analysis"></p>
<p>No surprises here as well. The lowest recorded runtime is when we have a 1:1 allocation, with 8 writers and 8 producer threads.</p>
<p><img src="img/performance_heatmap_10M_total_time_ms.png" alt="Total Runtime Analysis"></p>
<p>To recap, future optimizations should target reducing the Rayon threadpool overhead to improve single-core efficiency. The current pipeline achieves a high-level of parallelism by being able to fully utilize 11 out of 16 available cores. There is close to ~3X headroom remaining for improving the current ~2.6 million records/per/core throughput, to a maximum possible ~7 million records/per/core throughput. Since the bottleneck changes after each optimization, it is therefore important to continue with a data-driven approach.</p>


      </main>

      <footer></footer>

      <!-- Current page: /posts/2025-09-01-arrow-shredding-pipeline-perf/ -->
    </body>
  </div>
</html>
